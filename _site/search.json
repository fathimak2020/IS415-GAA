[
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#setting-the-scene",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#setting-the-scene",
    "title": "Take-home Exercise 2 [DATA PREPARATION]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.1 Setting the scene",
    "text": "2.1 Setting the scene\nDengue Hemorrhagic Fever (commonly known as dengue fever) is a prevalent mosquito-borne disease in tropical and subtropical regions. Caused by the dengue virus transmitted by female Aedes aegypti and Aedes albopictus mosquitoes, it is characterised by acute symptoms. In 2015, Taiwan experienced a severe outbreak, reporting over 43,000 cases and 228 deaths. Subsequently, annual cases remained below 200 until 2023, when Taiwan recorded 26,703 cases, with Tainan City alone reporting over 25,000 cases."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#objectives",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#objectives",
    "title": "Take-home Exercise 2 [DATA PREPARATION]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.2 Objectives",
    "text": "2.2 Objectives\nThe goal is to determine if the distribution of dengue fever outbreaks in Tainan City, Taiwan, is independent spatially and spatio-temporally. If dependencies exist, the focus shifts to identifying clusters, outliers, and emerging hot spots/cold spots to gain insights into the specific spatial and spatio-temporal dynamics of the outbreaks.\nThe specific objectives of this take-home exercise include the following tasks:\n\nUtilise the appropriate functions from the sf and tidyverse packages to prepare a geospatial data layer. This layer should consist of a study area represented by sf polygon features at the village level, confined to the D01, D02, D04, D06, D07, D08, D32, and D39 counties of Tainan City, Taiwan.\nGenerate a dengue fever layer within the specified study area, represented by sf point features. The dengue fever cases should be limited to epidemiology weeks 31-50 in the year 2023.\nCreate a derived dengue fever layer using the spacetime s3 class of sfdep. This layer should include, among other relevant information, a data field indicating the number of dengue fever cases by village and by epidemiology week.\nUtilise the extracted data to conduct a global spatial autocorrelation analysis.\nUtilise the extracted data for local spatial autocorrelation analysis.\nPerform an emerging hotspot analysis using the extracted data.\nProvide a detailed description of the spatial patterns revealed by the analyses conducted in the previous steps."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#getting-started",
    "title": "Take-home Exercise 2 [DATA PREPARATION]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.3 Getting Started",
    "text": "2.3 Getting Started"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#data-acquisition",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#data-acquisition",
    "title": "Take-home Exercise 2 [DATA PREPARATION]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.3.1 Data Acquisition",
    "text": "2.3.1 Data Acquisition\nThe study will utilise the following datasets to explore the distribution of dengue fever outbreaks in Tainan City, Taiwan, with respect to both spatial and spatio-temporal dimensions.\n\n\n\nDataset\nType\nSource\nPath\n\n\n\n\nTAIWAN_VILLAGE_2020\nVillage boundary of Taiwan\nGeospatial (.shp)\nhttps://data.gov.tw/en/datasets/130549\ndata/geospatial/TAIWAN_VILLAGE_2020\n\n\nDengue_Daily.csv\nReported dengue cases in Taiwan\nAspatial (.csv)\nhttps://data.cdc.gov.tw/en/dataset/dengue-daily-determined-cases-1998\ndata/aspatial/Dengue_Daily.csv"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#importing-relevant-r-packages",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#importing-relevant-r-packages",
    "title": "Take-home Exercise 2 [DATA PREPARATION]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.3.2 Importing Relevant R Packages",
    "text": "2.3.2 Importing Relevant R Packages\nThe R packages used in this project are:\n\ndplyr: for data manipulation\nmaptools: set of tools for reading and manipulating spatial data formats, such as shapefiles\nrgdal: from CRAN, enables users to import, export, and manipulate spatial data within the R environment\nsf: for importing, managing, and processing geospatial data\ntidyverse: a family of other R packages for performing data science tasks such as importing, wrangling, and visualizing data\ntmap: creating static and interactive maps\nggplot2: used for data visualization\nplotly: interactive graphing library for R\nlubridate: for working with date-time data\n\nPacman assists us by helping us load R packages that we require.\n\npacman::p_load(dplyr,maptools, rgdal,sf, sp, tidyverse, tmap, ggplot2, plotly, lubridate)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#importing-geospatial-datasets",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#importing-geospatial-datasets",
    "title": "Take-home Exercise 2 [DATA PREPARATION]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.3.3 Importing Geospatial Datasets",
    "text": "2.3.3 Importing Geospatial Datasets\n\n2.3.3.1 Village Boudary of Taiwan\nFor shapefile format, two arguments are required: dsn to define the data path, and layer to provide the shapefile name. \n\ntainan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"TAINAN_VILLAGE\")\n\nReading layer `TAINAN_VILLAGE' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Take-home_Ex\\Take-home_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 649 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0269 ymin: 22.88751 xmax: 120.6563 ymax: 23.41374\nGeodetic CRS:  TWD97\n\n\nWe see that the CRS is TWD 97; let’s verify whether it has been assigned the accurate EPSG code as well.\n\n#st_crs(tainan_sf)\n\nThe correct EPSG for TWD 97 is 3826, and it has been inaccurately assigned. Let’s assign the correct EPSG code.\n\n#tainan_sf &lt;- st_set_crs(tainan_sf, 3826)\n#st_crs(tainan_sf)\n\n\n\n\n\n\n\nImportant\n\n\n\nProject Transformation\nThe imported data has been assigned the EPSG code 3826 for TWD 97 using st_set_crs.\n\n\n\npar(mar = c(0,0,0,0))\nplot(st_geometry(tainan_sf))\n\n\n\n\nWe use head() to gather more information about the dataset.\n\nhead(tainan_sf)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.053 ymin: 22.93251 xmax: 120.2905 ymax: 23.16919\nGeodetic CRS:  TWD97\n     VILLCODE COUNTYNAME TOWNNAME VILLNAME        VILLENG COUNTYID COUNTYCODE\n1 67000280002     臺南市   歸仁區   六甲里    Liujia Vil.        D      67000\n2 67000350032     臺南市   安南區   青草里   Qingcao Vil.        D      67000\n3 67000150009     臺南市   七股區   溪南里     Xinan Vil.        D      67000\n4 67000150010     臺南市   七股區   七股里      Qigu Vil.        D      67000\n5 67000150008     臺南市   七股區   龍山里  Longshan Vil.        D      67000\n6 67000150017     臺南市   七股區   中寮里 Zhongliao Vil.        D      67000\n  TOWNID TOWNCODE NOTE                       geometry\n1    D33 67000280 &lt;NA&gt; POLYGON ((120.2725 22.95868...\n2    D06 67000350 &lt;NA&gt; POLYGON ((120.1176 23.08387...\n3    D22 67000150 &lt;NA&gt; POLYGON ((120.121 23.1355, ...\n4    D22 67000150 &lt;NA&gt; POLYGON ((120.1312 23.1371,...\n5    D22 67000150 &lt;NA&gt; POLYGON ((120.0845 23.13503...\n6    D22 67000150 &lt;NA&gt; POLYGON ((120.126 23.16917,...\n\n\nIn the dataset, county IDs are contained in a column named TOWNID. Our focus is on the counties with the IDs D01, D02, D04, D06, D07, D08, D32, and D39. The provided code snippet utilises the subset function to extract the relevant counties from the tainan_sf dataset, and these selected rows are then stored in a new dataframe named counties_sf.\n\nd01 &lt;- subset(tainan_sf, tainan_sf$TOWNID == \"D01\")\nd02 &lt;- subset(tainan_sf, tainan_sf$TOWNID == \"D02\")\nd04 &lt;- subset(tainan_sf, tainan_sf$TOWNID == \"D04\")\nd06 &lt;- subset(tainan_sf, tainan_sf$TOWNID == \"D06\")\nd07 &lt;- subset(tainan_sf, tainan_sf$TOWNID == \"D07\")\nd08 &lt;- subset(tainan_sf, tainan_sf$TOWNID == \"D08\")\nd32 &lt;- subset(tainan_sf, tainan_sf$TOWNID == \"D32\")\nd39 &lt;- subset(tainan_sf, tainan_sf$TOWNID == \"D39\")\n\n\ncounties_sf &lt;- dplyr::bind_rows(list(d01, d02, d04, d06, d07, d08, d32, d39))\n\nWe visualise our study area using tmap and apply a color code based on the county ID.\n\ntmap_mode('plot')\ntm_shape(counties_sf) + \n  tm_polygons(\"TOWNID\") \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have obtained the counties of interest as the study area."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#importing-aspatial-dataset",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#importing-aspatial-dataset",
    "title": "Take-home Exercise 2 [DATA PREPARATION]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.3.4 Importing Aspatial Dataset",
    "text": "2.3.4 Importing Aspatial Dataset\n\n2.3.4.1 Dengue Cases\n\ndengue &lt;- read_csv(\"data/aspatial/Dengue_Daily.csv\")\n\nWe use head() to gather more information about the dataset.\n\nhead(dengue)\n\n# A tibble: 6 × 26\n  發病日     個案研判日 通報日     性別  年齡層 居住縣市 居住鄉鎮 居住村里\n  &lt;date&gt;     &lt;chr&gt;      &lt;date&gt;     &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n1 1998-01-02 None       1998-01-07 男    40-44  屏東縣   屏東市   None    \n2 1998-01-03 None       1998-01-14 男    30-34  屏東縣   東港鎮   None    \n3 1998-01-13 None       1998-02-18 男    55-59  宜蘭縣   宜蘭市   None    \n4 1998-01-15 None       1998-01-23 男    35-39  高雄市   苓雅區   None    \n5 1998-01-20 None       1998-02-04 男    55-59  宜蘭縣   五結鄉   None    \n6 1998-01-22 None       1998-02-19 男    20-24  桃園市   蘆竹區   None    \n# ℹ 18 more variables: 最小統計區 &lt;chr&gt;, 最小統計區中心點X &lt;chr&gt;,\n#   最小統計區中心點Y &lt;chr&gt;, 一級統計區 &lt;chr&gt;, 二級統計區 &lt;chr&gt;,\n#   感染縣市 &lt;chr&gt;, 感染鄉鎮 &lt;chr&gt;, 感染村里 &lt;chr&gt;, 是否境外移入 &lt;chr&gt;,\n#   感染國家 &lt;chr&gt;, 確定病例數 &lt;dbl&gt;, 居住村里代碼 &lt;chr&gt;, 感染村里代碼 &lt;chr&gt;,\n#   血清型 &lt;chr&gt;, 內政部居住縣市代碼 &lt;chr&gt;, 內政部居住鄉鎮代碼 &lt;chr&gt;,\n#   內政部感染縣市代碼 &lt;chr&gt;, 內政部感染鄉鎮代碼 &lt;chr&gt;\n\n\nWe must retrieve epidemiological data for weeks 31-50 of the year 2023 from this dataset. We can utilise the lubridate package. Then we will choose the necessary columns, rename them, and save the results in a new dataframe named dengue_clean.\nThe columns to be selected and renamed are:\n\n發病日: Onset date (we need to convert to numeric)\n最小統計區中心點X: x-coordinate (Note: class is character, we need to convert to numeric)\n最小統計區中心點Y: y-coordinate (Note: class is character, we need to convert to numeric)\n居住村里: VILLNAME\n居住鄉鎮: TOWNNAME\n\nFirst let’s extract the week numbers between 31-50 from the year 2023 using the lubridate package. To do this, we first check if our columns are in the right format.\n\ndengue$發病日 &lt;- as.Date(dengue$發病日)\ndengue$week_number &lt;- as.numeric(format(dengue$發病日, \"%V\"))\n\nNote: %v is to represent the ISO 8601 week number of the year.\n\ndengue_clean &lt;- dengue %&gt;%\n  filter(week_number &gt;= 31 & week_number &lt;= 50 & year(發病日) == 2023)\n\n\ndengue_clean &lt;- dengue_clean %&gt;%\n  rename(`Onset_date` = 發病日,\n    `X_coordinate` = 最小統計區中心點X,\n    `Y_coordinate` = 最小統計區中心點Y,\n    `VILLNAME` = 居住村里,\n    `TOWNNAME` = 居住鄉鎮\n      )\n\nChange the format of X and Y coordinates to numeric data type from character.\n\ndengue_clean$`X_coordinate` &lt;- as.numeric(dengue_clean$`X_coordinate`)\ndengue_clean$`Y_coordinate` &lt;- as.numeric(dengue_clean$`Y_coordinate`)\n\nNote: NAs introduced by coercion so we remove them.\n\ndengue_clean &lt;- na.omit(dengue_clean)\n\nNow, we will convert dengue_clean to an sf object by assigning the appropriate CRS, same as counties_sf.\n\ndengue_sf &lt;- st_as_sf(dengue_clean, coords = c('X_coordinate', 'Y_coordinate'), crs = st_crs(counties_sf))\n\nWe need to identify which points from the dengue_sf object intersect with our study area defined by the counties_sf.\n\n#dengue_counties &lt;- st_intersection(counties_sf,dengue_sf)\n\n\n#write_rds(dengue_counties, \"data/rds/dengue_counties.rds\")\ndengue_counties &lt;- read_rds(\"data/rds/dengue_counties.rds\")\n\nLet’s plot and visualise these points on a map.\n\ntm_shape(counties_sf) +\n  tm_polygons(\"TOWNID\") +\ntm_shape(dengue_counties) +\n  tm_dots(col = \"black\")\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have extracted a dengue fever layer within the study area.\n\n\nOur next task is to create a derived dengue fever layer and add a data field indicating the number of dengue fever cases by village and by epidemiology week.\nTo do this, we use the group_by function and sum the count of dengue cases (確定病例數).\n\ndengue_grouped &lt;- dengue_clean %&gt;%\n  group_by(TOWNNAME, VILLNAME, week_number) %&gt;%\n  summarise(case_count = sum(確定病例數))\n\nSimultaneously, any missing values will be eliminated. Cells containing missing values are denoted by the word ‘none’. So, we will filter out rows where values are not equal to ‘none’ to eliminate missing values\n\ndengue_grouped &lt;- dengue_grouped %&gt;%\n  filter(VILLNAME != \"None\")\n\nNow we will perform a left join to include this new dengue_grouped data into our counties_sf dataframe, using townname and villname as the joining criteria.\n\ndengue_join &lt;- left_join(counties_sf, dengue_grouped, by = c(\"TOWNNAME\",\"VILLNAME\"))\n\nThe join has resulted in some NA values. To help with our subsequent analysis, we will replace all NA values with 0.\n\ndengue_join$case_count &lt;- ifelse(is.na(dengue_join$case_count), 0, dengue_join$case_count)\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have created a data field indicating the number of dengue fever cases by village and by epidemiology week."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html",
    "title": "Take-home Exercise 1b [ANALYSIS]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "",
    "text": "pacman::p_load(arrow, dplyr, lubridate, maptools, raster, rgdal, RColorBrewer, rmapshaper, sf, sp, spNetwork, spatstat, tidyverse, tmap, ggplot2, plotly)\n\n#update.packages(ask = FALSE, dependencies = TRUE)\n## install.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")\n\n\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MPSZ-2019\") %&gt;%\n    st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Take-home_Ex\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\ngrab_origin &lt;- read_rds(\"data/rds/grab_origin.rds\")\n\ngrab_origin_ppp_sg_km &lt;- read_rds(\"data/rds/grab_origin_ppp_sg_km.rds\")\n\ngrab_dest_ppp_sg_km &lt;- read_rds(\"data/rds/grab_dest_ppp_sg_km.rds\")\n\ngrab_origin_ppp_sg &lt;- read_rds(\"data/rds/grab_origin_ppp_sg.rds\")\n\ngrab_dest_ppp_sg &lt;- read_rds(\"data/rds/grab_dest_ppp_sg.rds\")\n\nsg_owin &lt;- read_rds(\"data/rds/sg_owin\")\n\nsg_roads &lt;- read_rds(\"data/rds/sg_roads.rds\")\n\n\n\nKernel Density Estimation (KDE) is a technique used in spatial point pattern analysis to estimate the density of events across a continuous space based on a set of observed point locations. It is particularly when we have a set of spatial points and want to visualise the spatial distribution of these points in a smoother and more continuous way.\nThe are different approaches for selecting the bandwidth or smoothing parameter of the kernel.\n\nautomatic bandwidth method\nfixed bandwidth method\n\n\n\n\nHere the bandwidth is determined automatically by the algorithm based on some optimisation criterion.\nThere are several spatstat functions that we can use for automatic bandwidth selection.\nThe density function allows us to compute a kernel density for a given set of point events.\nFirst, we use the bw.diggle() method.\n\nbw.diggle() Cross Validated Bandwidth Selection for Kernel Density\n\n\nkde_grab_origin_sg_bw &lt;- density(grab_origin_ppp_sg_km,\n                                   sigma=bw.diggle,\n                                   edge=TRUE,\n                                   kernel=\"gaussian\")\n\n\npar(mar = c(1,1,1,1))\nplot(kde_grab_origin_sg_bw,main = \"KDE Automatic Bandwidth for Origin Points\")\n\n\n\n\n\n\n\nkde_grab_dest_sg_bw &lt;- density(grab_dest_ppp_sg_km,\n                                   sigma=bw.diggle,\n                                   edge=TRUE,\n                                   kernel=\"gaussian\")\nkde_grab_dest_sg_bw\n\nreal-valued pixel image\n128 x 128 pixel array (ny, nx)\nenclosing rectangle: [2.6675, 55.942] x [21.448, 50.256] km\n\n\n\npar(mar = c(1,1,1,1))\nplot(kde_grab_dest_sg_bw,main = \"KDE Automatic Bandwidth for Destination Points\")\n\n\n\n\n\n\n\n\n\nAdditionally we can choose from a series of options for ‘kernel’ which is the smoothing parameter that we will explore later. Let’s explore the other methods for automatic bandwidth selection and retrieve sigma value. The sigma value tells us the amount of smoothing applied when estimating the kernel density.\n\nbw.diggle() Cross Validated Bandwidth Selection for Kernel Density\n\n\nbw_diggle &lt;- bw.diggle(grab_origin_ppp_sg_km)\nbw_diggle\n\n\nbw.CvL() Cronie and Van Lieshout’s Criterion for Bandwidth Selection for Kernel Density\n\n\nbw_CvL &lt;- bw.CvL(grab_origin_ppp_sg_km)\nbw_CvL\n\n\nbw.scott() Scott’s Rule for Bandwidth Selection for Kernel Density\n\n\nbw_scott &lt;- bw.scott(grab_origin_ppp_sg_km)\nbw_scott\n\n\nbw.ppl() Likelihood Cross Validation Bandwidth Selection for Kernel Density\n\n\nbw_ppl &lt;- bw.ppl(grab_origin_ppp_sg_km)\nbw_ppl\n\nLet’s plot to compare the output of each method, so that we can see the distinct differences in KDE layers .\n\nkde_diggle &lt;- density(grab_origin_ppp_sg_km, 0.008098505 )\nkde_CvL &lt;- density(grab_origin_ppp_sg_km, 3.147573)\nkde_scott &lt;- density(grab_origin_ppp_sg_km, 1.5925974,0.9389125)\nkde_ppl &lt;- density(grab_origin_ppp_sg_km, 0.1238747)\n\n\npar(mfrow = c(2,2), mar = c(1,1,1,1))\nplot(kde_diggle,main = \"KDE diggle\")\nplot(kde_CvL,main = \"KDE CvL\")\nplot(kde_scott,main = \"KDE Scott\")\nplot(kde_ppl,main = \"KDE ppl\")\n\n\n\n\n\n\n\nFrom first glance, it looks as though KDE Scott shows the best resuts with the clearest peaks.\nBut in order to pick the most suitable method for our analysis, we need to compare the distribution of KDE values. We can do so simply by visualising the distribution using histograms. Let’s check again KDE Scott shows the most ideal result.\n\n\npar(mfrow = c(2,2),mar = c(3,3,3,3))\nhist(kde_diggle,main = \"KDE diggle\")\nabline(v=50, \n       col=\"red\")\nhist(kde_CvL,main = \"KDE CvL\")\nabline(v=50, \n       col=\"red\")\nhist(kde_scott,main = \"KDE Scott\")\nabline(v=50, \n       col=\"red\")\nhist(kde_ppl,main = \"KDE ppl\")\nabline(v=50, \n       col=\"red\")\n\n\n\n\n\n\n\n\n\n\nLooking at the histograms, the one for bw_scott() shows that it has a broad spread as compared to the others and does not peak which means there is an even distribution of points across all bins. Therefore we will pick bw_scott().\n\n\n\n\nHere, we manually specify a fixed bandwidth value for the KDE layer. This allows us to control the level of smoothing applied to the point pattern. We will plot using bw_scott()as identified previously as the most suitable method.\n\nfixed_bw_scott &lt;- bw.scott(grab_origin_ppp_sg_km)\nfixed_bw_scott\n\n\n#write_rds(fixed_bw_scott, \"data/rds/fixed_bw_scott.rds\")\nfixed_bw_scott &lt;- read_rds(\"data/rds/fixed_bw_scott.rds\")\n\nThe values returned are 1.59 and 0.94 for sigma.x and sigma.y respectively.\nThen, we plot to visualise the fixed bandwidth using bw_scott\n\nkde_fixed_bw_scott &lt;- density(grab_origin_ppp_sg_km, 1.5925974,0.9389125)\n\n\npar(mar = c(1,1,1,1))\nplot(kde_fixed_bw_scott, main ='Scott Method Fixed Bandwidth KDE for Origin Points')\n\n\n\n\n\n\n\n\n\nThe default kernel in density.ppp() is the gaussian. There are other options such as epanechnikov, quartic and disc.\nLet’s explore the different kernel function selection methods.\n\nkde_fixed_bw_scott_gaussian &lt;- density(grab_origin_ppp_sg_km, \n                          sigma=fixed_bw_scott, \n                          edge=TRUE, \n                          kernel=\"gaussian\")\n\n\nkde_fixed_bw_scott_epanechnikov &lt;- density(grab_origin_ppp_sg_km, \n                          sigma=fixed_bw_scott, \n                          edge=TRUE, \n                          kernel=\"epanechnikov\")\n   \nkde_fixed_bw_scott_quartic &lt;- density(grab_origin_ppp_sg_km, \n                          sigma=fixed_bw_scott, \n                          edge=TRUE, \n                          kernel=\"quartic\")\n       \n   \nkde_fixed_scott_disc &lt;- density(grab_origin_ppp_sg_km, \n                          sigma=fixed_bw_scott, \n                          edge=TRUE, \n                          kernel=\"disc\")\n\nLet’s visualise the different kernel methods.\n\npar(mfrow = c(2,2), mar = c(2,2,2,2))\nplot(kde_fixed_bw_scott_gaussian, main=\"Gaussian\")\nplot(kde_fixed_bw_scott_epanechnikov, main=\"Epanechnikov\")\nplot(kde_fixed_bw_scott_quartic, main=\"Quartic\")\nplot(kde_fixed_scott_disc,main=\"Disc\")\n\n\n\n\n\n\nThere are subtle differences in the smoothness and dispersion among the four plots, but they collectively show the same pattern in the end.\n\n\n\nHere, we use the most common adaptive bandwidth method called Adaptive Kernel Density Estimate.\n\nkde_adaptive &lt;- adaptive.density(grab_origin_ppp_sg_km, method=\"kernel\")\n\n\n#write_rds(kde_adaptive, \"data/rds/kde_adaptive.rds\")\nkde_adaptive &lt;- read_rds(\"data/rds/kde_adaptive.rds\")\n\n\n\n\nLet’s do a side-by-side comparision of fixed bandwidth and adaptive bandwidth method .\n\npar(mfrow=c(1,2), mar = c(3,3,3,3))\nplot(kde_fixed_bw_scott_gaussian, main = \"Fixed bandwidth\")\nplot(kde_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\nThe fixed bandwidth method makes it easier to identify areas of higher origin point clusters\n\n\n\n\nNow let’s plot interactive KDE maps to have a closer look.\n\n\n\nWe need to convert our KDE output into grid objects for mapping purposes, here we use the raster() function.\n\nkde_fixed_bw_scott_raster &lt;- raster(kde_fixed_bw_scott)\nkde_adaptive_kernel_raster &lt;- raster(kde_adaptive)\n\nThen we perform project transformation.\n\nprojection(kde_fixed_bw_scott_raster) &lt;- CRS(\"+init=EPSG:3414 +units=km\")\nprojection(kde_adaptive_kernel_raster) &lt;- CRS(\"+init=EPSG:3414 +units=km\")\n\n\n\n\nFinally, we can visualise our Kernel Density Maps on OpenStreetMap\n\nred_palette &lt;- colorRampPalette(c(\"lightpink\", \"darkred\"))\ntmap_mode('view')\nkde_fixed_bw_scott_map &lt;- tm_basemap(\"OpenStreetMap\") +\n  tm_view(set.zoom.limits=c(10, 15)) +\n  tm_shape(kde_fixed_bw_scott_raster) +\n  tm_raster(alpha = 0.65, title = \"KDE_Fixed_Scott\", palette = red_palette(12)) +\n  tm_shape(mpsz_sf)+\n  tm_polygons(alpha=0.1, id=\"PLN_AREA_N\")+\n  tmap_options(check.and.fix = TRUE)+\n  tm_layout(title = \"Scott Method Fixed Bandwidth KDE for Origin Points\")\ntmap_leaflet(kde_fixed_bw_scott_map)\n\n\n\n\n\n\n\ntmap_mode('view')\nkde_adaptive_kernel_map &lt;- tm_basemap(\"OpenStreetMap\") +\n  tm_view(set.zoom.limits=c(10, 15)) +\n  tm_shape(kde_adaptive_kernel_raster) +\n  tm_raster(alpha = 0.65, title = \"KDE_Adaptive_Kernel\", palette = red_palette(12)) +\n  tm_shape(mpsz_sf)+\n  tm_polygons(alpha=0.1, id=\"PLN_AREA_N\")+\n  tmap_options(check.and.fix = TRUE)+\n  tm_layout(title = \"Adaptive Bandwidth KDE for Origin Points\")\ntmap_leaflet(kde_adaptive_kernel_map)\n\n\n\n\n\n\n\n\n\nLet’s begin by extracting insights from the fixed bandwidth map, where it’s notably easier to identify the high-density pickup areas represented by yellow clusters. A prominent cluster emerges in the central-south region, encompassing stations such as Newton, Orchard, Downtown East, and Rochor. These areas exhibit increased demand for Grab pickups, potentially influenced by their status as tourist attractions. The tendency for individuals to explore these locales and then opt for Grab as their origin point prompts questions about the efficacy of public transport planning in encouraging more sustainable transportation choices.\nAdditionally, a big cluster forms at Changi Airport, which is quite understandable. Travelers landing in Singapore, often fatigued and burdened with luggage, may prefer the convenience of Grab over public transportation for their journey home.\nBeyond these, smaller yet discernible clusters show up in various residential zones. Referencing the adaptive bandwidth map for precise locations, we observe clusters in the north (Choa Chu Kang, Bukit Panjang), west (Jurong West, Jurong East), east (Tampines, Pasir Ris), and north-east (Woodlands, Sembawang, Yishun). This prompts further inquiries into the connectivity of these regions to public transport networks and the factors influencing residents to choose Grab over alternative transportation modes.\nIn essence, these spatial patterns raise intriguing questions about the accessibility and appeal of public transportation in these specific areas.\n\n\n\nFrom the array of residential clusters pinpointed in the previous section, we will focus on four specific stations, each corresponding to distinct geographical regions: north, east, west, and north-east.\n\nNorth: Choa Chu Kang\nWest: Jurong East\nEast: Tampines\nNorth-East: Woodlands\n\nLet’s extract out these study areas from mpsz_sf using filter and store it in new objects.\n\nje = mpsz_sf%&gt;%\n  filter(PLN_AREA_N == \"JURONG EAST\")\ntm = mpsz_sf%&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck = mpsz_sf%&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\nwd = mpsz_sf%&gt;%\n  filter(PLN_AREA_N == \"WOODLANDS\")\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(st_geometry(je), main = \"Jurong East\")\nplot(st_geometry(tm), main = \"Tampines\")\nplot(st_geometry(ck), main = \"Choa Chu Kang\")\nplot(st_geometry(wd), main = \"Woodlands\")\n\n\n\n\n\n\nNext, we will create owin objects to represent the observation windows for respective planning area.\n\nje_owin = as.owin(je)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\nwd_owin = as.owin(wd)\n\norigin_je_ppp = grab_origin_ppp_sg[je_owin]\norigin_tm_ppp = grab_origin_ppp_sg[tm_owin]\norigin_ck_ppp = grab_origin_ppp_sg[ck_owin]\norigin_wd_ppp = grab_origin_ppp_sg[wd_owin]\n\n\n\n\nje_kde_scott &lt;- density(origin_je_ppp, sigma=bw.scott, main=\"Jurong East\")\ntm_kde_scott &lt;- density(origin_tm_ppp, sigma=bw.scott, main=\"Tampines\")\nck_kde_scott &lt;- density(origin_ck_ppp, sigma=bw.scott, main=\"Choa Chu Kang\")\nwd_kde_scott &lt;- density(origin_wd_ppp, sigma=bw.scott, main=\"Woodlands\")\n\n\npar(mfrow = c(2,2))\nplot(je_kde_scott,main = \"KDE Jurong East\")\nplot(tm_kde_scott,main = \"KDE Tampines\")\nplot(ck_kde_scott,main = \"KDE Choa Chu Kang\")\nplot(wd_kde_scott,main = \"KDE Woodlands\")\n\n\n\n\n\n\nNow, identifying clusters within each planning area is easily achievable. However, pinpointing the exact locations of these clusters requires the incorporation of road networks for each planning area. To achieve this precision, we will leverage a more advanced KDE technique known as Network Kernel Density Estimation (NKDE) in the upcoming sections. This approach will provide deeper insights into the specific roads or areas within each planning area that host these clusters."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#import-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#import-data",
    "title": "Take-home Exercise 1b [ANALYSIS]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "",
    "text": "pacman::p_load(arrow, dplyr, lubridate, maptools, raster, rgdal, RColorBrewer, rmapshaper, sf, sp, spNetwork, spatstat, tidyverse, tmap, ggplot2, plotly)\n\n#update.packages(ask = FALSE, dependencies = TRUE)\n## install.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")\n\n\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MPSZ-2019\") %&gt;%\n    st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Take-home_Ex\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\ngrab_origin &lt;- read_rds(\"data/rds/grab_origin.rds\")\n\ngrab_origin_ppp_sg_km &lt;- read_rds(\"data/rds/grab_origin_ppp_sg_km.rds\")\n\ngrab_dest_ppp_sg_km &lt;- read_rds(\"data/rds/grab_dest_ppp_sg_km.rds\")\n\ngrab_origin_ppp_sg &lt;- read_rds(\"data/rds/grab_origin_ppp_sg.rds\")\n\ngrab_dest_ppp_sg &lt;- read_rds(\"data/rds/grab_dest_ppp_sg.rds\")\n\nsg_owin &lt;- read_rds(\"data/rds/sg_owin\")\n\nsg_roads &lt;- read_rds(\"data/rds/sg_roads.rds\")\n\n\n\nKernel Density Estimation (KDE) is a technique used in spatial point pattern analysis to estimate the density of events across a continuous space based on a set of observed point locations. It is particularly when we have a set of spatial points and want to visualise the spatial distribution of these points in a smoother and more continuous way.\nThe are different approaches for selecting the bandwidth or smoothing parameter of the kernel.\n\nautomatic bandwidth method\nfixed bandwidth method\n\n\n\n\nHere the bandwidth is determined automatically by the algorithm based on some optimisation criterion.\nThere are several spatstat functions that we can use for automatic bandwidth selection.\nThe density function allows us to compute a kernel density for a given set of point events.\nFirst, we use the bw.diggle() method.\n\nbw.diggle() Cross Validated Bandwidth Selection for Kernel Density\n\n\nkde_grab_origin_sg_bw &lt;- density(grab_origin_ppp_sg_km,\n                                   sigma=bw.diggle,\n                                   edge=TRUE,\n                                   kernel=\"gaussian\")\n\n\npar(mar = c(1,1,1,1))\nplot(kde_grab_origin_sg_bw,main = \"KDE Automatic Bandwidth for Origin Points\")\n\n\n\n\n\n\n\nkde_grab_dest_sg_bw &lt;- density(grab_dest_ppp_sg_km,\n                                   sigma=bw.diggle,\n                                   edge=TRUE,\n                                   kernel=\"gaussian\")\nkde_grab_dest_sg_bw\n\nreal-valued pixel image\n128 x 128 pixel array (ny, nx)\nenclosing rectangle: [2.6675, 55.942] x [21.448, 50.256] km\n\n\n\npar(mar = c(1,1,1,1))\nplot(kde_grab_dest_sg_bw,main = \"KDE Automatic Bandwidth for Destination Points\")\n\n\n\n\n\n\n\n\n\nAdditionally we can choose from a series of options for ‘kernel’ which is the smoothing parameter that we will explore later. Let’s explore the other methods for automatic bandwidth selection and retrieve sigma value. The sigma value tells us the amount of smoothing applied when estimating the kernel density.\n\nbw.diggle() Cross Validated Bandwidth Selection for Kernel Density\n\n\nbw_diggle &lt;- bw.diggle(grab_origin_ppp_sg_km)\nbw_diggle\n\n\nbw.CvL() Cronie and Van Lieshout’s Criterion for Bandwidth Selection for Kernel Density\n\n\nbw_CvL &lt;- bw.CvL(grab_origin_ppp_sg_km)\nbw_CvL\n\n\nbw.scott() Scott’s Rule for Bandwidth Selection for Kernel Density\n\n\nbw_scott &lt;- bw.scott(grab_origin_ppp_sg_km)\nbw_scott\n\n\nbw.ppl() Likelihood Cross Validation Bandwidth Selection for Kernel Density\n\n\nbw_ppl &lt;- bw.ppl(grab_origin_ppp_sg_km)\nbw_ppl\n\nLet’s plot to compare the output of each method, so that we can see the distinct differences in KDE layers .\n\nkde_diggle &lt;- density(grab_origin_ppp_sg_km, 0.008098505 )\nkde_CvL &lt;- density(grab_origin_ppp_sg_km, 3.147573)\nkde_scott &lt;- density(grab_origin_ppp_sg_km, 1.5925974,0.9389125)\nkde_ppl &lt;- density(grab_origin_ppp_sg_km, 0.1238747)\n\n\npar(mfrow = c(2,2), mar = c(1,1,1,1))\nplot(kde_diggle,main = \"KDE diggle\")\nplot(kde_CvL,main = \"KDE CvL\")\nplot(kde_scott,main = \"KDE Scott\")\nplot(kde_ppl,main = \"KDE ppl\")\n\n\n\n\n\n\n\nFrom first glance, it looks as though KDE Scott shows the best resuts with the clearest peaks.\nBut in order to pick the most suitable method for our analysis, we need to compare the distribution of KDE values. We can do so simply by visualising the distribution using histograms. Let’s check again KDE Scott shows the most ideal result.\n\n\npar(mfrow = c(2,2),mar = c(3,3,3,3))\nhist(kde_diggle,main = \"KDE diggle\")\nabline(v=50, \n       col=\"red\")\nhist(kde_CvL,main = \"KDE CvL\")\nabline(v=50, \n       col=\"red\")\nhist(kde_scott,main = \"KDE Scott\")\nabline(v=50, \n       col=\"red\")\nhist(kde_ppl,main = \"KDE ppl\")\nabline(v=50, \n       col=\"red\")\n\n\n\n\n\n\n\n\n\n\nLooking at the histograms, the one for bw_scott() shows that it has a broad spread as compared to the others and does not peak which means there is an even distribution of points across all bins. Therefore we will pick bw_scott().\n\n\n\n\nHere, we manually specify a fixed bandwidth value for the KDE layer. This allows us to control the level of smoothing applied to the point pattern. We will plot using bw_scott()as identified previously as the most suitable method.\n\nfixed_bw_scott &lt;- bw.scott(grab_origin_ppp_sg_km)\nfixed_bw_scott\n\n\n#write_rds(fixed_bw_scott, \"data/rds/fixed_bw_scott.rds\")\nfixed_bw_scott &lt;- read_rds(\"data/rds/fixed_bw_scott.rds\")\n\nThe values returned are 1.59 and 0.94 for sigma.x and sigma.y respectively.\nThen, we plot to visualise the fixed bandwidth using bw_scott\n\nkde_fixed_bw_scott &lt;- density(grab_origin_ppp_sg_km, 1.5925974,0.9389125)\n\n\npar(mar = c(1,1,1,1))\nplot(kde_fixed_bw_scott, main ='Scott Method Fixed Bandwidth KDE for Origin Points')\n\n\n\n\n\n\n\n\n\nThe default kernel in density.ppp() is the gaussian. There are other options such as epanechnikov, quartic and disc.\nLet’s explore the different kernel function selection methods.\n\nkde_fixed_bw_scott_gaussian &lt;- density(grab_origin_ppp_sg_km, \n                          sigma=fixed_bw_scott, \n                          edge=TRUE, \n                          kernel=\"gaussian\")\n\n\nkde_fixed_bw_scott_epanechnikov &lt;- density(grab_origin_ppp_sg_km, \n                          sigma=fixed_bw_scott, \n                          edge=TRUE, \n                          kernel=\"epanechnikov\")\n   \nkde_fixed_bw_scott_quartic &lt;- density(grab_origin_ppp_sg_km, \n                          sigma=fixed_bw_scott, \n                          edge=TRUE, \n                          kernel=\"quartic\")\n       \n   \nkde_fixed_scott_disc &lt;- density(grab_origin_ppp_sg_km, \n                          sigma=fixed_bw_scott, \n                          edge=TRUE, \n                          kernel=\"disc\")\n\nLet’s visualise the different kernel methods.\n\npar(mfrow = c(2,2), mar = c(2,2,2,2))\nplot(kde_fixed_bw_scott_gaussian, main=\"Gaussian\")\nplot(kde_fixed_bw_scott_epanechnikov, main=\"Epanechnikov\")\nplot(kde_fixed_bw_scott_quartic, main=\"Quartic\")\nplot(kde_fixed_scott_disc,main=\"Disc\")\n\n\n\n\n\n\nThere are subtle differences in the smoothness and dispersion among the four plots, but they collectively show the same pattern in the end.\n\n\n\nHere, we use the most common adaptive bandwidth method called Adaptive Kernel Density Estimate.\n\nkde_adaptive &lt;- adaptive.density(grab_origin_ppp_sg_km, method=\"kernel\")\n\n\n#write_rds(kde_adaptive, \"data/rds/kde_adaptive.rds\")\nkde_adaptive &lt;- read_rds(\"data/rds/kde_adaptive.rds\")\n\n\n\n\nLet’s do a side-by-side comparision of fixed bandwidth and adaptive bandwidth method .\n\npar(mfrow=c(1,2), mar = c(3,3,3,3))\nplot(kde_fixed_bw_scott_gaussian, main = \"Fixed bandwidth\")\nplot(kde_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\nThe fixed bandwidth method makes it easier to identify areas of higher origin point clusters\n\n\n\n\nNow let’s plot interactive KDE maps to have a closer look.\n\n\n\nWe need to convert our KDE output into grid objects for mapping purposes, here we use the raster() function.\n\nkde_fixed_bw_scott_raster &lt;- raster(kde_fixed_bw_scott)\nkde_adaptive_kernel_raster &lt;- raster(kde_adaptive)\n\nThen we perform project transformation.\n\nprojection(kde_fixed_bw_scott_raster) &lt;- CRS(\"+init=EPSG:3414 +units=km\")\nprojection(kde_adaptive_kernel_raster) &lt;- CRS(\"+init=EPSG:3414 +units=km\")\n\n\n\n\nFinally, we can visualise our Kernel Density Maps on OpenStreetMap\n\nred_palette &lt;- colorRampPalette(c(\"lightpink\", \"darkred\"))\ntmap_mode('view')\nkde_fixed_bw_scott_map &lt;- tm_basemap(\"OpenStreetMap\") +\n  tm_view(set.zoom.limits=c(10, 15)) +\n  tm_shape(kde_fixed_bw_scott_raster) +\n  tm_raster(alpha = 0.65, title = \"KDE_Fixed_Scott\", palette = red_palette(12)) +\n  tm_shape(mpsz_sf)+\n  tm_polygons(alpha=0.1, id=\"PLN_AREA_N\")+\n  tmap_options(check.and.fix = TRUE)+\n  tm_layout(title = \"Scott Method Fixed Bandwidth KDE for Origin Points\")\ntmap_leaflet(kde_fixed_bw_scott_map)\n\n\n\n\n\n\n\ntmap_mode('view')\nkde_adaptive_kernel_map &lt;- tm_basemap(\"OpenStreetMap\") +\n  tm_view(set.zoom.limits=c(10, 15)) +\n  tm_shape(kde_adaptive_kernel_raster) +\n  tm_raster(alpha = 0.65, title = \"KDE_Adaptive_Kernel\", palette = red_palette(12)) +\n  tm_shape(mpsz_sf)+\n  tm_polygons(alpha=0.1, id=\"PLN_AREA_N\")+\n  tmap_options(check.and.fix = TRUE)+\n  tm_layout(title = \"Adaptive Bandwidth KDE for Origin Points\")\ntmap_leaflet(kde_adaptive_kernel_map)\n\n\n\n\n\n\n\n\n\nLet’s begin by extracting insights from the fixed bandwidth map, where it’s notably easier to identify the high-density pickup areas represented by yellow clusters. A prominent cluster emerges in the central-south region, encompassing stations such as Newton, Orchard, Downtown East, and Rochor. These areas exhibit increased demand for Grab pickups, potentially influenced by their status as tourist attractions. The tendency for individuals to explore these locales and then opt for Grab as their origin point prompts questions about the efficacy of public transport planning in encouraging more sustainable transportation choices.\nAdditionally, a big cluster forms at Changi Airport, which is quite understandable. Travelers landing in Singapore, often fatigued and burdened with luggage, may prefer the convenience of Grab over public transportation for their journey home.\nBeyond these, smaller yet discernible clusters show up in various residential zones. Referencing the adaptive bandwidth map for precise locations, we observe clusters in the north (Choa Chu Kang, Bukit Panjang), west (Jurong West, Jurong East), east (Tampines, Pasir Ris), and north-east (Woodlands, Sembawang, Yishun). This prompts further inquiries into the connectivity of these regions to public transport networks and the factors influencing residents to choose Grab over alternative transportation modes.\nIn essence, these spatial patterns raise intriguing questions about the accessibility and appeal of public transportation in these specific areas.\n\n\n\nFrom the array of residential clusters pinpointed in the previous section, we will focus on four specific stations, each corresponding to distinct geographical regions: north, east, west, and north-east.\n\nNorth: Choa Chu Kang\nWest: Jurong East\nEast: Tampines\nNorth-East: Woodlands\n\nLet’s extract out these study areas from mpsz_sf using filter and store it in new objects.\n\nje = mpsz_sf%&gt;%\n  filter(PLN_AREA_N == \"JURONG EAST\")\ntm = mpsz_sf%&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck = mpsz_sf%&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\nwd = mpsz_sf%&gt;%\n  filter(PLN_AREA_N == \"WOODLANDS\")\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(st_geometry(je), main = \"Jurong East\")\nplot(st_geometry(tm), main = \"Tampines\")\nplot(st_geometry(ck), main = \"Choa Chu Kang\")\nplot(st_geometry(wd), main = \"Woodlands\")\n\n\n\n\n\n\nNext, we will create owin objects to represent the observation windows for respective planning area.\n\nje_owin = as.owin(je)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\nwd_owin = as.owin(wd)\n\norigin_je_ppp = grab_origin_ppp_sg[je_owin]\norigin_tm_ppp = grab_origin_ppp_sg[tm_owin]\norigin_ck_ppp = grab_origin_ppp_sg[ck_owin]\norigin_wd_ppp = grab_origin_ppp_sg[wd_owin]\n\n\n\n\nje_kde_scott &lt;- density(origin_je_ppp, sigma=bw.scott, main=\"Jurong East\")\ntm_kde_scott &lt;- density(origin_tm_ppp, sigma=bw.scott, main=\"Tampines\")\nck_kde_scott &lt;- density(origin_ck_ppp, sigma=bw.scott, main=\"Choa Chu Kang\")\nwd_kde_scott &lt;- density(origin_wd_ppp, sigma=bw.scott, main=\"Woodlands\")\n\n\npar(mfrow = c(2,2))\nplot(je_kde_scott,main = \"KDE Jurong East\")\nplot(tm_kde_scott,main = \"KDE Tampines\")\nplot(ck_kde_scott,main = \"KDE Choa Chu Kang\")\nplot(wd_kde_scott,main = \"KDE Woodlands\")\n\n\n\n\n\n\nNow, identifying clusters within each planning area is easily achievable. However, pinpointing the exact locations of these clusters requires the incorporation of road networks for each planning area. To achieve this precision, we will leverage a more advanced KDE technique known as Network Kernel Density Estimation (NKDE) in the upcoming sections. This approach will provide deeper insights into the specific roads or areas within each planning area that host these clusters."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#network-constrained-kernel-density-estimation-nkde",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#network-constrained-kernel-density-estimation-nkde",
    "title": "Take-home Exercise 1b [ANALYSIS]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "5.0 Network Constrained Kernel Density Estimation (NKDE)",
    "text": "5.0 Network Constrained Kernel Density Estimation (NKDE)\nNetwork Kernel Density Estimation (NKDE) is an advanced technique that builds upon the traditional KDE method.\nWe employ NKDE because the distribution of origin points in our planning areas is not randomly distributed; it is significantly influenced by network structures. NKDE allows us to incorporate this network context, providing a more accurate representation of spatial patterns, particularly along roadways. By considering the connectivity and pathways of the network, NKDE enhances our ability to capture the nuanced distribution of clusters within each planning area, leading to more insightful and precise spatial analysis results\nThe main difference between the KDE and NKDE is that KDE treats space as a continuous field, and overlooks the underlying network structure, such as roads. NKDE takes into account the network structure, and recognises that spatial relationships may be constrained by the existing road infrastructure. Therefore, NKDE offers better localization of clusters by considering the connectivity and pathways of the network.\nHere, we will use spNetwork to create NKDE maps for each of our planning areas and explore what insights we can yield from each."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#extract-origin-points-and-road-network-of-our-study-areas",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#extract-origin-points-and-road-network-of-our-study-areas",
    "title": "Take-home Exercise 1b [ANALYSIS]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "5.1 Extract origin points and road network of our study areas",
    "text": "5.1 Extract origin points and road network of our study areas\nFirstly we need to extract the road networks within each of our study area by using st_intersection. We also use st_union to combine all the geometries of each object into a single geometry.\n\nje_roads = st_intersection(sg_roads,st_union(je))\ntm_roads = st_intersection(sg_roads,st_union(tm))\nck_roads = st_intersection(sg_roads,st_union(ck))\nwd_roads = st_intersection(sg_roads,st_union(wd))\n\nAfter that, we will use st_intersection again to find the origin spots that intersect with our planning areas.\n\nje_origin = st_intersection(grab_origin,st_union(je))\ntm_origin = st_intersection(grab_origin,st_union(tm))\nck_origin = st_intersection(grab_origin,st_union(ck))\nwd_origin = st_intersection(grab_origin,st_union(wd))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#lixels",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#lixels",
    "title": "Take-home Exercise 1b [ANALYSIS]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "5.2 Lixels",
    "text": "5.2 Lixels\nEach network edge is divided into lixels which represent the lines of the network. To get the lixels of each planning area we use the st_cast function first to convert the geometry types of the features in our object to LINESTRING.\n\nje_roads &lt;- st_cast(je_roads, \"LINESTRING\")\ntm_roads &lt;- st_cast(tm_roads, \"LINESTRING\")\nck_roads &lt;- st_cast(ck_roads, \"LINESTRING\")\nwd_roads &lt;- st_cast(wd_roads, \"LINESTRING\")\n\nAfter we have converted the geometry type of our planning areas, we use lixelize_lines to create lixels from a set of road lines represented by the planning area objects. The road lines are divided into lixels, each with a length of 750 units. mindist represents the minimum distance between lixels to ensure that resulting lixels are not too close to each other. If the length of the resulting lixel is less than the specified minimum distance, it is combined with the previous lixel.\n\nje_lixels &lt;- lixelize_lines(je_roads, \n                         750, \n                         mindist = 375)\n\ntm_lixels &lt;- lixelize_lines(tm_roads, \n                         750, \n                         mindist = 375)\n\nck_lixels &lt;- lixelize_lines(ck_roads, \n                         750, \n                         mindist = 375)\n\nwd_lixels &lt;- lixelize_lines(wd_roads, \n                         750, \n                         mindist = 375)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#line-center",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#line-center",
    "title": "Take-home Exercise 1b [ANALYSIS]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "5.3 Line Center",
    "text": "5.3 Line Center\nThen we extract the centers of the lixels using lines_center. These serve as the locations for intensity estimation.\n\nje_samples &lt;- lines_center(je_lixels)\ntm_samples &lt;- lines_center(tm_lixels)\nck_samples &lt;- lines_center(ck_lixels)\nwd_samples &lt;- lines_center(wd_lixels)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#using-simple-method-to-compute-nkde",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#using-simple-method-to-compute-nkde",
    "title": "Take-home Exercise 1b [ANALYSIS]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "5.4 Using Simple Method to Compute NKDE",
    "text": "5.4 Using Simple Method to Compute NKDE\nFinally we use the nkde function from spNetwork to get the NKDE. There are several parameters that we can define\n\nevents: the event associated with the analysis\nw: weight vector, creates a vector of ones with a length equal to the number of rows in the planning area data frame\nsamples: samples used for density estimation\nkernel_name: type of kernel to be used\nbw: determines the scale of influence for each point in the density estimation\ndiv: the method used to determine the bandwidth\nmethod: method used for density estimation\ndigits: number of significant digits displayed in the output\ntol: tolerance level for convergence in iterative algorithms\ngrid_shape: shape of the grid for calculating the density\nmax_depth: Maximum depth of the tree when building the spatial index\nagg: number of points aggregated into each grid cell\nsparse: whether to use sparse matrix representation for efficiency\nverbose: suppresses verbose output during the process\n\n\nje_density &lt;- nkde(je_roads, \n                  events = je_origin,\n                  w = rep(1,nrow(je_origin)),\n                  samples = je_samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\ntm_density &lt;- nkde(tm_roads, \n                  events = tm_origin,\n                  w = rep(1,nrow(tm_origin)),\n                  samples = tm_samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\nck_density &lt;- nkde(ck_roads, \n                  events = ck_origin,\n                  w = rep(1,nrow(ck_origin)),\n                  samples = ck_samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\nwd_density &lt;- nkde(wd_roads, \n                  events = wd_origin,\n                  w = rep(1,nrow(wd_origin)),\n                  samples = wd_samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\n#write_rds(je_density, \"data/rds/je_density.rds\")\nje_density &lt;- read_rds(\"data/rds/je_density.rds\")\n\n#write_rds(tm_density, \"data/rds/tm_density.rds\")\ntm_density &lt;- read_rds(\"data/rds/tm_density.rds\")\n\n#write_rds(ck_density, \"data/rds/ck_density.rds\")\nck_density &lt;- read_rds(\"data/rds/ck_density.rds\")\n\n#write_rds(wd_density, \"data/rds/wd_density.rds\")\nwd_density &lt;- read_rds(\"data/rds/wd_density.rds\")\n\nWe are also required to join the density values into the samples and lixels objects. The obtained densities will be scaled by the total number of origin points and we will multiply by 1000 for km measurement.\n\nje_samples$density &lt;- je_density*nrow(je_origin)*1000\nje_lixels$density &lt;- je_density*nrow(je_origin)*1000\n\ntm_samples$density &lt;- tm_density*nrow(tm_origin)*1000\ntm_lixels$density &lt;- tm_density*nrow(tm_origin)*1000\n\nck_samples$density &lt;- ck_density*nrow(ck_origin)*1000\nck_lixels$density &lt;- ck_density*nrow(ck_origin)*1000\n\nwd_samples$density &lt;- wd_density*nrow(wd_origin)*1000\nwd_lixels$density &lt;- wd_density*nrow(wd_origin)*1000"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#plotting-maps-for-different-study-areas",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#plotting-maps-for-different-study-areas",
    "title": "Take-home Exercise 1b [ANALYSIS]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "5.5 Plotting Maps for Different Study Areas",
    "text": "5.5 Plotting Maps for Different Study Areas\nLet’s plot the NKDE map for each planning area.\n\n5.5.1 Jurong East\n\ntmap_mode('view')\nje_density_map &lt;- tm_basemap(\"OpenStreetMap\") +\ntm_shape(je_lixels)+\n  tm_lines(col =\"density\", lwd = 3, palette = red_palette(12))+\ntm_shape(je_origin)+\n  tm_dots(size=0.05)+\n  tm_layout(title = \"Jurong East NKDE\")\ntmap_leaflet(je_density_map)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnalysis\nIn Jurong East, notable clusters of Grab pick-up points have been observed around Yuhua Place, Yuhua Senior Activity Center, New Jurong Polyclinic, and the nursing home vicinity. Additionally, clusters are prevalent near Parc Oasis, Singtel, Zai Shun Seafood, in proximity to Toh Guan, Westgate, Shen Hong Temple, Jurong East Interchange, IMM, Yuhua Primary School, and Crest Secondary School.\nThis clustering phenomenon may be attributed to\n\nCommercial Hubs: Areas like Westgate and IMM are major commercial centers, attracting a higher demand for Grab services. For example, people may book Grab to pick them up after they finish shopping in these areas.\nHealthcare Facilities: Proximity to healthcare facilities such as the New Jurong Polyclinic and nursing homes may lead to increased transportation needs. For example, patients or visitors might utilise Grab for convenient travel from medical appointments, contributing to the clustering effect around healthcare establishments.\nEducational Institutions: The presence of schools like Yuhua Primary School and Crest Secondary School could contribute to higher Grab demand during school-related activities. For example, parents may pick up their children after school and book Grab to their residence.\nTransportation Hubs: Jurong East Interchange serves as a transportation hub, leading to concentrated Grab activity in the area. Commuters arriving at or departing from the interchange might prefer Grab for last-mile connectivity, resulting in a clustering effect around this transportation hub.\nRecreational Areas: Clusters around Parc Oasis and Yuhua Senior Activity Center may be influenced by recreational and leisure activities. Caregivers visiting seniors at the Senior Activity Center may book Grab from there to their homes.\nCultural and Religious Centers: Locations like Shen Hong Temple may attract Grab pick-ups during events or gatherings. Attendees of cultural or religious events may use Grab for transportation from these centers.\nResidential Areas: Proximity to residential areas like Yuhua Place may result in frequent Grab pickup requests for residents. Residents in these areas might regularly utilise Grab for daily commuting or transportation needs.\nCulinary Hotspots: Clusters around Zai Shun Seafood restaurant may be influenced by popular dining establishments, drawing people to the area and making them book Grab from there to their journey back.\n\n\n\n\n\n5.5.2 Tampines\n\ntmap_mode('view')\ntm_density_map &lt;- tm_basemap(\"OpenStreetMap\") +\ntm_shape(tm_lixels)+\n  tm_lines(col =\"density\", lwd = 3, palette = red_palette(12))+\ntm_shape(tm_origin)+\n  tm_dots(size=0.05)+\n  tm_layout(title = \"Tampines NKDE\")\ntmap_leaflet(tm_density_map)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnalysis\nIn Tampines, clusters of Grab pick-up points are notable around Our Tampines Hub, Tanah Merah Country Club, opposite Tampines Neighbourhood Police Center, opposite Ngee Ann Secondary School, Tampines East, Laguna Country Club, and East Coast Park.\nThis clustering phenomenon may be attributed to\nRecreational Clubs: Areas like Tanah Merah Country Club and Laguna Country Club may contribute to clusters of Grab pick-up points. Individuals visiting these recreational clubs for leisure activities may opt for Grab services for their journey back from here for convenience.\nPolice Center: The area opposite Tampines Neighbourhood Police Center may attract Grab pick-ups due visitors requiring transportation from the area.\nCommunity and Recreational Center: Our Tampines Hub is a central community and recreational center, leading to higher demand for Grab services. People utilising the various facilities may book Grab for their journeys back.\nEducational Institutions: The area opposite Ngee Ann Secondary School could contribute to higher Grab demand during school-related activities. For example, parents may pick up their children after school and book Grab to their residence.\nResidential Areas: Tampines East, being a residential area, may result in frequent Grab pickup requests for residents. Residents in these areas might regularly utilise Grab for daily commuting or transportation needs.\nRecreational Destination: East Coast Park, a popular recreational area, may attract individuals for outdoor activities. Visitors to the park may choose to book Grab for their journey back home, contributing to the clustering effect.\n\n\n\n\n5.5.3 Choa Chu Kang\n\ntmap_mode('view')\nck_density_map &lt;- tm_basemap(\"OpenStreetMap\") +\ntm_shape(ck_lixels)+\n  tm_lines(col =\"density\", lwd = 3, palette = red_palette(12))+\ntm_shape(ck_origin)+\n  tm_dots(size=0.05)+\n  tm_layout(title = \"Choa Chu Kang NKDE\")\ntmap_leaflet(ck_density_map)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnalysis\nIn Choa Chu Kang, clusters of Grab pick-up points can be seen around Inn See Temple, Choa Chu Kang Interchange, SAFRA@CCK, Gain City, Lot One, Keat Hong Colours, Phoenix Station, Bukit Panjang Post Office, Yew Mei Condominium, MWS Nursing Home, and Yew Tee Point.\nThis clustering phenomenon may be attributed to\n\nCultural and Religious Centers: Inn See Temple’s vicinity may attract Grab pick-ups during religious events, contributing to the observed cluster.\nTransportation Hub: Choa Chu Kang Interchange serves as a transportation hub, leading to concentrated Grab activity in the area. Commuters arriving at or departing from the interchange might prefer Grab for last-mile connectivity, resulting in a clustering effect around this transportation hub.\nRecreational Facility: The presence of SAFRA@CCK could contribute to increased Grab pickup activity, especially after events and recreational activities hosted at the facility.\nShopping Centers: Yew Tee Point, Gain City and Lot One, being prominent shopping destinations, may experience a higher level of Grab pick-up points as shoppers prefer convenient transportation after their shopping sprees.\nResidential Areas: Clusters around Keat Hong Colours, and Yew Mei Condominium may be attributed to the residential nature of these areas, with residents relying on Grab for commuting needs.\nPostal Area: Bukit Panjang Post Office may attract Grab pick-ups, for individuals who have completed their postal services or nearby activities.\nHealthcare: Presence of MWS Nursing Home may lead to increased transportation needs. Caregivers visiting seniors at the nursing home may book Grab from there to their homes.\n\n\n\n\n\n5.5.4 Woodlands\n\ntmap_mode('view')\nwd_density_map &lt;- tm_basemap(\"OpenStreetMap\") +\ntm_shape(wd_lixels)+\n  tm_lines(col =\"density\", lwd = 3, palette = red_palette(12))+\ntm_shape(wd_origin)+\n  tm_dots(size=0.05)+\n  tm_layout(title = \"Woodlands NKDE\")\ntmap_leaflet(wd_density_map)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnalysis\nIn Woodlands, distinct clusters of Grab pick-up points have been identified around Innova Junior College, Singapore Sports School, Civic Centre, STELLAR@TE2, Singapore Turf Club, Old Woodlands Town Centre, Masjid An Nur, Woodlands Cinema, Greenwood Primary School, and Mega@Woodlands.\nThis clustering phenomenon may be attributed to\n\nEducational Institutions: Clusters around Innova Junior College and Singapore Sports School could be attributed to the presence of these educational institutions. Students, staff, and visitors may opt for Grab for convenient transportation from these locations.\nCivic and Community Center: Civic Centre, being a civic and community hub, may experience higher demand for Grab pickup services.\nCommercial Hub: STELLAR@TE2’s may have individuals possibly relying on Grab for commuting on their journey back from here.\nSports and Recreation: The presence of Singapore Turf Club may contribute to the clustering effect, with people choosing Grab for transportation from sports and recreational activities.\nCommercial and Residential Hub: Old Woodlands Town Centre’s central location may attract Grab pick-ups from both commercial and residential areas.\nReligious Center: Masjid An Nur’s may witness increased Grab activity during religious events.\nEntertainment Venue: Woodlands Cinema’s indicates a potential concentration of Grab pick-up points, especially after movie screenings, as patrons opt for Grab for their journey home.\nEducational Facility: Greenwood Primary School’s location could contribute to higher Grab demand during school-related activities. For example, parents may pick up their children after school and book Grab to their residence."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#conclusion",
    "title": "Take-home Exercise 1b [ANALYSIS]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "6.0 Conclusion",
    "text": "6.0 Conclusion\nThrough this take-home exercise, we delved into Spatial Point Patterns Analysis to unravel the geographical distribution of Grab Hailing Services in Singapore. Our analyses provided valuable insights, unveiling patterns such as the peak day and time for Grab pick-ups, the specific locations where these pickups occur, hotspots, and the particular roads that witness heightened activity. This information serves as a strategic tool for better planning and decision-making.\nFor instance, we can leverage these findings to enhance public transport accessibility. Understanding the road networks with the highest Grab pick-up activity allows us to identify areas where improved public transportation services could be implemented. This strategic planning aims to encourage people to opt for public transport, contributing to even pollution reduction by minimising car usage.\nIn our future endeavors, we can expand our exploration by delving into temporal Network Kernel Density Estimation (NKDE). This advanced analysis will enable us to scrutinise the intricate relationship between time and Grab Hailing Services. By identifying popular pick-up points during specific time intervals, we can propose strategic interventions, such as increasing the number of buses, adjusting bus frequencies, or implementing targeted measures to enhance transportation infrastructure in those areas. This forward-looking approach ensures a nuanced understanding of temporal patterns and facilitates more informed decisions for optimizing transportation services in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#references",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01b.html#references",
    "title": "Take-home Exercise 1b [ANALYSIS]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "7.0 References",
    "text": "7.0 References\n\nKam, T. S. (2022). R for Geospatial Data Science and Analytics. Retrieved from https://r4gdsa.netlify.app.\nGimond (2023). Chapter 11 Point Pattern Analysis. Retrieved from https://mgimond.github.io/Spatial/index.html.\nRey, S.J., Arribas-Bel, D., & Wolf, L.J. (2023). Point Pattern Analysis. In: Geographic Data Science with python. CRC Press.\nMoraga, P. Spatial Statistics for Data Science: Theory and Practice with R. Retrieved from https://www.paulamoraga.com/book-spatial/spatial-point-patterns.html."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#setting-the-scene",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#setting-the-scene",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "1.1 Setting the scene",
    "text": "1.1 Setting the scene\nUnderstanding how people move around in a city is like figuring out its heartbeat—it shows us the rhythms that shape our urban lives. Thanks to smartphones and technology, we now have a bunch of data about how people move. When we use smart analysis tools like GIS, we can unlock valuable insights that help us plan cities better.\nIn 2020, GRAB shared a set of data called Grab Posisi, all about how people move around in Singapore. This kind of information isn’t just interesting; it’s super helpful for businesses, people who make decisions about the city, and those who plan how cities work. It’s like having a dynamic picture of how people move, helping us create cities that work well for everyone."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#objectives",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#objectives",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "1.2 Objectives",
    "text": "1.2 Objectives\nThe objectives of this take-home exercise are to:\n\nApply geospatial analytics to address societal challenges\nUse spatial point patterns analysis methods to explore Grab hailing services distribution in Singapore\nOrganise geospatial data into sf tibble data.frames using sf and tidyverse functions\nFocus on Grab taxi location points, road layer within Singapore, and Singapore coastal boundary layer\nGenerate traditional Kernel Density Estimation layers\nCreate Network Kernel Density Estimation (NKDE)\nUtilise tmap functions to display kernel density layers on OSM\nDescribe spatial patterns revealed by the kernel density maps\n\nBy this exercise, I will:\n\nEnhance my understanding of geospatial analytics applications\nDevelop proficiency in spatial point patterns analysis\nGain hands-on experience in dealing with geospatial data\nExplore Grab hailing services distribution patterns in Singapore\nGenerate and interpret Kernel Density Estimation layers\nUnderstand the nuances of Network Kernel Density Estimation (NKDE)\nLearn about the visualisation of spatial patterns using tmap functions on OSM"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#getting-started",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "2 Getting Started",
    "text": "2 Getting Started"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-acquisition",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-acquisition",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "2.1 Data Acquisition",
    "text": "2.1 Data Acquisition\nThe study will utilise the following datasets to explore spatial point patterns analysis methods and reveal the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore.\n\n\n\nDataset Name\nType\nSource\nPath\n\n\n\n\nGrab-Posisi\nAspatial (.parquet)\nhttps://engineering.grab.com/grab-posisi\ndata/aspatial/grabPosisi\n\n\nMaster Plan 2019 Subzone Boundary (No Sea)\nGeospatial (.shp)\nhttps://beta.data.gov.sg/collections/2104/view\ndata/geospatial/MPSZ-2019\n\n\nOpen Street Map Road Data\nGeospatial (.shp)\nhttps://download.geofabrik.de/asia/malaysia-singapore-brunei.html\ndata/geospatial/OSM/gis_osm_roads_free_1"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-relevant-r-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-relevant-r-packages",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "2.2 Importing Relevant R Packages",
    "text": "2.2 Importing Relevant R Packages\nThe R packages used in this project are:\n\narrow: for reading and writing Parquet files\ndplyr: for data manipulation\nlubridate: for working with date-time data\nmaptools: set of tools for reading and manipulating spatial data formats, such as shapefiles\nraster: reads, writes, manipulates, analyses, and models gridded spatial data\nrgdal: from CRAN, enables users to import, export, and manipulate spatial data within the R environment\nRcolorBrewer: package providing color schemes for maps and other visualizations\nrmapshaper: a package for simplifying and modifying geographic shapes in R\nsf: for importing, managing, and processing geospatial data\nspNetwork: to perform spatial analysis for NKDE\nspatstat: for performing spatial point patterns analysis\ntidyverse: a family of other R packages for performing data science tasks such as importing, wrangling, and visualizing data\ntmap: creating static and interactive maps\nggplot2: used for data visualization\nplotly: interactive graphing library for R\n\nPacman assists us by helping us load R packages that we require.\n\n pacman::p_load(arrow, dplyr, lubridate, maptools, raster, rgdal, RColorBrewer, rmapshaper, sf, sp, spNetwork, spatstat, tidyverse, tmap, ggplot2, plotly)\n\n#update.packages(ask = FALSE, dependencies = TRUE)\n## install.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-geospatial-datasets",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-geospatial-datasets",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "2.3 Importing Geospatial Datasets",
    "text": "2.3 Importing Geospatial Datasets\n\n2.3.1 Master Plan 2019 Subzone Boundary (No Sea)\nFor shapefile format, two arguments are required: dsn to define the data path, and layer to provide the shapefile name. \n\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MPSZ-2019\") %&gt;%\n    st_transform(crs = 3414)\n\n\n\n\n\n\n\nImportant\n\n\n\nProject Transformation\nGiven that our dataset corresponds to the geographical boundaries of Singapore, it is necessary to specify the appropriate CRS for accurate spatial analysis. To achieve this, the st_transform() function is used to convert the CRS of mpsz_sf to SVY21 (EPSG: 3414)\n\n\n\n\n2.3.2 Coastal Outline\nIn order to create a costal outline of singapore, we will use the st_union function to consolidate all subzone boundaries from mpsz_sf into a single polygon.\n\noutline = mpsz_sf %&gt;% st_union()\nplot(outline)\n\n\n\n2.3.2.1 Extracting Outer Islands\nAs seen in the figure above, the coastal outline includes outer islands where Grab service is unavailable. Through the code chunk below, we use the subset function to select subzones from the mpsz_sf dataset to exclude. These excluded rows of data are stored in new dataframes.\n\nsemakau &lt;- subset(mpsz_sf,mpsz_sf$SUBZONE_N == \"SEMAKAU\") #western island\nsudong &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"SUDONG\") #western island\nbukom &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N ==  \"JURONG ISLAND AND BUKOM\")\nnorth &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"NORTH-EASTERN ISLANDS\")\nsouth &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"SOUTHERN GROUP\")\n\nWe combine the newly created dataframes using the bind_rows function and store the data in another dataframe called outer.\n\nouter &lt;- dplyr::bind_rows(list(semakau,sudong,bukom,north,south))\n\n\n\n2.3.2.2 Rendering a Coastal Boundary Excluding Outer Islands\nWe use the st_union function again to merge the geometries of mpsz_sf and outer, and the st_difference function to eliminate the overlap between the two layers. This results in a coastal boundary, sg_sf, which excludes the outer islands.\n\nsg_sf &lt;- st_difference(st_union(mpsz_sf), st_union(outer))\nplot(sg_sf)\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have obtained the coastal boundary layer of Singapore, excluding the outer islands.\n\n\n\n#write_rds(sg_sf, \"data/rds/sg_sf.rds\")\nsg_sf &lt;- read_rds(\"data/rds/sg_sf.rds\")\n\n\n\n2.3.3 Open Street Map Road Data\nLet’s import the road data obtained from OSM.\n\nallroads = st_read(dsn = \"data/geospatial/OSM\", \n                         layer = \"gis_osm_roads_free_1\")  %&gt;% st_transform(crs = 3414)\n\n\n\n\n\n\n\nImportant\n\n\n\nProject Transformation\nGiven that our dataset corresponds to the geographical boundaries of Singapore, it’s necessary to specify the appropriate CRS for accurate spatial analysis. To achieve this, the st_transform() function is used to convert the CRS of allroads to SVY21 (EPSG: 3414)\n\n\nThis dataset encompasses road networks spanning Singapore, Malaysia, and Brunei. To narrow our focus, we will extract roads exclusively within the Singapore boundary using the st_intersection function to check the intersection between sg_sf and allroads.\n\nsg_roads_all &lt;- st_intersection(allroads,sg_sf)\n\n\n#write_rds(sg_roads_all, \"data/rds/sg_roads_all.rds\")\nsg_roads_all &lt;- read_rds(\"data/rds/sg_roads_all.rds\")\n\nThe next step involves examining the various road types within the road network.\n\nunique(sg_roads_all$fclass)\n\nGiven our focus on Grab services, which primarily operate on roads excluding expressways (it is not possible for Grab to pick-up or drop off passengers along expressways), we will extract the relevant road types from the road network. To determine which roads are applicable, we can look at the OSM fclass. The image below provides descriptions for each road type, and for our analysis, we will focus on the most relevant types: primary, residential, tertiary, service, secondary, primary_link, secondary_link, and tertiary_link\n\nWe store the selected road types in a dataframe called sg_roads_filtered.\n\nsg_roads_filtered &lt;- c(\"primary\", \"residential\", \"tertiary\", \"service\", \"secondary\", \"primary_link\", \"secondary_link\", \"tertiary_link\")\n\n\nsg_roads &lt;- sg_roads_all[sg_roads_all$fclass %in% sg_roads_filtered, ]\n\nunique(sg_roads$fclass)\n\nWe will now visualise the selected road types within the boundaries of Singapore using tmap.\n\nroad_type_palette &lt;- brewer.pal(12, \"Set3\")\ntmap_mode('view')\ntm_shape(sg_sf) + \n  tm_borders(lwd = 2, col = 'grey') +\n  tm_shape(sg_roads) + \n  tm_lines(col = \"fclass\", palette = road_type_palette)\n  tm_layout(frame = FALSE, main.title = \"Types of Road Networks in Singapore\")\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have extracted the road layer within Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-aspatial-datasets",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-aspatial-datasets",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "2.4 Importing Aspatial Datasets",
    "text": "2.4 Importing Aspatial Datasets\n\n2.4.1 Grab-Posisi\nGrab-Posisi, is a GPS trajectory dataset. Each trajectory is serialised in a file in Apache Parquet format.\nWe will use the read_parquet function from the arrow package, to read Parquet files.\n\ngrab0 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00000.parquet\", as_data_frame = TRUE)\ngrab1 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00001.parquet\", as_data_frame = TRUE)\ngrab2 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00002.parquet\", as_data_frame = TRUE)\ngrab3 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00003.parquet\", as_data_frame = TRUE)\ngrab4 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00004.parquet\", as_data_frame = TRUE)\ngrab5 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00005.parquet\", as_data_frame = TRUE)\ngrab6 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00006.parquet\", as_data_frame = TRUE)\ngrab7 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00007.parquet\", as_data_frame = TRUE)\ngrab8 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00008.parquet\", as_data_frame = TRUE)\ngrab9 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00009.parquet\", as_data_frame = TRUE)\n\nThen we join all the read files into one dataframe.\n\ngrab &lt;- bind_rows(grab0,grab1, grab2,grab3,grab4,grab5,grab6,grab7,grab8,grab9) \n\n\nhead(grab)\n\nThe head function reveals that there are 9 columns in the dataframe.\nThe field pingtimestamp is not in proper date-time format. It is stored as an int value. The following code chunk converts the data type of pingtimestamp from int to date-time format.\n\ngrab$pingtimestamp &lt;- as_datetime(grab$pingtimestamp)\n\n\n\n2.4.1.2 Converting Aspatial Data Frame into a Simple Feature Data Frame\nWe will proceed to convert the grab dataset, currently in an aspatial data frame, into an sf tibble dataframe.\n\ngrab_sf &lt;- st_as_sf(grab, \n                       coords = c(\"rawlng\", \"rawlat\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n# write_rds(grab_sf, \"data/rds/grab_sf.rds\")\ngrab_sf &lt;- read_rds(\"data/rds/grab_sf.rds\")\n\n\n\n\n\n\n\nImportant\n\n\n\nProject Transformation\nAssuming the dataset is initially in the WGS84 Geographic Coordinate System, as indicated by the latitude/longitude fields, we need to define the suitable CRS for spatial analysis within Singapore. The st_transform() function is utilised to convert the CRS of the grab dataset to SVY21 (EPSG: 3414).\n\n\nThis gives us the new simple feature data frame, grab_sf"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-wrangling",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "2.5 Data Wrangling",
    "text": "2.5 Data Wrangling\n\n2.5.1 Extracting Grab Trips Starting Locations\nWe will extract trip starting point for all unqiue trajectories and store them to a new df named grab_origin. To isolate the origin locations, we use the following methodology:\n\nGrouping by Trajectory ID:\n\nThe dataset is grouped by the unique trajectory identifier (trj_id).\n\nArranging by Timestamp:\n\nWithin each trajectory group, records are arranged in ascending order based on the timestamp (pingtimestamp).\n\nFiltering for the First Row:\n\nBy selecting the first row within each grouped trajectory (row_number() == 1), we identify the earliest recorded location for each trip. This is indicative of the trip’s starting point.\n\nAdding Temporal Information:\n\nAdditional temporal context is provided by introducing new columns:\n\nweekday: Day of the week based on the timestamp\nstart_hr: Starting hour of the trip\nday: Day of the month when the trip started\n\n\n\n\ngrab_origin &lt;- grab_sf %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(pingtimestamp) %&gt;%\n  filter(row_number()==1) %&gt;% #after sorting by timestamp, first row gives origin location \n  mutate(weekday = wday(pingtimestamp, #define workday\n                        label = TRUE,\n                        abbr = TRUE), #Monday = MON \n        start_hr = factor(hour(pingtimestamp)),\n        day = factor(mday(pingtimestamp))) #to change to ordinal scale\n\n\n#write_rds(grab_origin, \"data/rds/grab_origin.rds\")\ngrab_origin &lt;- read_rds(\"data/rds/grab_origin.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have retrieved the origin location coordinates for Grab services.\n\n\n\n\n2.5.2 Extracting Grab Trips Ending Locations\nWe will extract trip ending point for all unique trajectories and store them to a new df named grab_dest. We employ a similar methodology to extracting a trips origin location. Except here, within each trajectory group, records are arranged in descending order based on the timestamp (pingtimestamp). By selecting the first row within each grouped trajectory (row_number() == 1), we identify the latest recorded location for each trip. This corresponds to the trip’s ending point.\n\ngrab_dest &lt;- grab_sf %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(desc(pingtimestamp)) %&gt;% #function from dplyr\n  filter(row_number()==1) %&gt;% #first row after arranging in desc order gives dest  \n  mutate(weekday = wday(pingtimestamp, #define workday\n                        label = TRUE,\n                        abbr = TRUE), #Monday = MON \n        end_hr = factor(hour(pingtimestamp)),\n        day = factor(mday(pingtimestamp))) #to change to ordinal scale\n\n\n#write_rds(grab_dest, \"data/rds/grab_dest.rds\")\ngrab_dest &lt;- read_rds(\"data/rds/grab_dest.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have retrieved the destination location coordinates for Grab services.\n\n\n\n\n2.5.3 Converting Simple Features to Planar Point Pattern Objects\nTo perform spatial point pattern analysis, firstly, we’ll need to convert the simple features objects (grab_orign and grab_dest) into Spatial* classes. For this we use as.ppp from spatstat package. The st_coordinates function extracts the coordiates from our sf objects and st_bbox will extract the minimum and maximum coordinates of the object along each axis.\n\ngrab_origin_ppp &lt;- as.ppp(st_coordinates(grab_origin), st_bbox(grab_origin))\n\npar(mar = c(1,1,1,1))\nplot(grab_origin_ppp, main = \"Grab Origin Points as PPP Objects\")\n\n\ngrab_dest_ppp &lt;- as.ppp(st_coordinates(grab_dest), st_bbox(grab_dest))\n\npar(mar = c(1,1,1,1))\nplot(grab_dest_ppp, main = \"Grab Destination Points as PPP Objects\")\n\n\n\n\n\n\n\nTip\n\n\n\nIt is not neccessary to change sg_sf into a ppp object as it will be converted to owin instead.\n\n\n\n\n2.5.4 Check for Duplicates and Handle Data Errors\nLet’s have a look at the ppp objects we have created to make sure that there is no duplicates or errors in our data.\n\nsummary(grab_origin_ppp)\nsummary(grab_dest_ppp)\n\n\n\n\n\n\n\nNote\n\n\n\ngrab_origin_ppp and grab_dest_ppp objects have no duplicated points, but just to be sure, we check again using the any(duplicated() function.\n\n\n\nany(duplicated(grab_origin_ppp)) \nany(duplicated(grab_dest_ppp)) \n\nThe output is false for both objects so we move on to the next step.\n\n\n2.5.5 Introducing OWIN\nWhen analysing spatial point patterns, we’ll confine our analysis within a certain geographical area - such as the Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\n\n\n2.5.5.1 Creating OWIN Object\nTo create a two dimensional observation window using sg_sf coastal boundary we created earlier, we use the as.owin function.\n\nsg_owin &lt;- as.owin(sg_sf)\nplot(sg_owin)\n\n\n\n2.5.5.2 Combining Point Events and OWIN Object\nNow, we’ll extract the relevant point events that are located within Singapore.\n\n\n2.5.5.2.1 Origin Points in OWIN Object\n\ngrab_origin_ppp_sg &lt;- grab_origin_ppp[sg_owin]\npar(mar = c(1,1,1,1))\nplot(grab_origin_ppp_sg, main = '[OWIN] Grab Origin Points' )\n\n\n\n2.5.5.2.1 Destination Points in OWIN Object\n\ngrab_dest_ppp_sg &lt;- grab_dest_ppp[sg_owin]\npar(mar = c(1,1,1,1))\nplot(grab_dest_ppp_sg, main = '[OWIN] Grab Destination Points' )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#exploratory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#exploratory-data-analysis",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "3.0 Exploratory Data Analysis",
    "text": "3.0 Exploratory Data Analysis\nTo better understand the spatial data, we employ a series of exploratory techniques.\n\n3.1 Spatial Relationship between Origin and Destination Points\nThe map below shows the geographical distribution of origin and destination points. By overlaying both sets of points, we can observe areas where origin and destination locations overlap. These overlapping areas indicate regions with bidirectional travel.\n\ncombined_map &lt;- ggplot() +\n  geom_sf(data = grab_origin, aes(color = \"Origin\"), alpha = 0.7) +\n  geom_sf(data = grab_dest, aes(color = \"Destination\"), alpha = 0.7) +\n  ggtitle(\"Spatial Relationship Between Origin and Destination Locations\") +\n  labs(color = \"Location Type\") +\n  scale_color_manual(values = c(\"Origin\" = \"blue\", \"Destination\" = \"red\"), \n                     name = \"Location Type\", \n                     labels = c(\"Origin\", \"Destination\")) +\n  theme_minimal()\n\nprint(combined_map)\n\n\n\n3.2 Spatio-Temporal Visualisations\nSpatio-temporal visualisations show us a view of how data evolves over both space and time. These visualisations are particularly useful for analysing patterns, trends, and relationships within datasets.\n\n\n3.2.1 Frequency of Trip by Hour\nLet’s create interactive plots to provide an insightful representation of the distribution of origin/destination locations and emphasise the specific hours that stand out in terms of pickup/drop-off frequency.\n\n\n3.2.1.1 Origin or Pickup Frequency\nFor creating a bar plot with ggplot, the pickup hours are first converted to numeric values for analysis. TThe x-axis represents the pickup hours, and the y-axis represents the count of origin locations. To highlight the hours with the highest count with a visual indication, we add a red dashed vertical line on the plot. The ggplot object is then converted into an interactive plot using ggplotly, allowing users to hover over bars for detailed information.\n\ngrab_origin$start_hr &lt;- as.numeric(grab_origin$start_hr)\norg&lt;- ggplot(data = grab_origin, aes(x = start_hr)) +\n  geom_bar() +\n  labs(title = \"Distribution of Origin Locations by Pickup Hour\",\n       x = \"Pickup Hour\",\n       y = \"Count\") +\n  scale_x_continuous(breaks = seq(0, 24, by = 1)) +\n  theme_minimal()\nmax_hour &lt;- which.max(table(grab_origin$start_hr))\norg &lt;- org + geom_vline(xintercept = max_hour, linetype = \"dashed\", color = \"red\")\n\norg &lt;- ggplotly(org)\norg\n\nThe chart above indicates that most pickups occur at 2-3pm, a potential reason could include:\n\nAfter-School Activities: this time period may coincide with school release times and after-school activities. Parents might be picking up children or transporting them to various activities.\n\n\n\n3.2.1.2 Destination or Drop-off Frequency\nFor creating a bar plot with ggplot, the drop-off hours are first converted to numeric values for analysis. TThe x-axis represents the drop-off hours, and the y-axis represents the count of destination locations. To highlight the hours with the highest count with a visual indication, we add a red dashed vertical line on the plot. The ggplot object is then converted into an interactive plot using ggplotly, allowing users to hover over bars for detailed information.\n\ngrab_dest$end_hr &lt;- as.numeric(grab_dest$end_hr)\ndest&lt;- ggplot(data = grab_dest, aes(x = end_hr)) +\n  geom_bar() +\n  labs(title = \"Distribution of Destinatioon Locations by Drop-off Hour\",\n       x = \"Drop-off Hour\",\n       y = \"Count\") +\n  scale_x_continuous(breaks = seq(0, 24, by = 1)) +\n  theme_minimal()\nmax_hour &lt;- which.max(table(grab_dest$end_hr))\ndest &lt;- dest + geom_vline(xintercept = max_hour, linetype = \"dashed\", color = \"red\")\n\ndest &lt;- ggplotly(dest)\ndest\n\nThe chart above indicates that most drop-offs occur at 2pm, a potential reason could include:\n\nLunchtime Rush: 2pm is often around the time people finish their lunch breaks. This could result in increased travel demand as people return to work or resume their activities.\n\n\n\n3.2.2 Frequency of Trip by Day of Week\nLet’s create a bar chart to explore the frequency of origins/destinations across different days and identify any noteworthy patterns.\n\n\n3.2.2.1 Origin or Pickup Frequency by Days\nThe code uses ggplot to create a bar plot, where each bar represents the frequency of pickups on a specific day. This code produces a informative visualisation to explore patterns in pickup frequency throughout the week.\n\norigin_day &lt;- ggplot(data = grab_origin, aes(x = weekday))  +\n              geom_bar(fill = \"#4C78A8\", color = \"#4C78A8\", alpha = 0.8) +\n              labs(title = \"Frequency of Pickup by Day of Week\",\n                   x = \"Day\",\n                   y = \"Count\") +\n              theme_minimal()\norigin_day &lt;- ggplotly(origin_day)\norigin_day\n\n\nThe distribution of trips across days appears generally uniform, with a subtle increase observed on Wednesdays.\n\n\n\n3.2.2.2 Destination or Drop-off Frequency by Days\nThe code uses ggplot to create a bar plot, where each bar represents the frequency of drop-offs on a specific day. This code produces a informative visualisation to explore patterns in drop-off frequency throughout the week.\n\ndest_day &lt;- ggplot(data = grab_dest, aes(x = weekday))  +\n              geom_bar(fill = \"#FF0000\", color = \"#FF0000\", alpha = 0.8) +\n              labs(title = \"Frequency of Drop-off by Day of Week\",\n                   x = \"Day\",\n                   y = \"Count\") +\n              theme_minimal()\ndest_day &lt;- ggplotly(dest_day)\ndest_day\n\n\nIf we plot by destination, we see the same result. This is expected, considering that each trip’s origin corresponds to a destination."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#first-order-spatial-point-patterns-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#first-order-spatial-point-patterns-analysis",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "4.0 First-Order Spatial Point Patterns Analysis",
    "text": "4.0 First-Order Spatial Point Patterns Analysis\nFirst-order spatial point pattern analysis focuses on the distribution of individual points in a given spatial domain. It involves examining the basic characteristics and properties of the point pattern itself, without considering the interactions or relationship between points.\n\n4.1 Rescale grab_original_ppp_sg and grab_dest_ppp_sg\nFor further analysis, it is necessary to convert our data to kilometers, and we can achieve this by utilising the rescale function on grab_original_ppp_sg and grab_dest_ppp_sg which are in metres.\n\n# | eval: false\ngrab_origin_ppp_sg_km &lt;- rescale(grab_origin_ppp_sg, 1000, 'km')\ngrab_dest_ppp_sg_km &lt;- rescale(grab_dest_ppp_sg, 1000, 'km')\n\n\n#write_rds(grab_origin_ppp_sg_km, \"data/rds/grab_origin_ppp_sg_km.rds\")\ngrab_origin_ppp_sg_km &lt;- read_rds(\"data/rds/grab_origin_ppp_sg_km.rds\")\n\n\n#write_rds(grab_dest_ppp_sg_km, \"data/rds/grab_dest_ppp_sg_km.rds\")\ngrab_dest_ppp_sg_km &lt;- read_rds(\"data/rds/grab_dest_ppp_sg_km.rds\")\n\n\n\n4.2 Kernel Density Estimation\nKernel Density Estimation (KDE) is a technique used in spatial point pattern analysis to estimate the density of events across a continuous space based on a set of observed point locations. It is particularly when we have a set of spatial points and want to visualise the spatial distribution of these points in a smoother and more continuous way.\nThe are different approaches for selecting the bandwidth or smoothing parameter of the kernel.\n\nautomatic bandwidth method\nfixed bandwidth method\n\n\n\n4.2.1 Automatic bandwidth method\nHere the bandwidth is determined automatically by the algorithm based on some optimisation criterion.\nThere are several spatstat functions that we can use for automatic bandwidth selection.\nThe density function allows us to compute a kernel density for a given set of point events.\nFirst, we use the bw.diggle() method.\n\nbw.diggle() Cross Validated Bandwidth Selection for Kernel Density\n\n\nkde_grab_origin_sg_bw &lt;- density(grab_origin_ppp_sg_km,\n                                   sigma=bw.diggle,\n                                   edge=TRUE,\n                                   kernel=\"gaussian\")\n\n\npar(mar = c(1,1,1,1))\nplot(kde_grab_origin_sg_bw,main = \"KDE Automatic Bandwidth for Origin Points\")\n\n\nkde_grab_dest_sg_bw &lt;- density(grab_dest_ppp_sg_km,\n                                   sigma=bw.diggle,\n                                   edge=TRUE,\n                                   kernel=\"gaussian\")\nkde_grab_dest_sg_bw\n\n\npar(mar = c(1,1,1,1))\nplot(kde_grab_dest_sg_bw,main = \"KDE Automatic Bandwidth for Destination Points\")\n\n\n\n4.2.1.1 Other Bandwidth Selection Methods\nAdditionally we can choose from a series of options for ‘kernel’ which is the smoothing parameter that we will explore later. Let’s explore the other methods for automatic bandwidth selection and retrieve sigma value. The sigma value tells us the amount of smoothing applied when estimating the kernel density.\n\nbw.diggle() Cross Validated Bandwidth Selection for Kernel Density\n\n\nbw_diggle &lt;- bw.diggle(grab_origin_ppp_sg_km)\nbw_diggle\n\n\nbw.CvL() Cronie and Van Lieshout’s Criterion for Bandwidth Selection for Kernel Density\n\n\nbw_CvL &lt;- bw.CvL(grab_origin_ppp_sg_km)\nbw_CvL\n\n\nbw.scott() Scott’s Rule for Bandwidth Selection for Kernel Density\n\n\nbw_scott &lt;- bw.scott(grab_origin_ppp_sg_km)\nbw_scott\n\n\nbw.ppl() Likelihood Cross Validation Bandwidth Selection for Kernel Density\n\n\nbw_ppl &lt;- bw.ppl(grab_origin_ppp_sg_km)\nbw_ppl\n\nLet’s plot to compare the output of each method, so that we can see the distinct differences in KDE layers .\n\nkde_diggle &lt;- density(grab_origin_ppp_sg_km, bw_diggle)\nkde_CvL &lt;- density(grab_origin_ppp_sg_km, bw_CvL)\nkde_scott &lt;- density(grab_origin_ppp_sg_km, bw_scott)\nkde_ppl &lt;- density(grab_origin_ppp_sg_km, bw_ppl)\n\npar(mfrow = c(2,2), mar = c(1,1,1,1))\nplot(kde_diggle,main = \"KDE diggle\")\nplot(kde_CvL,main = \"KDE CvL\")\nplot(kde_scott,main = \"KDE Scott\")\nplot(kde_ppl,main = \"KDE ppl\")\n\n\nFrom first glance, it looks as though KDE Scott shows the best resuts with the clearest peaks.\nBut in order to pick the most suitable method for our analysis, we need to compare the distribution of KDE values. We can do so simply by visualising the distribution using histograms. Let’s check again KDE Scott shows the most ideal result.\n\n\npar(mfrow = c(2,2),mar = c(3,3,3,3))\nhist(kde_diggle,main = \"KDE diggle\")\nabline(v=50, \n       col=\"red\")\nhist(kde_CvL,main = \"KDE CvL\")\nabline(v=50, \n       col=\"red\")\nhist(kde_scott,main = \"KDE Scott\")\nabline(v=50, \n       col=\"red\")\nhist(kde_ppl,main = \"KDE ppl\")\nabline(v=50, \n       col=\"red\")\n\n\n\n4.2.1.2 Choosing the Most Appropriate KDE Selection Method\n\nLooking at the histograms, the one for bw_scott() shows that it has a broad spread as compared to the others and does not peak which means there is an even distribution of points across all bins. Therefore we will pick bw_scott().\n\n\n\n4.2.2 Fixed Bandwidth Selection\nHere, we manually specify a fixed bandwidth value for the KDE layer. This allows us to control the level of smoothing applied to the point pattern. We will plot using bw_scott()as identified previously as the most suitable method.\n\nfixed_bw_scott &lt;- bw.scott(grab_origin_ppp_sg_km)\nfixed_bw_scott\n\nThe values returned are 1.59 and 0.94 for sigma.x and sigma.y respectively.\nThen, we plot to visualise the fixed bandwidth using bw_scott\n\nkde_fixed_bw_scott &lt;- density(grab_origin_ppp_sg_km, fixed_bw_scott)\npar(mar = c(1,1,1,1))\nplot(kde_fixed_bw_scott, main ='Scott Method Fixed Bandwidth KDE for Origin Points')\n\n\n\n4.2.3 Different Kernel Function Selection Methods for Fixed Bandwidth\nThe default kernel in density.ppp() is the gaussian. There are other options such as epanechnikov, quartic and disc.\nLet’s explore the different kernel function selection methods.\n\nkde_fixed_bw_scott_gaussian &lt;- density(grab_origin_ppp_sg_km, \n                          sigma=fixed_bw_scott, \n                          edge=TRUE, \n                          kernel=\"gaussian\")\n\n\nkde_fixed_bw_scott_epanechnikov &lt;- density(grab_origin_ppp_sg_km, \n                          sigma=fixed_bw_scott, \n                          edge=TRUE, \n                          kernel=\"epanechnikov\")\n   \nkde_fixed_bw_scott_quartic &lt;- density(grab_origin_ppp_sg_km, \n                          sigma=fixed_bw_scott, \n                          edge=TRUE, \n                          kernel=\"quartic\")\n       \n   \nkde_fixed_scott_disc &lt;- density(grab_origin_ppp_sg_km, \n                          sigma=fixed_bw_scott, \n                          edge=TRUE, \n                          kernel=\"disc\")\n\nLet’s visualise the different kernel methods.\n\npar(mfrow = c(2,2), mar = c(2,2,2,2))\nplot(kde_fixed_bw_scott_gaussian, main=\"Gaussian\")\nplot(kde_fixed_bw_scott_epanechnikov, main=\"Epanechnikov\")\nplot(kde_fixed_bw_scott_quartic, main=\"Quartic\")\nplot(kde_fixed_scott_disc,main=\"Disc\")\n\nThere are subtle differences in the smoothness and dispersion among the four plots, but they collectively show the same pattern in the end.\n\n\n4.2.3 KDE Layers with Spatially Adaptive Bandwidth\nHere, we use the most common adaptive bandwidth method called Adaptive Kernel Density Estimate.\n\nkde_adaptive &lt;- adaptive.density(grab_origin_ppp_sg_km, method=\"kernel\")\n\n\n\n4.2.4 Comparing Fixed and Adaptive Bandwidth\nLet’s do a side-by-side comparision of fixed bandwidth and adaptive bandwidth method .\n\npar(mfrow=c(1,2), mar = c(3,3,3,3))\nplot(kde_fixed_bw_scott_gaussian, main = \"Fixed bandwidth\")\nplot(kde_adaptive, main = \"Adaptive bandwidth\")\n\n\nThe fixed bandwidth method makes it easier to identify areas of higher origin point clusters\n\n\n\n4.2.5 Interactive KDE Maps\nNow let’s plot interactive KDE maps to have a closer look.\n\n\n4.2.5.1 Converting KDE Output into Grid Object into RasterLayer Object\nWe need to convert our KDE output into grid objects for mapping purposes, here we use the raster() function.\n\nkde_fixed_bw_scott_raster &lt;- raster(kde_fixed_bw_scott)\nkde_adaptive_kernel_raster &lt;- raster(kde_adaptive)\n\nThen we perform project transformation.\n\nprojection(kde_fixed_bw_scott_raster) &lt;- CRS(\"+init=EPSG:3414 +units=km\")\nprojection(kde_adaptive_kernel_raster) &lt;- CRS(\"+init=EPSG:3414 +units=km\")\n\n\n\n4.2.5.2 Kernel Density Maps on OpenStreetMap\nFinally, we can visualise our Kernel Density Maps on OpenStreetMap\n\ntmap_mode('view')\nkde_fixed_bw_scott_map &lt;- tm_basemap(\"OpenStreetMap\") +\n  tm_view(set.zoom.limits=c(10, 15)) +\n  tm_shape(kde_fixed_bw_scott_raster) +\n  tm_raster(alpha = 0.65, title = \"KDE_Fixed_Scott\", palette = brewer.pal(12, \"Set3\")) +\n  tm_shape(mpsz_sf)+\n  tm_polygons(alpha=0.1, id=\"PLN_AREA_N\")+\n  tmap_options(check.and.fix = TRUE)+\n  tm_layout(title = \"Scott Method Fixed Bandwidth KDE for Origin Points\")\ntmap_leaflet(kde_fixed_bw_scott_map)\n\n\ntmap_mode('view')\nkde_adaptive_kernel_map &lt;- tm_basemap(\"OpenStreetMap\") +\n  tm_view(set.zoom.limits=c(10, 15)) +\n  tm_shape(kde_adaptive_kernel_raster) +\n  tm_raster(alpha = 0.65, title = \"KDE_Adaptive_Kernel\", palette = brewer.pal(12, \"Set3\")) +\n  tm_shape(mpsz_sf)+\n  tm_polygons(alpha=0.1, id=\"PLN_AREA_N\")+\n  tmap_options(check.and.fix = TRUE)+\n  tm_layout(title = \"Adaptive Bandwidth KDE for Origin Points\")\ntmap_leaflet(kde_adaptive_kernel_map)\n\n\n\n4.2.6 Kernel Density Map Analysis\nLet’s begin by extracting insights from the fixed bandwidth map, where it’s notably easier to identify the high-density pickup areas represented by yellow clusters. A prominent cluster emerges in the central-south region, encompassing stations such as Newton, Orchard, Downtown East, and Rochor. These areas exhibit increased demand for Grab pickups, potentially influenced by their status as tourist attractions. The tendency for individuals to explore these locales and then opt for Grab as their origin point prompts questions about the efficacy of public transport planning in encouraging more sustainable transportation choices.\nAdditionally, a big cluster forms at Changi Airport, which is quite understandable. Travelers landing in Singapore, often fatigued and burdened with luggage, may prefer the convenience of Grab over public transportation for their journey home.\nBeyond these, smaller yet discernible clusters show up in various residential zones. Referencing the adaptive bandwidth map for precise locations, we observe clusters in the north (Choa Chu Kang, Bukit Panjang), west (Jurong West, Jurong East), east (Tampines, Pasir Ris), and north-east (Woodlands, Sembawang, Yishun). This prompts further inquiries into the connectivity of these regions to public transport networks and the factors influencing residents to choose Grab over alternative transportation modes.\nIn essence, these spatial patterns raise intriguing questions about the accessibility and appeal of public transportation in these specific areas.\n\n\n4.2.7 Extract Planning Areas\nFrom the array of residential clusters pinpointed in the previous section, we will focus on four specific stations, each corresponding to distinct geographical regions: north, east, west, and north-east.\n\nNorth: Choa Chu Kang\nWest: Jurong East\nEast: Tampines\nNorth-East: Woodlands\n\nLet’s extract out these study areas from mpsz_sf using filter and store it in new objects.\n\nje = mpsz_sf%&gt;%\n  filter(PLN_AREA_N == \"JURONG EAST\")\ntm = mpsz_sf%&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck = mpsz_sf%&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\nwd = mpsz_sf%&gt;%\n  filter(PLN_AREA_N == \"WOODLANDS\")\n\n\n\n4.2.7.1 Plotting Target Planning Areas\n\npar(mfrow=c(2,2))\nplot(st_geometry(je), main = \"Jurong East\")\nplot(st_geometry(tm), main = \"Tampines\")\nplot(st_geometry(ck), main = \"Choa Chu Kang\")\nplot(st_geometry(wd), main = \"Woodlands\")\n\nNext, we will create owin objects to represent the observation windows for respective planning area.\n\nje_owin = as.owin(je)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\nwd_owin = as.owin(wd)\n\norigin_je_ppp = grab_origin_ppp_sg[je_owin]\norigin_tm_ppp = grab_origin_ppp_sg[tm_owin]\norigin_ck_ppp = grab_origin_ppp_sg[ck_owin]\norigin_wd_ppp = grab_origin_ppp_sg[wd_owin]\n\n\n4.2.7.2 KDE Fixed-Bandwidth for Target Planning Areas\n\nje_kde_scott &lt;- density(origin_je_ppp, sigma=bw.scott, main=\"Jurong East\")\ntm_kde_scott &lt;- density(origin_tm_ppp, sigma=bw.scott, main=\"Tampines\")\nck_kde_scott &lt;- density(origin_ck_ppp, sigma=bw.scott, main=\"Choa Chu Kang\")\nwd_kde_scott &lt;- density(origin_wd_ppp, sigma=bw.scott, main=\"Woodlands\")\n\n\npar(mfrow = c(2,2))\nplot(je_kde_scott,main = \"KDE Jurong East\")\nplot(tm_kde_scott,main = \"KDE Tampines\")\nplot(ck_kde_scott,main = \"KDE Choa Chu Kang\")\nplot(wd_kde_scott,main = \"KDE Woodlands\")\n\nNow, identifying clusters within each planning area is easily achievable. However, pinpointing the exact locations of these clusters requires the incorporation of road networks for each planning area. To achieve this precision, we will leverage a more advanced KDE technique known as Network Kernel Density Estimation (NKDE) in the upcoming sections. This approach will provide deeper insights into the specific roads or areas within each planning area that host these clusters.\n\n\n\n4.2.8 Nearest Neighbour Analysis\nNearest Neighbor Analysis helps assess whether the observed spatial pattern is clustered, dispersed, or random.\nHere we will be using the Clark-Evans Test of Aggregation.\n\n\n4.2.8.1 Clark-Evans Test of Aggregation\nHere, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\n\n\n4.2.8.1.1 Origin Points in Jurong East\nThe test hypotheses are:\n\nH0 = The distribution of origin points in Jurong East are randomly distributed\nH1= The distribution of origin points in Jurong East are not randomly distributed\n\nThe 95% confident interval will be used.\n\nclarkevans.test(origin_je_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\nWe can conclude that the distribution of origin points in Jurong East is not randomly distributed\n\n\n4.2.8.1.2 Origin Points in Tampines\nThe test hypotheses are:\n\nH0 = The distribution of origin points in Tampines are randomly distributed\nH1= The distribution of origin points in Tampines are not randomly distributed\n\nThe 95% confidence interval will be used.\n\nclarkevans.test(origin_tm_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\nWe can conclude that the distribution of origin points in Tampines is not randomly distributed\n\n\n4.2.8.1.3 Origin Points in Choa Chu Kang\nThe test hypotheses are:\n\nH0 = The distribution of origin points in Choa Chu Kang are randomly distributed\nH1= The distribution of origin points in Choa Chu Kang are not randomly distributed\n\nThe 95% confidence interval will be used.\n\nclarkevans.test(origin_ck_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\nWe can conclude that the distribution of origin points in Choa Chu Kang is not randomly distributed\n\n\n4.2.8.1.4 Origin Points in Woodlands\nThe test hypotheses are:\n\nH0 = The distribution of origin points in Woodlands are randomly distributed\nH1= The distribution of origin points in Woodlands are not randomly distributed\n\nThe 95% confidence interval will be used.\n\nclarkevans.test(origin_wd_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\nWe can conclude that the distribution of origin points in Woodlands is not randomly distributed"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#network-constrained-kernel-density-estimation-nkde",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#network-constrained-kernel-density-estimation-nkde",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "5.0 Network Constrained Kernel Density Estimation (NKDE)",
    "text": "5.0 Network Constrained Kernel Density Estimation (NKDE)\nNetwork Kernel Density Estimation (NKDE) is an advanced technique that builds upon the traditional KDE method.\nWe employ NKDE because the distribution of origin points in our planning areas is not randomly distributed; it is significantly influenced by network structures. NKDE allows us to incorporate this network context, providing a more accurate representation of spatial patterns, particularly along roadways. By considering the connectivity and pathways of the network, NKDE enhances our ability to capture the nuanced distribution of clusters within each planning area, leading to more insightful and precise spatial analysis results\nThe main difference between the KDE and NKDE is that KDE treats space as a continuous field, and overlooks the underlying network structure, such as roads. NKDE takes into account the network structure, and recognises that spatial relationships may be constrained by the existing road infrastructure. Therefore, NKDE offers better localization of clusters by considering the connectivity and pathways of the network.\nHere, we will use spNetwork to create NKDE maps for each of our planning areas and explore what insights we can yield from each."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#extract-origin-points-and-road-network-of-our-study-areas",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#extract-origin-points-and-road-network-of-our-study-areas",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "5.1 Extract origin points and road network of our study areas",
    "text": "5.1 Extract origin points and road network of our study areas\nFirstly we need to extract the road networks within each of our study area by using st_intersection. We also use st_union to combine all the geometries of each object into a single geometry.\n\nje_roads = st_intersection(sg_roads,st_union(je))\ntm_roads = st_intersection(sg_roads,st_union(tm))\nck_roads = st_intersection(sg_roads,st_union(ck))\nwd_roads = st_intersection(sg_roads,st_union(wd))\n\nAfter that, we will use st_intersection again to find the origin spots that intersect with our planning areas.\n\nje_origin = st_intersection(grab_origin,st_union(je))\ntm_origin = st_intersection(grab_origin,st_union(tm))\nck_origin = st_intersection(grab_origin,st_union(ck))\nwd_origin = st_intersection(grab_origin,st_union(wd))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#lixels",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#lixels",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "5.2 Lixels",
    "text": "5.2 Lixels\nEach network edge is divided into lixels which represent the lines of the network. To get the lixels of each planning area we use the st_cast function first to convert the geometry types of the features in our object to LINESTRING.\n\nje_roads &lt;- st_cast(je_roads, \"LINESTRING\")\ntm_roads &lt;- st_cast(tm_roads, \"LINESTRING\")\nck_roads &lt;- st_cast(ck_roads, \"LINESTRING\")\nwd_roads &lt;- st_cast(wd_roads, \"LINESTRING\")\n\nAfter we have converted the geometry type of our planning areas, we use lixelize_lines to create lixels from a set of road lines represented by the planning area objects. The road lines are divided into lixels, each with a length of 750 units. mindist represents the minimum distance between lixels to ensure that resulting lixels are not too close to each other. If the length of the resulting lixel is less than the specified minimum distance, it is combined with the previous lixel.\n\nje_lixels &lt;- lixelize_lines(je_roads, \n                         750, \n                         mindist = 375)\n\ntm_lixels &lt;- lixelize_lines(tm_roads, \n                         750, \n                         mindist = 375)\n\nck_lixels &lt;- lixelize_lines(ck_roads, \n                         750, \n                         mindist = 375)\n\nwd_lixels &lt;- lixelize_lines(wd_roads, \n                         750, \n                         mindist = 375)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#line-center",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#line-center",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "5.3 Line Center",
    "text": "5.3 Line Center\nThen we extract the centers of the lixels using lines_center. These serve as the locations for intensity estimation.\n\nje_samples &lt;- lines_center(je_lixels)\ntm_samples &lt;- lines_center(tm_lixels)\nck_samples &lt;- lines_center(ck_lixels)\nwd_samples &lt;- lines_center(wd_lixels)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#using-simple-method-to-compute-nkde",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#using-simple-method-to-compute-nkde",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "5.4 Using Simple Method to Compute NKDE",
    "text": "5.4 Using Simple Method to Compute NKDE\nFinally we use the nkde function from spNetwork to get the NKDE. There are several parameters that we can define\n\nevents: the event associated with the analysis\nw: weight vector, creates a vector of ones with a length equal to the number of rows in the planning area data frame\nsamples: samples used for density estimation\nkernel_name: type of kernel to be used\nbw: determines the scale of influence for each point in the density estimation\ndiv: the method used to determine the bandwidth\nmethod: method used for density estimation\ndigits: number of significant digits displayed in the output\ntol: tolerance level for convergence in iterative algorithms\ngrid_shape: shape of the grid for calculating the density\nmax_depth: Maximum depth of the tree when building the spatial index\nagg: number of points aggregated into each grid cell\nsparse: whether to use sparse matrix representation for efficiency\nverbose: suppresses verbose output during the process\n\n\nje_density &lt;- nkde(je_roads, \n                  events = je_origin,\n                  w = rep(1,nrow(je_origin)),\n                  samples = je_samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\ntm_density &lt;- nkde(tm_roads, \n                  events = tm_origin,\n                  w = rep(1,nrow(tm_origin)),\n                  samples = tm_samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\nck_density &lt;- nkde(ck_roads, \n                  events = ck_origin,\n                  w = rep(1,nrow(ck_origin)),\n                  samples = ck_samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\nwd_density &lt;- nkde(wd_roads, \n                  events = wd_origin,\n                  w = rep(1,nrow(wd_origin)),\n                  samples = wd_samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWe are also required to join the density values into the samples and lixels objects. The obtained densities will be scaled by the total number of origin points and we will multiply by 1000 for km measurement.\n\nje_samples$density &lt;- je_density*nrow(je_origin)*1000\nje_lixels$density &lt;- je_density*nrow(je_origin)*1000\n\ntm_samples$density &lt;- tm_density*nrow(tm_origin)*1000\ntm_lixels$density &lt;- tm_density*nrow(tm_origin)*1000\n\nck_samples$density &lt;- ck_density*nrow(ck_origin)*1000\nck_lixels$density &lt;- ck_density*nrow(ck_origin)*1000\n\nwd_samples$density &lt;- wd_density*nrow(wd_origin)*1000\nwd_lixels$density &lt;- wd_density*nrow(wd_origin)*1000"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#plotting-maps-for-different-study-areas",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#plotting-maps-for-different-study-areas",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "5.5 Plotting Maps for Different Study Areas",
    "text": "5.5 Plotting Maps for Different Study Areas\nLet’s plot the NKDE map for each planning area.\n\n5.5.1 Jurong East\n\ntmap_mode('view')\nje_density_map &lt;- tm_basemap(\"OpenStreetMap\") +\ntm_shape(je_lixels)+\n  tm_lines(col =\"density\", lwd = 3, palette = brewer.pal(12, \"Set3\"))+\ntm_shape(je_origin)+\n  tm_dots(size=0.05)+\n  tm_layout(title = \"Jurong East NKDE\")\ntmap_leaflet(je_density_map)\n\n\n\n\n\n\n\nNote\n\n\n\nAnalysis\nIn Jurong East, notable clusters of taxi pick-up points have been observed around Yuhua Place, Yuhua Senior Activity Center, New Jurong Polyclinic, and the nursing home vicinity. Additionally, clusters are prevalent near Parc Oasis, Singtel, Zai Shun Seafood, in proximity to Toh Guan, Westgate, Shen Hong Temple, Jurong East Interchange, IMM, Yuhua Primary School, and Crest Secondary School.\nThis clustering phenomenon may be attributed to\n\nCommercial Hubs: Areas like Westgate and IMM are major commercial centers, attracting a higher demand for Grab services. For example, people may book Grab to pick them up after they finish shopping in these areas.\nHealthcare Facilities: Proximity to healthcare facilities such as the New Jurong Polyclinic and nursing homes may lead to increased transportation needs. For example, patients or visitors might utilise Grab for convenient travel from medical appointments, contributing to the clustering effect around healthcare establishments.\nEducational Institutions: The presence of schools like Yuhua Primary School and Crest Secondary School could contribute to higher Grab demand during school-related activities. For example, parents may pick up their children after school and book Grab to their residence.\nTransportation Hubs: Jurong East Interchange serves as a transportation hub, leading to concentrated Grab activity in the area. Commuters arriving at or departing from the interchange might prefer Grab for last-mile connectivity, resulting in a clustering effect around this transportation hub.\nRecreational Areas: Clusters around Parc Oasis and Yuhua Senior Activity Center may be influenced by recreational and leisure activities. Caregivers visiting seniors at the Senior Activity Center may book Grab from there to their homes.\nCultural and Religious Centers: Locations like Shen Hong Temple may attract Grab pick-ups during events or gatherings. Attendees of cultural or religious events may use Grab for transportation from these centers.\nResidential Areas: Proximity to residential areas like Yuhua Place may result in frequent Grab pickup requests for residents. Residents in these areas might regularly utilise Grab for daily commuting or transportation needs.\nCulinary Hotspots: Clusters around Zai Shun Seafood restaurant may be influenced by popular dining establishments, drawing people to the area and making them book Grab from there to their journey back.\n\n\n\n\n\n5.5.2 Tampines\n\ntmap_mode('view')\ntm_density_map &lt;- tm_basemap(\"OpenStreetMap\") +\ntm_shape(tm_lixels)+\n  tm_lines(col =\"density\", lwd = 3, palette = brewer.pal(12, \"Set3\"))+\ntm_shape(tm_origin)+\n  tm_dots(size=0.05)+\n  tm_layout(title = \"Tampines NKDE\")\ntmap_leaflet(tm_density_map)\n\n\n\n\n\n\n\nNote\n\n\n\nAnalysis\nIn Tampines, clusters of Grab pick-up points are notable around Our Tampines Hub, Tanah Merah Country Club, opposite Tampines Neighbourhood Police Center, opposite Ngee Ann Secondary School, Tampines East, Laguna Country Club, and East Coast Park.\nThis clustering phenomenon may be attributed to\nRecreational Clubs: Areas like Tanah Merah Country Club and Laguna Country Club may contribute to clusters of Grab pick-up points. Individuals visiting these recreational clubs for leisure activities may opt for Grab services for their journey back from here for convenience.\nPolice Center: The area opposite Tampines Neighbourhood Police Center may attract Grab pick-ups due visitors requiring transportation from the area.\nCommunity and Recreational Center: Our Tampines Hub is a central community and recreational center, leading to higher demand for Grab services. People utilising the various facilities may book Grab for their journeys back.\nEducational Institutions: The area opposite Ngee Ann Secondary School could contribute to higher Grab demand during school-related activities. For example, parents may pick up their children after school and book Grab to their residence.\nResidential Areas: Tampines East, being a residential area, may result in frequent Grab pickup requests for residents. Residents in these areas might regularly utilise Grab for daily commuting or transportation needs.\nRecreational Destination: East Coast Park, a popular recreational area, may attract individuals for outdoor activities. Visitors to the park may choose to book Grab for their journey back home, contributing to the clustering effect.\n\n\n\n\n5.5.3 Choa Chu Kang\n\ntmap_mode('view')\nck_density_map &lt;- tm_basemap(\"OpenStreetMap\") +\ntm_shape(ck_lixels)+\n  tm_lines(col =\"density\", lwd = 3, palette = brewer.pal(12, \"Set3\"))+\ntm_shape(ck_origin)+\n  tm_dots(size=0.05)+\n  tm_layout(title = \"Choa Chu Kang NKDE\")\ntmap_leaflet(ck_density_map)\n\n\n\n\n\n\n\nNote\n\n\n\nAnalysis\nIn Choa Chu Kang, clusters of Grab pick-up points can be seen around Inn See Temple, Choa Chu Kang Interchange, SAFRA@CCK, Gain City, Lot One, Keat Hong Colours, Phoenix Station, Bukit Panjang Post Office, Yew Mei Condominium, MWS Nursing Home, and Yew Tee Point.\nThis clustering phenomenon may be attributed to\n\nCultural and Religious Centers: Inn See Temple’s vicinity may attract Grab pick-ups during religious events, contributing to the observed cluster.\nTransportation Hub: Choa Chu Kang Interchange serves as a transportation hub, leading to concentrated Grab activity in the area. Commuters arriving at or departing from the interchange might prefer Grab for last-mile connectivity, resulting in a clustering effect around this transportation hub.\nRecreational Facility: The presence of SAFRA@CCK could contribute to increased Grab pickup activity, especially after events and recreational activities hosted at the facility.\nShopping Centers: Yew Tee Point, Gain City and Lot One, being prominent shopping destinations, may experience a higher level of Grab pick-up points as shoppers prefer convenient transportation after their shopping sprees.\nResidential Areas: Clusters around Keat Hong Colours, and Yew Mei Condominium may be attributed to the residential nature of these areas, with residents relying on Grab for commuting needs.\nPostal Area: Bukit Panjang Post Office may attract Grab pick-ups, for individuals who have completed their postal services or nearby activities.\nHealthcare: Presence of MWS Nursing Home may lead to increased transportation needs. Caregivers visiting seniors at the nursing home may book Grab from there to their homes.\n\n\n\n\n\n5.5.4 Woodlands\n\ntmap_mode('view')\nwd_density_map &lt;- tm_basemap(\"OpenStreetMap\") +\ntm_shape(wd_lixels)+\n  tm_lines(col =\"density\", lwd = 3, palette = brewer.pal(12, \"Set3\"))+\ntm_shape(wd_origin)+\n  tm_dots(size=0.05)+\n  tm_layout(title = \"Woodlands NKDE\")\ntmap_leaflet(wd_density_map)\n\n\n\n\n\n\n\nNote\n\n\n\nIn Woodlands, distinct clusters of taxi pick-up points have been identified around Innova Junior College, Singapore Sports School, Civic Centre, STELLAR@TE2, Singapore Turf Club, Old Woodlands Town Centre, Masjid An Nur, Woodlands Cinema, Greenwood Primary School, and Mega@Woodlands.\nThis clustering phenomenon may be attributed to\n\nEducational Institutions: Clusters around Innova Junior College and Singapore Sports School could be attributed to the presence of these educational institutions. Students, staff, and visitors may opt for Grab for convenient transportation from these locations.\nCivic and Community Center: Civic Centre, being a civic and community hub, may experience higher demand for Grab pickup services.\nCommercial Hub: STELLAR@TE2’s may have individuals possibly relying on Grab for commuting on their journey back from here.\nSports and Recreation: The presence of Singapore Turf Club may contribute to the clustering effect, with people choosing Grab for transportation from sports and recreational activities.\nCommercial and Residential Hub: Old Woodlands Town Centre’s central location may attract Grab pick-ups from both commercial and residential areas.\nReligious Center: Masjid An Nur’s may witness increased Grab activity during religious events.\nEntertainment Venue: Woodlands Cinema’s indicates a potential concentration of Grab pick-up points, especially after movie screenings, as patrons opt for Grab for their journey home.\nEducational Facility: Greenwood Primary School’s location could contribute to higher Grab demand during school-related activities. For example, parents may pick up their children after school and book Grab to their residence."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#conclusion",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "6.0 Conclusion",
    "text": "6.0 Conclusion\nThrough this take-home exercise, we delved into Spatial Point Patterns Analysis to unravel the geographical distribution of Grab Hailing Services in Singapore. Our analyses provided valuable insights, unveiling patterns such as the peak day and time for Grab pick-ups, the specific locations where these pickups occur, hotspots, and the particular roads that witness heightened activity. This information serves as a strategic tool for better planning and decision-making.\nFor instance, we can leverage these findings to enhance public transport accessibility. Understanding the road networks with the highest Grab pick-up activity allows us to identify areas where improved public transportation services could be implemented. This strategic planning aims to encourage people to opt for public transport, contributing to even pollution reduction by minimising car usage.\nIn our future endeavors, we can expand our exploration by delving into temporal Network Kernel Density Estimation (NKDE). This advanced analysis will enable us to scrutinise the intricate relationship between time and Grab Hailing Services. By identifying popular pick-up points during specific time intervals, we can propose strategic interventions, such as increasing the number of buses, adjusting bus frequencies, or implementing targeted measures to enhance transportation infrastructure in those areas. This forward-looking approach ensures a nuanced understanding of temporal patterns and facilitates more informed decisions for optimizing transportation services in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#references",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#references",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore [DATA PREPARATION]",
    "section": "7.0 References",
    "text": "7.0 References\n\nKam, T. S. (2022). R for Geospatial Data Science and Analytics. Retrieved from https://r4gdsa.netlify.app.\nGimond (2023). Chapter 11 Point Pattern Analysis. Retrieved from https://mgimond.github.io/Spatial/index.html.\nRey, S.J., Arribas-Bel, D., & Wolf, L.J. (2023). Point Pattern Analysis. In: Geographic Data Science with python. CRC Press.\nMoraga, P. Spatial Statistics for Data Science: Theory and Practice with R. Retrieved from https://www.paulamoraga.com/book-spatial/spatial-point-patterns.html."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "pacman::p_load(sf,sfedp, tmap, tidyverse)\n\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC)+\n  tm_fill(\"GDPPC\", \n          style = \"quantile\",\n          platte = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by county, Hunan province\"\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legenf.height = 0.45,\n            legend,widht = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguinity(geometry),\n         wt = st_weights(nb,\n                         style = \"w\"),\n         .before= 1)\n\n\nset.seed(1234)\n\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "",
    "text": "Code\ninstall.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")\n\n\n\n\nCode\npacman::p_load(maptools, sf, raster, spatstat, tmap, tidyverse)\n\n\n##Spatial Data Wrangling\n\n\nCode\nchildcare_sf &lt;- st_read(\"data/geospatial/ChildCareServices.geojson\")%&gt;%\n  st_transform(crs = 3414)\nmpsz_sf &lt;- st_read(dsn= \"data/geospatial\",\n                   layer = \"MP14_SUBZONE_WEB_PL\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-maptools",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-maptools",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "",
    "text": "Code\ninstall.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")\n\n\n\n\nCode\npacman::p_load(maptools, sf, raster, spatstat, tmap, tidyverse)\n\n\n##Spatial Data Wrangling\n\n\nCode\nchildcare_sf &lt;- st_read(\"data/geospatial/ChildCareServices.geojson\")%&gt;%\n  st_transform(crs = 3414)\nmpsz_sf &lt;- st_read(dsn= \"data/geospatial\",\n                   layer = \"MP14_SUBZONE_WEB_PL\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#creating-costal-outline",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#creating-costal-outline",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "creating costal outline",
    "text": "creating costal outline\n\n\nCode\nsg_sf &lt;- mpsz_sf %&gt;%\n  st_union()\n\n\n\n\nCode\nplot(sg_sf)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#importing-the-spatial",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#importing-the-spatial",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "Importing the spatial",
    "text": "Importing the spatial"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#mapping-the-geospaital",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#mapping-the-geospaital",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "Mapping the geospaital",
    "text": "Mapping the geospaital"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#geospatial-data-wrangling",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "Geospatial Data wrangling",
    "text": "Geospatial Data wrangling"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#creating-ppp-object-sf-method",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#creating-ppp-object-sf-method",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "creating PPP object: sf method",
    "text": "creating PPP object: sf method\n\n\nCode\nchildcare_ppp &lt;- as.ppp(childcare_sf)\n#as.ppp is only for sf layer not sp\n\n\n\n\nCode\nsummary(childcare_ppp)\n\n\n\n\nCode\nplot(childcare_ppp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#handling-duplicated-data",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#handling-duplicated-data",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "Handling duplicated data",
    "text": "Handling duplicated data\n\n\nCode\nany(duplicated(childcare_ppp))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#creating-owin-object",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#creating-owin-object",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "Creating owin object",
    "text": "Creating owin object\n\n\nCode\nsg_owin &lt;- as.owin(sg_sf)\n\n\n\n\nCode\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\n\n\nCode\nsummary(childcareSG_ppp)\n\n\n\n\nCode\npg &lt;- mpsz_sf %&gt;% filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;% filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;% filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;% filter(PLN_AREA_N == \"JURONG WEST\")\n\n\n\n\nCode\nplot (pg, main =\"Ponggol\")\nplot (tm, main =\"Tampines\")\nplot (ck, main =\"Choa Chu Kang\")\nplot (jw, main =\"Jurong West\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#combining-point",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#combining-point",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "Combining point",
    "text": "Combining point"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#first-order-spatal",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#first-order-spatal",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "First-order spatal",
    "text": "First-order spatal"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#kernel-density-estimation",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#kernel-density-estimation",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#computing-kernel",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#computing-kernel",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "Computing kernel",
    "text": "Computing kernel"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#rescaling-kde",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#rescaling-kde",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "Rescaling KDE",
    "text": "Rescaling KDE"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#working-with-different",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#working-with-different",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "working with different",
    "text": "working with different"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In-class Exercise 2: R for Geospatial Data Science",
    "section": "",
    "text": "In this in-class exercise, the following R packages will be used:\n\narrow to read Parquet files\nlubridate for working with date-time data\ntidyverse\ntmap for interactive maps\nsf\n\n\n\nCode\npacman::p_load(arrow, lubridate, tidyverse, tmap, sf)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#load-r-packages",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#load-r-packages",
    "title": "In-class Exercise 2: R for Geospatial Data Science",
    "section": "",
    "text": "In this in-class exercise, the following R packages will be used:\n\narrow to read Parquet files\nlubridate for working with date-time data\ntidyverse\ntmap for interactive maps\nsf\n\n\n\nCode\npacman::p_load(arrow, lubridate, tidyverse, tmap, sf)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#extracting-trips-starting-locations",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#extracting-trips-starting-locations",
    "title": "In-class Exercise 2: R for Geospatial Data Science",
    "section": "Extracting trips starting locations",
    "text": "Extracting trips starting locations\n\nextract trips origin locations\nderive new columns : weekday, starting hour and day of month\n\n\n\nCode\norigin_df &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange (pingtimestamp) %&gt;%\n  filter(row_number()==1) %&gt;% #first row gives origin location \n  mutate(weekday = wday(pingtimestamp, #define workday\n                        label = TRUE,\n                        abbr = TRUE), #Monday = MON \n        start_hr = factor(hour(pingtimestamp)),\n        day = factor(mday(pingtimestamp))) #to change to ordinal scale"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#extracting-trips-ending-locations",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#extracting-trips-ending-locations",
    "title": "In-class Exercise 2: R for Geospatial Data Science",
    "section": "Extracting trips ending locations",
    "text": "Extracting trips ending locations\n\nextract trips destination locations\nderive new columns: weekday, ending hour and day of month\n\n\n\nCode\ndest_df &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(desc(pingtimestamp)) %&gt;% #function from dplyr\n  filter(row_number()==1) %&gt;% #first row after arranging in desc order gives dest  \n  mutate(weekday = wday(pingtimestamp, #define workday\n                        label = TRUE,\n                        abbr = TRUE), #Monday = MON \n        end_hr = factor(hour(pingtimestamp)),\n        day = factor(mday(pingtimestamp))) #to change to ordinal scale\n\n\n#Import Data for future use\n\n\nCode\n#origin_df &lt;-read_rds(data/rds/origin_df.rds)\n#dest_df &lt;-read_rds(data/rds/dest_df.rds)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#visualising-frequency-distribution",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#visualising-frequency-distribution",
    "title": "In-class Exercise 2: R for Geospatial Data Science",
    "section": "Visualising frequency distribution",
    "text": "Visualising frequency distribution"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis involves examining the pattern or distribution of a group of points on a surface. These points may represent:\n\nOccurrences like crime incidents, traffic accidents, or the onset of diseases\nBusiness-related points such as coffee shops and fast-food outlets, or essential services like childcare and eldercare facilities.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childcare centres in Singapore.\nThe specific questions we aim to address are:\n\nIs there a random distribution of childcare centers across Singapore?\nIf not, the subsequent question becomes identifying the areas where there is a higher concentration of childcare centers."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis involves examining the pattern or distribution of a group of points on a surface. These points may represent:\n\nOccurrences like crime incidents, traffic accidents, or the onset of diseases\nBusiness-related points such as coffee shops and fast-food outlets, or essential services like childcare and eldercare facilities.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childcare centres in Singapore.\nThe specific questions we aim to address are:\n\nIs there a random distribution of childcare centers across Singapore?\nIf not, the subsequent question becomes identifying the areas where there is a higher concentration of childcare centers."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#datasets",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#datasets",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.2 Datasets",
    "text": "6.2 Datasets\n\nCHILDCARE: a point feature data providing both location and attribute information of childcare centres\nMP14_SUBZONE_WEB_PL: a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data\nCostalOutline: a polygon feature data showing the national boundary of Singapore"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#r-packages",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.3 R Packages",
    "text": "6.3 R Packages\nThe following R packages will be used:\n\nsf, designed to import, manage and process vector-based geospatial data in R.\nspatstat, functions for point pattern analysis\nraster reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster)\nmaptools provides tools for manipulating geographic data\ntmap provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\n\ninstall.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")\n\npackage 'maptools' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\fathi\\AppData\\Local\\Temp\\RtmpKEWhxO\\downloaded_packages\n\npacman::p_load(sf, spatstat, raster, maptools, tmap)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-data-wrangling",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.4 Spatial Data Wrangling",
    "text": "6.4 Spatial Data Wrangling"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-spatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#importing-spatial-data",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.4.1 Importing Spatial Data",
    "text": "6.4.1 Importing Spatial Data\nst_read() from sf package will be used to import these three geospatial data sets into R.\nLocations and Attributes of Childcare Centres\n\nchildcare_sf &lt;- st_read(\"data/aspatial/PreSchoolsLocation.geojson\") %&gt;% \n  st_transform(crs = 3414)\n\nReading layer `PreSchoolsLocation' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex06\\data\\aspatial\\PreSchoolsLocation.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nURA 2014 Master Plan Planning Subzone Boundaries\n\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nNational Boundary of Singapore\n\nsg_sf &lt;- st_read(dsn = \"data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nDIY: Using the appropriate sf function you learned in Hands-on Exercise 2, retrieve the referencing system information of these geospatial data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#map-projection",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#map-projection",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.4.1.1 Map Projection",
    "text": "6.4.1.1 Map Projection\nBefore we can use these data for analysis, it is important for us to ensure that they are projected in same projection system.\n\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nExcept childcare_sf, both mpsz_sf and sg_sf do not have proper CRS information.\n\nDIY: Using the method you learned in Lesson 2, assign the correct crs to mpsz_sf and sg_sf simple feature data frames.\n\nHence, the correct CRS must be assigned to mpsz_sf and sg_sf\n\nmpsz_sf &lt;- st_set_crs(mpsz_sf, 3414)\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n sg_sf &lt;- st_set_crs(sg_sf, 3414)\n st_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#mapping-the-geospatial-data-sets",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#mapping-the-geospatial-data-sets",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.4.2 Mapping the Geospatial Data Sets",
    "text": "6.4.2 Mapping the Geospatial Data Sets\nAfter checking the referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns.\n\nDIY: Using the mapping methods you learned in Hands-on Exercise 3, prepare a map as shown below.\n\n\ntm_shape(mpsz_sf) +\n  tm_polygons() +\ntm_shape(childcare_sf) +\n  tm_symbols(size = 0.02, col = \"black\")\n\n\n\n\nWe can also prepare a pin map by using the code chunk below.\n\ntmap_mode(\"view\") #set to interactive\ntm_shape(childcare_sf) +\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode('plot') #to return to default \n\nIn interactive mode, tmap is using leaflet for R API.We can also query the information of each simple feature by clicking on them. We can also change the background of the internet map layer. Three internet map layers are provided - ESRI.WorldGrayCanvas (default) - OpenStreetMap - ESRI.WorldTopoMaz"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.5 Geospatial Data Wrangling",
    "text": "6.5 Geospatial Data Wrangling\nAlthough sf data frame is gaining popularity against sp’s Spatial* classes, many geospatial analysis packages still require the input geospatial data to be in sp’s Spatial* classes."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#converting-sf-data-frames-to-sps-spatial-class",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#converting-sf-data-frames-to-sps-spatial-class",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.5.1 Converting sf data frames to sp’s Spatial* class",
    "text": "6.5.1 Converting sf data frames to sp’s Spatial* class\nConverting sf DataFrames to sp’s Spatial* Class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\nThe following are the information of the three Spatial* classes.\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 2290 \nextent      : 11810.03, 45404.24, 25596.33, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                       Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;3-IN-1 FAMILY CENTRE&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;ST0027&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;DF7EC9C2478FA5A5&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093631&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,   &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;Zulfa Kindergarten&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;PT9603&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;527C1231DDD0FA64&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093632&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \n\n\n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\nsg\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#converting-spatial-class-into-generic-sp-format",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#converting-spatial-class-into-generic-sp-format",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.5.2 Converting Spatial* class into Generic sp Format",
    "text": "6.5.2 Converting Spatial* class into Generic sp Format\nspatstat requires the analytical data in ppp object form. We need to convert the Spatial classes* into Spatial object.\nThe codes chunk below converts the Spatial* classes into generic sp objects.\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\") \nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\nThe properties of the sp objects are as follow:\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 2290 \nextent      : 11810.03, 45404.24, 25596.33, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#converting-generic-sp-format-into-spatstats-ppp-format",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#converting-generic-sp-format-into-spatstats-ppp-format",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.5.3 Converting Generic sp Format into spatstat’s ppp Format",
    "text": "6.5.3 Converting Generic sp Format into spatstat’s ppp Format\nNow, we will use as.ppp() of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nchildcare_ppp &lt;- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nPlanar point pattern: 2290 points\nwindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n\n\n\nplot(childcare_ppp)\n\n\n\n\nThis shows the summary statistics of the childcare_ppp object.\n\nsummary(childcare_ppp)\n\nPlanar point pattern:  2290 points\nAverage intensity 2.875673e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#handling-duplicated-points",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#handling-duplicated-points",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.5.4 Handling Duplicated Points",
    "text": "6.5.4 Handling Duplicated Points\nThe duplication in a ppp object can be checked by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nTo count the number of co-indicence point, multiplicity() can be used.\n\nmultiplicity(childcare_ppp)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    1    3    4    1    7    7    1    1    1    2    1    1    1    1    2 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    1    1    1    1    1    4    1    1    1    1    1    5    1    2    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    1    1    1    2    2    2    1    1    1    1    1    2    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   5    1    1    2    1    1    1    1    1    1    1    2    1    1    1    4 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1   10    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    4    1    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1   10 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n  10   10    1    1    1    1    1    1    1    1    1    1    1    1    3    1 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    1    2    1   10    1    1    1    1    1    1    2    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   1    1    1    3    1    1    1    1    1    3    1    1    1    1    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    2    1    1    3    1    1    1    2    1    2    2    2    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    1    1    1    1    2    1    1    1    1    1    2    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   1    2    1    1    1    1    1    1    3    1    1    1    4    1    1    1 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    4    1    1    1    1    1    1    1    1    1    1    2    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    3    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    1    1    1    1    1    1    1    2    7    1    3    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   2    1    1    1    1    1    1    1    3    2    1    1    1    1    1    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    2    1    1    2    1    1    1    2    1    1    1    2    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   1    1    1    1    1    1    2    3    2    1    2    1    1    1    1    5 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   1    1    2    1    1    3    1    1    1    1    1    1    5    1    1    1 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    4    1    1    1    1    1    1    1    1    3    1    1    2 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    2    1    1    1    1    1    1    3    1    1    1    1    1    1    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   1    1    1    2    1    1    1    1    1    2    1    1    1    1    1    1 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1    1    1    2    2    2    1    1    1    1    1   10    1 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   2    1    1    1    2    1    3    1    1    1    1    1    1    1    1    2 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   2    1    1    3    1    1    1    1    1    1    3    1    1    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    3    1    1    1    3    1    3    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    1    1    1    1    2    2    2    1    1    2    3    1 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    2    1    1    1    1    3    1    1    3    1    1    1    1    2 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    2    1    2    1   10    1    4    2    2    1    1    1    1    4    1 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    3    1    1    1    1    4    1    2    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    3    1    1    1    1    1    2    1    1    1    2    2    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    4    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   2    1    1    1    1    1    1    1    1    1    1    1   10    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    3    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    3    3    3    3    1    1    1    1    1    1    1    3    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   3    1    1    1    1    1    1    1    1    3    1    3    1    1    1    3 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   2    2    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   2    1    1    1    3    1    1    1    1    2    1    1    1    1    1    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    2    1 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    2    4    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    1    3    1    3    3    3    3    1    1    1    1    3    1 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   1    1    3    1    2    1    1    1    1    1    3    1    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   3    1    3    1    3    1    1    1    1    1    1    1    1    1    2    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    1    1    1    2    3    1    1    1    1    1   10    1    2    4    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    4    1    7    1    1    1    1    3    1    1    1    1    1    3 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   3    1    1    1    1    3    1    1    1    3    1    3    1    1    1    3 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   3    1    1    1    1    2    1    1    1    1    3    1    1    3    1    2 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    1    1    1    3    3    1    1    3    1    2    1    3    1    3    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    1    1    1    3    1    1    1    1    1    1    1    1    1    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    3    1    1    1    1    3    1    1    1    1    3    1    3 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    1    1    3    1    1    3    1    1    1    1    2    1    1    1    3 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    3    1    1    1    1    1    1    3    3    3    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   1    1    1    2    1    1    3    1    1    1    1    1    1    1    3    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   3    3    3    3    3    1    1    1    3    1    4    3    1    3    1    1 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   3    4    3    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   1    1    1    1    1    1    1    1    1    1    1    1    4    1    1    1 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    3    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   1    3    1    1    1    1    1    1    1   10    1    1    1    1    1    1 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    3    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    1    1    1    1    2    2    3    1    1    1    7    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   1    1    1    1    1    1    1    1    1    1    5    1    1    1    1    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    1    1    1    1    1    2    1    1    1    1    4    2    3    2    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    2    2    1    1    1    1    2    2    3    1    1    1    1    1    2 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    3    3    2    2    2    2    2    2    2    2    2    3    3    3 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   2    2    3    2    3    2    3    2    2    2    2    2    2    2    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   1    1    1    1    2    1    1    1    1    1    1    3    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    1    2    1    1    1    1    5    1    1    1    1 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 \n   3    1    1    2    1    1    1    2    1    1    1    1    1    1    1    1 \n1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 \n   1    1    7    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 \n   3    1    1    5    1    3    2    3    3    3    3    2    2    4    3    2 \n1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 \n   2    2    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 \n   1    1    1    1    1    5    1    1    3    1    1    1    1    1    1    1 \n1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    2    2 \n1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 \n   2    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 \n   2    2    2    3    2    2    2    2    2    2    2    4    2    2    2    2 \n1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 \n   2    4    3    2    2    2    2    3    2    2    2    2    2    2    2    2 \n1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 \n   2    2    4    2    2    2    2    2    2    2    1    2    2    2    2    2 \n1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 \n   3    2    2    2    2    2    2    2    2    2    3    2    2    2    2    2 \n1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 \n   2    2    2    2    2    2    2    5    2    2    2    7    2    2    2    2 \n1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 \n   2    2    2    2    2    2    7    2    4    2    2    2    2    2    2    2 \n1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 \n   2    2    2    2    2    2    2    2    3    2    2    2    2    2    1    2 \n1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 \n   2    2    2    3    2    2    2    2    3    2    2    2    2    3    2    2 \n1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 \n   2    2    2    2    3    2    2    3    3    3    3    3    2    3    2    3 \n1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 \n   3    3    3    2    2    3    3    2    3    3    2    2    2    2    2    3 \n1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 \n   2    3    3    3    3    2    2    2    2    2    3    2    2    2    3    2 \n1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 \n   3    3    2    2    2    2    3    3    3    3    3    3    3    2    2    3 \n1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 \n   2    2    2    2    3    3    3    3    2    2    3    3    2    2    3    2 \n1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 \n   3    3    2    2    3    3    2    2    3    4    3    3    2    3    2    2 \n1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 \n   3    2    2    2    2    3    2    2    2    7    2    1    2    7    2    2 \n1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 \n   4    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 \n   2    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 \n   2    2    2    2    2    2    5    2    2    2    2    2    4    1    1    1 \n1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 \n   1    1    1    1    1    1    1    1    1    3    3    3    3    1    3    1 \n1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 \n   3    3    3    3    3    3    3    3    3    3    3    3    3    1    3    3 \n1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 \n   3    3    3    3    3    3    3    3    3    3    3    1    1    3    3    3 \n2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 \n   2    3    3    3    3    3    4    3    2    3    3    3    3    3    3    3 \n2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 \n   3    3    3    3    3    3    3    3    2    3    3    3    2    2    2    2 \n2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 \n   3    2    2    2    2    2    2    2    4    2    2    2    2    1    2    2 \n2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 \n   2    2    2    2    2    3    2    3    2    2    2    3    3    2    2    2 \n2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 \n   3    2    3    2    2    2    2    2    2    3    2    2    3    2    2    2 \n2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 \n   2    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 \n   2    2    3    2    2    2    2    2    2    2    2    2    3    2    2    2 \n2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 \n   2    2    2    2    2    2    4    2    7    2    2    2    1    2    2    2 \n2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 \n   2    1    2    2    2    2    2    7    2    4    2    2    2    2    2    2 \n2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 \n   2    2    2    2    2    2    2    2    2    3    2    2    2    2    2    2 \n2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 \n   1    2    2    2    2    3    2    2    3    1    2    2    2    2    2    2 \n2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 \n   2    2    1    3    2    2    2    3    2    2    2    2    2    2    2    2 \n2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 \n   2    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 \n   2    2    2    2    2    4    2    7    2    7    2    2    4    2    2    2 \n2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 \n   2    2    3    2    2    2    2    2    2    2    2    2    2    4    2    2 \n2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 \n   2    2    2    2    3    2    2    2    2    2    2    2    2    2    2    2 \n2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 \n   2    2    2    2    2    4    2    2    2    2    2    2    2    2    2    4 \n2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 \n   2    2    2    2    2    2    2    3    3    2    2    2    2    2    2    2 \n2289 2290 \n   2    2 \n\n\nIf we want to know how many locations have more than one point event, we can use the code chunk below.\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 885\n\n\nTo view the locations of these duplicate point events, we will plot childcare data by using the code below.\n\ntmap_mode(\"view\")\ntm_shape(childcare) +\n  tm_dots(alpha = 0.4,\n          size = 0.05)\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThere are three ways to overcome this problem.\n\nThe easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\n\nThe code chunk below implements the jittering approach.\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp,    \n                                 retry = TRUE,\n                             nsim = 1,\n                             drop = TRUE)\n\nCheck if any duplicated point remains in the geospatial data.\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#creating-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#creating-owin-object",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.5.5 Creating owin Object",
    "text": "6.5.5 Creating owin Object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\nsg_owin &lt;- as(sg_sp, \"owin\")\n\nThe ouput object can be displayed by using plot()\n\nplot(sg_owin)\n\n\n\n\nand summary() function of Base R.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n60 separate polygons (no holes)\n            vertices        area relative.area\npolygon 1         38 1.56140e+04      2.09e-05\npolygon 2        735 4.69093e+06      6.27e-03\npolygon 3         49 1.66986e+04      2.23e-05\npolygon 4         76 3.12332e+05      4.17e-04\npolygon 5       5141 6.36179e+08      8.50e-01\npolygon 6         42 5.58317e+04      7.46e-05\npolygon 7         67 1.31354e+06      1.75e-03\npolygon 8         15 4.46420e+03      5.96e-06\npolygon 9         14 5.46674e+03      7.30e-06\npolygon 10        37 5.26194e+03      7.03e-06\npolygon 11        53 3.44003e+04      4.59e-05\npolygon 12        74 5.82234e+04      7.78e-05\npolygon 13        69 5.63134e+04      7.52e-05\npolygon 14       143 1.45139e+05      1.94e-04\npolygon 15       165 3.38736e+05      4.52e-04\npolygon 16       130 9.40465e+04      1.26e-04\npolygon 17        19 1.80977e+03      2.42e-06\npolygon 18        16 2.01046e+03      2.69e-06\npolygon 19        93 4.30642e+05      5.75e-04\npolygon 20        90 4.15092e+05      5.54e-04\npolygon 21       721 1.92795e+06      2.57e-03\npolygon 22       330 1.11896e+06      1.49e-03\npolygon 23       115 9.28394e+05      1.24e-03\npolygon 24        37 1.01705e+04      1.36e-05\npolygon 25        25 1.66227e+04      2.22e-05\npolygon 26        10 2.14507e+03      2.86e-06\npolygon 27       190 2.02489e+05      2.70e-04\npolygon 28       175 9.25904e+05      1.24e-03\npolygon 29      1993 9.99217e+06      1.33e-02\npolygon 30        38 2.42492e+04      3.24e-05\npolygon 31        24 6.35239e+03      8.48e-06\npolygon 32        53 6.35791e+05      8.49e-04\npolygon 33        41 1.60161e+04      2.14e-05\npolygon 34        22 2.54368e+03      3.40e-06\npolygon 35        30 1.08382e+04      1.45e-05\npolygon 36       327 2.16921e+06      2.90e-03\npolygon 37       111 6.62927e+05      8.85e-04\npolygon 38        90 1.15991e+05      1.55e-04\npolygon 39        98 6.26829e+04      8.37e-05\npolygon 40       415 3.25384e+06      4.35e-03\npolygon 41       222 1.51142e+06      2.02e-03\npolygon 42       107 6.33039e+05      8.45e-04\npolygon 43         7 2.48299e+03      3.32e-06\npolygon 44        17 3.28303e+04      4.38e-05\npolygon 45        26 8.34758e+03      1.11e-05\npolygon 46       177 4.67446e+05      6.24e-04\npolygon 47        16 3.19460e+03      4.27e-06\npolygon 48        15 4.87296e+03      6.51e-06\npolygon 49        66 1.61841e+04      2.16e-05\npolygon 50       149 5.63430e+06      7.53e-03\npolygon 51       609 2.62570e+07      3.51e-02\npolygon 52         8 7.82256e+03      1.04e-05\npolygon 53       976 2.33447e+07      3.12e-02\npolygon 54        55 8.25379e+04      1.10e-04\npolygon 55       976 2.33447e+07      3.12e-02\npolygon 56        61 3.33449e+05      4.45e-04\npolygon 57         6 1.68410e+04      2.25e-05\npolygon 58         4 9.45963e+03      1.26e-05\npolygon 59        46 6.99702e+05      9.35e-04\npolygon 60        13 7.00873e+04      9.36e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 748741000 square units\nFraction of frame area: 0.414"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#combining-point-events-object-and-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#combining-point-events-object-and-owin-object",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.5.6 Combining Point Events Object and owin Object",
    "text": "6.5.6 Combining Point Events Object and owin Object\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nPlanar point pattern:  2290 points\nAverage intensity 3.058467e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n60 separate polygons (no holes)\n            vertices        area relative.area\npolygon 1         38 1.56140e+04      2.09e-05\npolygon 2        735 4.69093e+06      6.27e-03\npolygon 3         49 1.66986e+04      2.23e-05\npolygon 4         76 3.12332e+05      4.17e-04\npolygon 5       5141 6.36179e+08      8.50e-01\npolygon 6         42 5.58317e+04      7.46e-05\npolygon 7         67 1.31354e+06      1.75e-03\npolygon 8         15 4.46420e+03      5.96e-06\npolygon 9         14 5.46674e+03      7.30e-06\npolygon 10        37 5.26194e+03      7.03e-06\npolygon 11        53 3.44003e+04      4.59e-05\npolygon 12        74 5.82234e+04      7.78e-05\npolygon 13        69 5.63134e+04      7.52e-05\npolygon 14       143 1.45139e+05      1.94e-04\npolygon 15       165 3.38736e+05      4.52e-04\npolygon 16       130 9.40465e+04      1.26e-04\npolygon 17        19 1.80977e+03      2.42e-06\npolygon 18        16 2.01046e+03      2.69e-06\npolygon 19        93 4.30642e+05      5.75e-04\npolygon 20        90 4.15092e+05      5.54e-04\npolygon 21       721 1.92795e+06      2.57e-03\npolygon 22       330 1.11896e+06      1.49e-03\npolygon 23       115 9.28394e+05      1.24e-03\npolygon 24        37 1.01705e+04      1.36e-05\npolygon 25        25 1.66227e+04      2.22e-05\npolygon 26        10 2.14507e+03      2.86e-06\npolygon 27       190 2.02489e+05      2.70e-04\npolygon 28       175 9.25904e+05      1.24e-03\npolygon 29      1993 9.99217e+06      1.33e-02\npolygon 30        38 2.42492e+04      3.24e-05\npolygon 31        24 6.35239e+03      8.48e-06\npolygon 32        53 6.35791e+05      8.49e-04\npolygon 33        41 1.60161e+04      2.14e-05\npolygon 34        22 2.54368e+03      3.40e-06\npolygon 35        30 1.08382e+04      1.45e-05\npolygon 36       327 2.16921e+06      2.90e-03\npolygon 37       111 6.62927e+05      8.85e-04\npolygon 38        90 1.15991e+05      1.55e-04\npolygon 39        98 6.26829e+04      8.37e-05\npolygon 40       415 3.25384e+06      4.35e-03\npolygon 41       222 1.51142e+06      2.02e-03\npolygon 42       107 6.33039e+05      8.45e-04\npolygon 43         7 2.48299e+03      3.32e-06\npolygon 44        17 3.28303e+04      4.38e-05\npolygon 45        26 8.34758e+03      1.11e-05\npolygon 46       177 4.67446e+05      6.24e-04\npolygon 47        16 3.19460e+03      4.27e-06\npolygon 48        15 4.87296e+03      6.51e-06\npolygon 49        66 1.61841e+04      2.16e-05\npolygon 50       149 5.63430e+06      7.53e-03\npolygon 51       609 2.62570e+07      3.51e-02\npolygon 52         8 7.82256e+03      1.04e-05\npolygon 53       976 2.33447e+07      3.12e-02\npolygon 54        55 8.25379e+04      1.10e-04\npolygon 55       976 2.33447e+07      3.12e-02\npolygon 56        61 3.33449e+05      4.45e-04\npolygon 57         6 1.68410e+04      2.25e-05\npolygon 58         4 9.45963e+03      1.26e-05\npolygon 59        46 6.99702e+05      9.35e-04\npolygon 60        13 7.00873e+04      9.36e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 748741000 square units\nFraction of frame area: 0.414\n\n\n\nDIY: Using the method you learned in previous exercise, plot the newly derived childcareSG_ppp as shown below.\n\nUsing plot(), the childcareSG_ppp object can be displayed.\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#extracting-study-area",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#extracting-study-area",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.5.7 Extracting Study Area",
    "text": "6.5.7 Extracting Study Area\nThe code chunk below will be used to extract the target planning areas.\n\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\nPlotting target planning areas\n\npar(mfrow = c(2,2)) \nplot(pg, main = \"Punggol\") \nplot(tm, main = \"Tampines\")\nplot(ck, main = \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#converting-spatial-point-dataframe-into-generic-sp-format",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#converting-spatial-point-dataframe-into-generic-sp-format",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.5.7.1 Converting Spatial Point DataFrame into Generic sp Format",
    "text": "6.5.7.1 Converting Spatial Point DataFrame into Generic sp Format\nNext, these SpatialPolygonsDataFrame layers will be converted into generic spatialpolygons layers.\n\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#creating-owin-object-1",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#creating-owin-object-1",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.5.7.2 Creating owin Object",
    "text": "6.5.7.2 Creating owin Object\nNow, these SpatialPolygons objects will be converted into owin objects that is required by spatstat.\n\npg_owin = as(pg_sp, \"owin\") \ntm_owin = as(tm_sp, \"owin\") \nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#combining-childcare-points-and-the-study-area",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#combining-childcare-points-and-the-study-area",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.5.7.3 Combining Childcare Points and the Study Area",
    "text": "6.5.7.3 Combining Childcare Points and the Study Area\nBy using the code chunk below, childcare centres within the specific region can be extracted for later analysis.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin] \nchildcare_tm_ppp = childcare_ppp_jit[tm_owin] \nchildcare_ck_ppp = childcare_ppp_jit[ck_owin] \nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale() is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\") \nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\") \nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\") \nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#second-order-spatial-point-patterns-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#second-order-spatial-point-patterns-analysis",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.6 Second-order Spatial Point Patterns Analysis",
    "text": "6.6 Second-order Spatial Point Patterns Analysis"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#analysing-spatial-point-process-using-g-function",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#analysing-spatial-point-process-using-g-function",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6.7 Analysing Spatial Point Process Using G-Function",
    "text": "6.7 Analysing Spatial Point Process Using G-Function\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, you will learn how to compute G-function estimation by using Gest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n5.7.1 Choa Chu Kang planning area\n\n5.7.1.1 Computing G-function estimation\nThe code chunk below is used to compute G-function using Gest() of spatat package.\n\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n5.7.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-function\n\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_CK.csr)\n\n\n\n\n\n\n\n5.7.2 Tampines planning area\n\n5.7.2.1 Computing G-function estimation\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\n5.7.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#analysing-spatial-point-process-using-f-function",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#analysing-spatial-point-process-using-f-function",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "5.8 Analysing Spatial Point Process Using F-Function",
    "text": "5.8 Analysing Spatial Point Process Using F-Function\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, you will learn how to compute F-function estimation by using Fest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n5.8.1 Choa Chu Kang planning area\n\n5.8.1.1 Computing F-function estimation\nThe code chunk below is used to compute F-function using Fest() of spatat package.\n\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n\n\n\n.8.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-fucntion\n\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_CK.csr)\n\n\n\n\n\n\n5.8.3 Tampines planning area\n\n5.8.3.1 Computing F-function estimation\nMonte Carlo test with F-fucntion\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n5.8.3.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#analysing-spatial-point-process-using-k-function",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#analysing-spatial-point-process-using-k-function",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "5.9 Analysing Spatial Point Process Using K-Function",
    "text": "5.9 Analysing Spatial Point Process Using K-Function\nK-function measures the number of events found up to a given distance of any particular event. In this section, you will learn how to compute K-function estimates by using Kest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n5.9.1 Choa Chu Kang planning area\n\n5.9.1.1 Computing K-fucntion estimate\n\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n5.9.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nK_ck.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n5.9.2 Tampines planning area\n\n5.9.2.1 Computing K-fucntion estimation\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n5.9.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#analysing-spatial-point-process-using-l-function",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#analysing-spatial-point-process-using-l-function",
    "title": "Hands-on Exercise 6: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "5.10 Analysing Spatial Point Process Using L-Function",
    "text": "5.10 Analysing Spatial Point Process Using L-Function\nIn this section, you will learn how to compute L-function estimation by using Lest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n5.10.1 Choa Chu Kang planning area\n\n5.10.1.1 Computing L Fucntion estimation\n\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n5.10.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n5.10.2 Tampines planning area\n\n5.10.2.1 Computing L-fucntion estimate\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n5.10.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below will be used to perform the hypothesis testing.\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62,  [8:05 remaining] 63,\n [7:45 remaining] 64,  [7:25 remaining] 65,  [7:06 remaining] 66,\n [6:47 remaining] 67,  [6:29 remaining] 68,  [6:12 remaining] 69,\n [5:54 remaining] 70,  [5:38 remaining] 71,  [5:22 remaining] 72,\n [5:06 remaining] 73,  [4:51 remaining] 74,  [4:36 remaining] 75,\n [4:21 remaining] 76,  [4:07 remaining] 77,  [3:54 remaining] 78,\n [3:40 remaining] 79,  [3:27 remaining] 80,  [3:15 remaining] 81,\n [3:02 remaining] 82,  [2:50 remaining] 83,  [2:38 remaining] 84,\n [2:27 remaining] 85,  [2:16 remaining] 86,  [2:05 remaining] 87,\n [1:54 remaining] 88,  [1:43 remaining] 89,  [1:33 remaining] 90,\n [1:23 remaining] 91,  [1:13 remaining] 92,  [1:03 remaining] 93,\n [54 sec remaining] 94,  [44 sec remaining] 95,  [35 sec remaining] 96,\n [26 sec remaining] 97,  [17 sec remaining] 98,  [8 sec remaining] \n99.\n\nDone.\n\n\n\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html",
    "title": "Hands-on Exercise 5: Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, wewill learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package.\nWe will:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#overview",
    "title": "Hands-on Exercise 5: Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, wewill learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package.\nWe will:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#getting-started",
    "title": "Hands-on Exercise 5: Global Measures of Spatial Autocorrelation",
    "section": "5.2 Getting Started",
    "text": "5.2 Getting Started\n\n5.2.1 Analytical Question\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.\nThe anaytical questions are:\n\nAre the development distributed geographically?\nIf no, are there sign of spatial clustering?\nIf yes, where are these clusters?\n\n\n\n5.2.2 Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n5.2.3 Load Relevant R Packages\nThe packages are\n\nsf for importing and handling geospatial data in R\ntidyverse for wrangling attribute data in R\nspdep to compute spatial weights, global and local spatial autocorrelation statistics\ntmap to prepare cartographic quality chropleth map\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#load-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#load-data",
    "title": "Hands-on Exercise 5: Global Measures of Spatial Autocorrelation",
    "section": "5.3 Load Data",
    "text": "5.3 Load Data\n\n5.3.1 Import Shapefiles into R\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex05\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n5.3.2 Import csv File into R\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n5.3.3 Relational Join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n5.3.4 Visualising Regional Development Indicator\nLets prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#global-measures-of-spatial-autocorrelation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#global-measures-of-spatial-autocorrelation",
    "title": "Hands-on Exercise 5: Global Measures of Spatial Autocorrelation",
    "section": "5.4 Global Measures of Spatial Autocorrelation",
    "text": "5.4 Global Measures of Spatial Autocorrelation\nHere we will compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n5.4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. In the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. \n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n5.4.2 Row-standardised Weights Matrix\nWe need to assign weights to each neighboring polygon. This is done by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#global-measures-of-spatial-autocorrelation-morans-i",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#global-measures-of-spatial-autocorrelation-morans-i",
    "title": "Hands-on Exercise 5: Global Measures of Spatial Autocorrelation",
    "section": "5.5 Global Measures of Spatial Autocorrelation: Moran’s I",
    "text": "5.5 Global Measures of Spatial Autocorrelation: Moran’s I\nHere we will perform Moran’s I statistics testing by using moran.test() of spdep.\n\n5.5.1 Maron’s I Test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer:\n\n\n\n\n\n5.5.2 Computing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n5.5.3 Visualising Monte Carlo Moran’s I\nIn the code chunk below hist() and abline() of R Graphics are used.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nQuestion: What statistical observation can you draw fro mthe output above?\n\n\nChallenge: Instead of using Base Graph to plot the values, plot the values by using ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "title": "Hands-on Exercise 5: Global Measures of Spatial Autocorrelation",
    "section": "5.6 Global Measures of Spatial Autocorrelation: Geary’s C",
    "text": "5.6 Global Measures of Spatial Autocorrelation: Geary’s C\nHere we will perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n5.6.1 Geary’s C Test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer:\n\n\n\n\n\n5.6.2 Computing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer:\n\n\n\n\n\n5.6.3 Visualising the Monte Carlo Geary’s C\nWe will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\") \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nQuestion: What statistical observation can you draw from the output?\nAnswer:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05a.html#spatial-correlogram",
    "title": "Hands-on Exercise 5: Global Measures of Spatial Autocorrelation",
    "section": "5.7 Spatial Correlogram",
    "text": "5.7 Spatial Correlogram\nSpatial correlograms show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.\n\n5.7.1 Compute Moran’s I Correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. \n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nQuestion: What statistical observation can you draw from the plot above?\nAnswer:\n\n\n\n\n\n5.7.2 Compute Geary’s C Correlogram and Plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis involves examining the pattern or distribution of a group of points on a surface. These points may represent:\n\nOccurrences like crime incidents, traffic accidents, or the onset of diseases\nBusiness-related points such as coffee shops and fast-food outlets, or essential services like childcare and eldercare facilities.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childcare centres in Singapore.\nThe specific questions we aim to address are:\n\nIs there a random distribution of childcare centers across Singapore?\nIf not, the subsequent question becomes identifying the areas where there is a higher concentration of childcare centers."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#importing-spatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#importing-spatial-data",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.4.1 Importing Spatial Data",
    "text": "3.4.1 Importing Spatial Data\nst_read() from sf package will be used to import these three geospatial data sets into R.\nLocations and Attributes of Childcare Centres\n\nchildcare_sf &lt;- st_read(\"data/aspatial/PreSchoolsLocation.geojson\") %&gt;% \n  st_transform(crs = 3414)\n\nReading layer `PreSchoolsLocation' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex03\\data\\aspatial\\PreSchoolsLocation.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nURA 2014 Master Plan Planning Subzone Boundaries\n\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nNational Boundary of Singapore\n\nsg_sf &lt;- st_read(dsn = \"data/geospatial\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#map-projection",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#map-projection",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.4.1.1 Map Projection",
    "text": "3.4.1.1 Map Projection\nBefore we can use these data for analysis, it is important for us to ensure that they are projected in same projection system.\n\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nExcept childcare_sf, both mpsz_sf and sg_sf do not have proper CRS information.\nHence, the correct CRS must be assigned to mpsz_sf and sg_sf\n\nmpsz_sf &lt;- st_set_crs(mpsz_sf, 3414)\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\nsg_sf &lt;- st_set_crs(sg_sf, 3414)\nst_crs(sg_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#mapping-geospatial-datasets",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#mapping-geospatial-datasets",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.4.2 Mapping Geospatial Datasets",
    "text": "3.4.2 Mapping Geospatial Datasets\nIt is also useful for us to plot a map to show their spatial patterns.\n\ntm_shape(mpsz_sf) +\n  tm_polygons() +\ntm_shape(childcare_sf) +\n  tm_symbols(size = 0.02, col = \"black\")\n\n\n\n\nWe can also prepare a pin map by using the code chunk below.\n\ntmap_mode(\"view\") #set to interactive\ntm_shape(childcare_sf) +\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode('plot') #to return to default \n\nIn interactive mode, tmap is using leaflet for R API.We can also query the information of each simple feature by clicking on them. We can also change the background of the internet map layer. Three internet map layers are provided - ESRI.WorldGrayCanvas (default) - OpenStreetMap - ESRI.WorldTopoMap"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#converting-sf-data-frames-to-sps-spatial-class",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#converting-sf-data-frames-to-sps-spatial-class",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.5.1 Converting sf data frames to sp’s Spatial* class",
    "text": "3.5.1 Converting sf data frames to sp’s Spatial* class\nConverting sf DataFrames to sp’s Spatial* Class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\nThe following are the information of the three Spatial* classes.\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 2290 \nextent      : 11810.03, 45404.24, 25596.33, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                       Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;3-IN-1 FAMILY CENTRE&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;ST0027&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;DF7EC9C2478FA5A5&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093631&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,   &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;Zulfa Kindergarten&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;PT9603&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;527C1231DDD0FA64&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093632&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \n\n\n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\nsg\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#converting-spatial-class-into-generic-sp-format",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#converting-spatial-class-into-generic-sp-format",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.5.2 Converting Spatial* class into Generic sp Format",
    "text": "3.5.2 Converting Spatial* class into Generic sp Format\nspatstat requires the analytical data in ppp object form. We need to convert the Spatial classes* into Spatial object.\nThe codes chunk below converts the Spatial* classes into generic sp objects.\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\nThe properties of the sp objects are as follow:\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 2290 \nextent      : 11810.03, 45404.24, 25596.33, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#converting-generic-sp-format-into-spatstats-ppp-format",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#converting-generic-sp-format-into-spatstats-ppp-format",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.5.3 Converting Generic sp Format into spatstat’s ppp Format",
    "text": "3.5.3 Converting Generic sp Format into spatstat’s ppp Format\nNow, we will use as.ppp() of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nchildcare_ppp &lt;- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nPlanar point pattern: 2290 points\nwindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n\n\n\nplot(childcare_ppp)\n\n\n\n\nThis shows the summary statistics of the childcare_ppp object.\n\nsummary(childcare_ppp)\n\nPlanar point pattern:  2290 points\nAverage intensity 2.875673e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#handling-duplicated-points",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#handling-duplicated-points",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.5.4 Handling Duplicated Points",
    "text": "3.5.4 Handling Duplicated Points\nThe duplication in a ppp object can be checked by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nTo count the number of co-indicence point, multiplicity() can be used.\n\nmultiplicity(childcare_ppp)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    1    3    4    1    7    7    1    1    1    2    1    1    1    1    2 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    1    1    1    1    1    4    1    1    1    1    1    5    1    2    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    1    1    1    2    2    2    1    1    1    1    1    2    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   5    1    1    2    1    1    1    1    1    1    1    2    1    1    1    4 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1   10    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    4    1    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1   10 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n  10   10    1    1    1    1    1    1    1    1    1    1    1    1    3    1 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    1    2    1   10    1    1    1    1    1    1    2    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   1    1    1    3    1    1    1    1    1    3    1    1    1    1    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    2    1    1    3    1    1    1    2    1    2    2    2    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    1    1    1    1    2    1    1    1    1    1    2    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   1    2    1    1    1    1    1    1    3    1    1    1    4    1    1    1 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    4    1    1    1    1    1    1    1    1    1    1    2    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    3    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    1    1    1    1    1    1    1    2    7    1    3    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   2    1    1    1    1    1    1    1    3    2    1    1    1    1    1    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    2    1    1    2    1    1    1    2    1    1    1    2    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   1    1    1    1    1    1    2    3    2    1    2    1    1    1    1    5 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   1    1    2    1    1    3    1    1    1    1    1    1    5    1    1    1 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    4    1    1    1    1    1    1    1    1    3    1    1    2 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    2    1    1    1    1    1    1    3    1    1    1    1    1    1    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   1    1    1    2    1    1    1    1    1    2    1    1    1    1    1    1 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1    1    1    2    2    2    1    1    1    1    1   10    1 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   2    1    1    1    2    1    3    1    1    1    1    1    1    1    1    2 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   2    1    1    3    1    1    1    1    1    1    3    1    1    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    3    1    1    1    3    1    3    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    1    1    1    1    2    2    2    1    1    2    3    1 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    2    1    1    1    1    3    1    1    3    1    1    1    1    2 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    2    1    2    1   10    1    4    2    2    1    1    1    1    4    1 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    3    1    1    1    1    4    1    2    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    3    1    1    1    1    1    2    1    1    1    2    2    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    4    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   2    1    1    1    1    1    1    1    1    1    1    1   10    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    3    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    3    3    3    3    1    1    1    1    1    1    1    3    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   3    1    1    1    1    1    1    1    1    3    1    3    1    1    1    3 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   2    2    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   2    1    1    1    3    1    1    1    1    2    1    1    1    1    1    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    2    1 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    2    4    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    1    3    1    3    3    3    3    1    1    1    1    3    1 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   1    1    3    1    2    1    1    1    1    1    3    1    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   3    1    3    1    3    1    1    1    1    1    1    1    1    1    2    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    1    1    1    2    3    1    1    1    1    1   10    1    2    4    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    4    1    7    1    1    1    1    3    1    1    1    1    1    3 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   3    1    1    1    1    3    1    1    1    3    1    3    1    1    1    3 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   3    1    1    1    1    2    1    1    1    1    3    1    1    3    1    2 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    1    1    1    3    3    1    1    3    1    2    1    3    1    3    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    1    1    1    3    1    1    1    1    1    1    1    1    1    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    3    1    1    1    1    3    1    1    1    1    3    1    3 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    1    1    3    1    1    3    1    1    1    1    2    1    1    1    3 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    3    1    1    1    1    1    1    3    3    3    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   1    1    1    2    1    1    3    1    1    1    1    1    1    1    3    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   3    3    3    3    3    1    1    1    3    1    4    3    1    3    1    1 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   3    4    3    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   1    1    1    1    1    1    1    1    1    1    1    1    4    1    1    1 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    3    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   1    3    1    1    1    1    1    1    1   10    1    1    1    1    1    1 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    3    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    1    1    1    1    2    2    3    1    1    1    7    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   1    1    1    1    1    1    1    1    1    1    5    1    1    1    1    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    1    1    1    1    1    2    1    1    1    1    4    2    3    2    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    2    2    1    1    1    1    2    2    3    1    1    1    1    1    2 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    3    3    2    2    2    2    2    2    2    2    2    3    3    3 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   2    2    3    2    3    2    3    2    2    2    2    2    2    2    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   1    1    1    1    2    1    1    1    1    1    1    3    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    1    2    1    1    1    1    5    1    1    1    1 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 \n   3    1    1    2    1    1    1    2    1    1    1    1    1    1    1    1 \n1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 \n   1    1    7    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 \n   3    1    1    5    1    3    2    3    3    3    3    2    2    4    3    2 \n1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 \n   2    2    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 \n   1    1    1    1    1    5    1    1    3    1    1    1    1    1    1    1 \n1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    2    2 \n1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 \n   2    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 \n   2    2    2    3    2    2    2    2    2    2    2    4    2    2    2    2 \n1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 \n   2    4    3    2    2    2    2    3    2    2    2    2    2    2    2    2 \n1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 \n   2    2    4    2    2    2    2    2    2    2    1    2    2    2    2    2 \n1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 \n   3    2    2    2    2    2    2    2    2    2    3    2    2    2    2    2 \n1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 \n   2    2    2    2    2    2    2    5    2    2    2    7    2    2    2    2 \n1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 \n   2    2    2    2    2    2    7    2    4    2    2    2    2    2    2    2 \n1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 \n   2    2    2    2    2    2    2    2    3    2    2    2    2    2    1    2 \n1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 \n   2    2    2    3    2    2    2    2    3    2    2    2    2    3    2    2 \n1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 \n   2    2    2    2    3    2    2    3    3    3    3    3    2    3    2    3 \n1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 \n   3    3    3    2    2    3    3    2    3    3    2    2    2    2    2    3 \n1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 \n   2    3    3    3    3    2    2    2    2    2    3    2    2    2    3    2 \n1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 \n   3    3    2    2    2    2    3    3    3    3    3    3    3    2    2    3 \n1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 \n   2    2    2    2    3    3    3    3    2    2    3    3    2    2    3    2 \n1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 \n   3    3    2    2    3    3    2    2    3    4    3    3    2    3    2    2 \n1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 \n   3    2    2    2    2    3    2    2    2    7    2    1    2    7    2    2 \n1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 \n   4    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 \n   2    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 \n   2    2    2    2    2    2    5    2    2    2    2    2    4    1    1    1 \n1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 \n   1    1    1    1    1    1    1    1    1    3    3    3    3    1    3    1 \n1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 \n   3    3    3    3    3    3    3    3    3    3    3    3    3    1    3    3 \n1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 \n   3    3    3    3    3    3    3    3    3    3    3    1    1    3    3    3 \n2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 \n   2    3    3    3    3    3    4    3    2    3    3    3    3    3    3    3 \n2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 \n   3    3    3    3    3    3    3    3    2    3    3    3    2    2    2    2 \n2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 \n   3    2    2    2    2    2    2    2    4    2    2    2    2    1    2    2 \n2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 \n   2    2    2    2    2    3    2    3    2    2    2    3    3    2    2    2 \n2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 \n   3    2    3    2    2    2    2    2    2    3    2    2    3    2    2    2 \n2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 \n   2    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 \n   2    2    3    2    2    2    2    2    2    2    2    2    3    2    2    2 \n2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 \n   2    2    2    2    2    2    4    2    7    2    2    2    1    2    2    2 \n2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 \n   2    1    2    2    2    2    2    7    2    4    2    2    2    2    2    2 \n2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 \n   2    2    2    2    2    2    2    2    2    3    2    2    2    2    2    2 \n2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 \n   1    2    2    2    2    3    2    2    3    1    2    2    2    2    2    2 \n2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 \n   2    2    1    3    2    2    2    3    2    2    2    2    2    2    2    2 \n2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 \n   2    2    2    2    2    2    2    2    2    2    2    2    2    2    2    2 \n2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 \n   2    2    2    2    2    4    2    7    2    7    2    2    4    2    2    2 \n2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 \n   2    2    3    2    2    2    2    2    2    2    2    2    2    4    2    2 \n2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 \n   2    2    2    2    3    2    2    2    2    2    2    2    2    2    2    2 \n2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 \n   2    2    2    2    2    4    2    2    2    2    2    2    2    2    2    4 \n2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 \n   2    2    2    2    2    2    2    3    3    2    2    2    2    2    2    2 \n2289 2290 \n   2    2 \n\n\nIf we want to know how many locations have more than one point event, we can use the code chunk below.\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 885\n\n\nThe output shows that there are 128 duplicated points.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code below.\n\ntmap_mode(\"view\")\ntm_shape(childcare) +\n  tm_dots(alpha = 0.4,\n          size = 0.05)\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThere are three ways to overcome this problem.\n\nThe easiest way is to delete the duplicates. But, that will also mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\n\nThe code chunk below implements the jittering approach.\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp,\n                             retry = TRUE,\n                             # No. of simulated realisations to be generated\n                             nsim = 1, \n                             drop = TRUE)\n\nCheck if any duplicated point remains in the geospatial data.\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#creating-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#creating-owin-object",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.5.5 Creating owin Object",
    "text": "3.5.5 Creating owin Object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\nsg_owin &lt;- as(sg_sp, \"owin\")\n\nThe ouput object can be displayed by using plot()\n\nplot(sg_owin)\n\n\n\n\nand summary() function of Base R.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n60 separate polygons (no holes)\n            vertices        area relative.area\npolygon 1         38 1.56140e+04      2.09e-05\npolygon 2        735 4.69093e+06      6.27e-03\npolygon 3         49 1.66986e+04      2.23e-05\npolygon 4         76 3.12332e+05      4.17e-04\npolygon 5       5141 6.36179e+08      8.50e-01\npolygon 6         42 5.58317e+04      7.46e-05\npolygon 7         67 1.31354e+06      1.75e-03\npolygon 8         15 4.46420e+03      5.96e-06\npolygon 9         14 5.46674e+03      7.30e-06\npolygon 10        37 5.26194e+03      7.03e-06\npolygon 11        53 3.44003e+04      4.59e-05\npolygon 12        74 5.82234e+04      7.78e-05\npolygon 13        69 5.63134e+04      7.52e-05\npolygon 14       143 1.45139e+05      1.94e-04\npolygon 15       165 3.38736e+05      4.52e-04\npolygon 16       130 9.40465e+04      1.26e-04\npolygon 17        19 1.80977e+03      2.42e-06\npolygon 18        16 2.01046e+03      2.69e-06\npolygon 19        93 4.30642e+05      5.75e-04\npolygon 20        90 4.15092e+05      5.54e-04\npolygon 21       721 1.92795e+06      2.57e-03\npolygon 22       330 1.11896e+06      1.49e-03\npolygon 23       115 9.28394e+05      1.24e-03\npolygon 24        37 1.01705e+04      1.36e-05\npolygon 25        25 1.66227e+04      2.22e-05\npolygon 26        10 2.14507e+03      2.86e-06\npolygon 27       190 2.02489e+05      2.70e-04\npolygon 28       175 9.25904e+05      1.24e-03\npolygon 29      1993 9.99217e+06      1.33e-02\npolygon 30        38 2.42492e+04      3.24e-05\npolygon 31        24 6.35239e+03      8.48e-06\npolygon 32        53 6.35791e+05      8.49e-04\npolygon 33        41 1.60161e+04      2.14e-05\npolygon 34        22 2.54368e+03      3.40e-06\npolygon 35        30 1.08382e+04      1.45e-05\npolygon 36       327 2.16921e+06      2.90e-03\npolygon 37       111 6.62927e+05      8.85e-04\npolygon 38        90 1.15991e+05      1.55e-04\npolygon 39        98 6.26829e+04      8.37e-05\npolygon 40       415 3.25384e+06      4.35e-03\npolygon 41       222 1.51142e+06      2.02e-03\npolygon 42       107 6.33039e+05      8.45e-04\npolygon 43         7 2.48299e+03      3.32e-06\npolygon 44        17 3.28303e+04      4.38e-05\npolygon 45        26 8.34758e+03      1.11e-05\npolygon 46       177 4.67446e+05      6.24e-04\npolygon 47        16 3.19460e+03      4.27e-06\npolygon 48        15 4.87296e+03      6.51e-06\npolygon 49        66 1.61841e+04      2.16e-05\npolygon 50       149 5.63430e+06      7.53e-03\npolygon 51       609 2.62570e+07      3.51e-02\npolygon 52         8 7.82256e+03      1.04e-05\npolygon 53       976 2.33447e+07      3.12e-02\npolygon 54        55 8.25379e+04      1.10e-04\npolygon 55       976 2.33447e+07      3.12e-02\npolygon 56        61 3.33449e+05      4.45e-04\npolygon 57         6 1.68410e+04      2.25e-05\npolygon 58         4 9.45963e+03      1.26e-05\npolygon 59        46 6.99702e+05      9.35e-04\npolygon 60        13 7.00873e+04      9.36e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 748741000 square units\nFraction of frame area: 0.414"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#combining-point-events-object-and-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#combining-point-events-object-and-owin-object",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.5.6 Combining Point Events Object and owin Object",
    "text": "3.5.6 Combining Point Events Object and owin Object\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nPlanar point pattern:  2290 points\nAverage intensity 3.058467e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n60 separate polygons (no holes)\n            vertices        area relative.area\npolygon 1         38 1.56140e+04      2.09e-05\npolygon 2        735 4.69093e+06      6.27e-03\npolygon 3         49 1.66986e+04      2.23e-05\npolygon 4         76 3.12332e+05      4.17e-04\npolygon 5       5141 6.36179e+08      8.50e-01\npolygon 6         42 5.58317e+04      7.46e-05\npolygon 7         67 1.31354e+06      1.75e-03\npolygon 8         15 4.46420e+03      5.96e-06\npolygon 9         14 5.46674e+03      7.30e-06\npolygon 10        37 5.26194e+03      7.03e-06\npolygon 11        53 3.44003e+04      4.59e-05\npolygon 12        74 5.82234e+04      7.78e-05\npolygon 13        69 5.63134e+04      7.52e-05\npolygon 14       143 1.45139e+05      1.94e-04\npolygon 15       165 3.38736e+05      4.52e-04\npolygon 16       130 9.40465e+04      1.26e-04\npolygon 17        19 1.80977e+03      2.42e-06\npolygon 18        16 2.01046e+03      2.69e-06\npolygon 19        93 4.30642e+05      5.75e-04\npolygon 20        90 4.15092e+05      5.54e-04\npolygon 21       721 1.92795e+06      2.57e-03\npolygon 22       330 1.11896e+06      1.49e-03\npolygon 23       115 9.28394e+05      1.24e-03\npolygon 24        37 1.01705e+04      1.36e-05\npolygon 25        25 1.66227e+04      2.22e-05\npolygon 26        10 2.14507e+03      2.86e-06\npolygon 27       190 2.02489e+05      2.70e-04\npolygon 28       175 9.25904e+05      1.24e-03\npolygon 29      1993 9.99217e+06      1.33e-02\npolygon 30        38 2.42492e+04      3.24e-05\npolygon 31        24 6.35239e+03      8.48e-06\npolygon 32        53 6.35791e+05      8.49e-04\npolygon 33        41 1.60161e+04      2.14e-05\npolygon 34        22 2.54368e+03      3.40e-06\npolygon 35        30 1.08382e+04      1.45e-05\npolygon 36       327 2.16921e+06      2.90e-03\npolygon 37       111 6.62927e+05      8.85e-04\npolygon 38        90 1.15991e+05      1.55e-04\npolygon 39        98 6.26829e+04      8.37e-05\npolygon 40       415 3.25384e+06      4.35e-03\npolygon 41       222 1.51142e+06      2.02e-03\npolygon 42       107 6.33039e+05      8.45e-04\npolygon 43         7 2.48299e+03      3.32e-06\npolygon 44        17 3.28303e+04      4.38e-05\npolygon 45        26 8.34758e+03      1.11e-05\npolygon 46       177 4.67446e+05      6.24e-04\npolygon 47        16 3.19460e+03      4.27e-06\npolygon 48        15 4.87296e+03      6.51e-06\npolygon 49        66 1.61841e+04      2.16e-05\npolygon 50       149 5.63430e+06      7.53e-03\npolygon 51       609 2.62570e+07      3.51e-02\npolygon 52         8 7.82256e+03      1.04e-05\npolygon 53       976 2.33447e+07      3.12e-02\npolygon 54        55 8.25379e+04      1.10e-04\npolygon 55       976 2.33447e+07      3.12e-02\npolygon 56        61 3.33449e+05      4.45e-04\npolygon 57         6 1.68410e+04      2.25e-05\npolygon 58         4 9.45963e+03      1.26e-05\npolygon 59        46 6.99702e+05      9.35e-04\npolygon 60        13 7.00873e+04      9.36e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 748741000 square units\nFraction of frame area: 0.414\n\n\nUsing plot(), the childcareSG_ppp object can be displayed.\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#kernel-density-estimation",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#kernel-density-estimation",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.6.1 Kernel Density Estimation",
    "text": "3.6.1 Kernel Density Estimation\nIn this section, we will compute the kernel density estimation (KDE) of childcare services in Singapore."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-kernel-density-estimation-using-automatic-bandwidth-selection-method",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-kernel-density-estimation-using-automatic-bandwidth-selection-method",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.6.1.1 Computing Kernel Density Estimation using Automatic Bandwidth Selection Method",
    "text": "3.6.1.1 Computing Kernel Density Estimation using Automatic Bandwidth Selection Method\nThe code chunk below computes a kernel density by using the following configurations of density() of spatstat:\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma = bw.diggle, # determines the area of influence of the estimation\n                              edge = TRUE,\n                              kernel = \"gaussian\")\n\n\nThe plot() function of Base R is then used to display the kernel density derived.\n\nplot(kde_childcareSG_bw)\n\n\n\n\nThe density values of the output range from 0 to 0.000035, which is way too small to comprehend. This is because the default unit of measurement of SVY21 is in meter. As a result, the density values computed is in “number of points per square meter”.\nBefore moving on, it is good to know that you can retrieve the bandwidth used to compute the KDE layer by using the code chunk below.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n281.8312"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#rescalling-kde-values",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#rescalling-kde-values",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.6.1.2 Rescalling KDE values",
    "text": "3.6.1.2 Rescalling KDE values\nIn the code chunk below, rescale() is used to covert the unit of measurement from meter to kilometer for a better scale.\n\nchildcareSG_ppp.km &lt;- rescale(childcareSG_ppp, 1000, \"km\")\n\nNow, we can re-run density() using the resale data set and plot the output KDE map.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km,\n                              sigma = bw.diggle,\n                              edge = TRUE,\n                              kernel = \"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\nThe output image looks identical to the earlier version, the only changes in the data values in the legend."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#working-with-different-automatic-bandwidth-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#working-with-different-automatic-bandwidth-methods",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.6.2 Working with Different Automatic Bandwidth Methods",
    "text": "3.6.2 Working with Different Automatic Bandwidth Methods\nBeside bw.diggle(), there are three other spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\nLet’s look at the bandwidth return by these automatic bandwidth calculation methods by using the code chunk below.\n\nbw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.2818312 \n\n\n\nbw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.111666 1.347496 \n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.2109048 \n\n\nThe code chunk below will be used to compare the output of using bw.diggle and bw.ppl methods.\n\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km,\n                               sigma = bw.ppl,\n                               edge = TRUE,\n                               kernel = \"gaussian\")\npar(mfrow = c(1,2))\nplot(kde_childcareSG_bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#working-with-different-kernel-methods",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#working-with-different-kernel-methods",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.6.3 Working with Different Kernel Methods",
    "text": "3.6.3 Working with Different Kernel Methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Disc.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\n\npar(mfrow = c(2,2))\nplot(density(childcareSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"epanechnikov\"),\n     main = \"Epanechnikov\")\nplot(density(childcareSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"quartic\"),\n     main = \"Quartic\")\nplot(density(childcareSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"disc\"),\n     main = \"Disc\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-kde-by-using-fixed-bandwidth",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-kde-by-using-fixed-bandwidth",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.1 Computing KDE by Using Fixed Bandwidth",
    "text": "3.7.1 Computing KDE by Using Fixed Bandwidth\nNow we compute a KDE layer by defining a bandwidth of 600 meter. The sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km,\n                               sigma = 0.6,\n                               edge = TRUE,\n                               kernal = \"gaussian\")\nplot(kde_childcareSG_600)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-kde-by-using-adaptive-bandwidth",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-kde-by-using-adaptive-bandwidth",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.2 Computing KDE by using Adaptive Bandwidth",
    "text": "3.7.2 Computing KDE by using Adaptive Bandwidth\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units, for example urban versus rural. One way to overcome this problem is by using adaptive bandwidth instead.\nIn this section, you will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method = \"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\nThe fixed and adaptive kernel density estimation outputs can be compared by using the code chunk below.\n\npar(mfrow = c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed Bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive Bandwidth\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#converting-kde-output-into-grid-object",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#converting-kde-output-into-grid-object",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.3 Converting KDE Output into Grid Object",
    "text": "3.7.3 Converting KDE Output into Grid Object\nSame results, just converted for mapping purposes\n\ngridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#converting-gridded-output-into-raster",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#converting-gridded-output-into-raster",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.3.1 Converting Gridded Output into Raster",
    "text": "3.7.3.1 Converting Gridded Output into Raster\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(gridded_kde_childcareSG_bw)\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : -1.274202e-14, 41.20628  (min, max)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#assigning-projection-systems",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#assigning-projection-systems",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.3.2 Assigning Projection Systems",
    "text": "3.7.3.2 Assigning Projection Systems\nThe code chunk below will be used to include the CRS information on kde_childcareSG_bw_raster RasterLayer.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : v \nvalues     : -1.274202e-14, 41.20628  (min, max)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#visualising-the-output-in-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#visualising-the-output-in-tmap",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.4 Visualising the Output in tmap",
    "text": "3.7.4 Visualising the Output in tmap\nFinally, the raster will be displayed in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) +\n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#comparing-spatial-point-patterns-using-kde",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#comparing-spatial-point-patterns-using-kde",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.5 Comparing Spatial Point Patterns using KDE",
    "text": "3.7.5 Comparing Spatial Point Patterns using KDE\nIn this section, we will compare KDE of childcare at Punggol, Tampines, Chua Chu Kang and Jurong West planning areas."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#extracting-study-area",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#extracting-study-area",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.5.1 Extracting Study Area",
    "text": "3.7.5.1 Extracting Study Area\nThe code chunk below will be used to extract the target planning areas.\n\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\nPlotting target planning areas\n\npar(mfrow = c(2,2))\nplot(pg, main = \"Punggol\")\nplot(tm, main = \"Tampines\")\nplot(ck, main = \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#converting-spatial-point-dataframe-into-generic-sp-format",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#converting-spatial-point-dataframe-into-generic-sp-format",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.5.2 Converting Spatial Point DataFrame into Generic sp Format",
    "text": "3.7.5.2 Converting Spatial Point DataFrame into Generic sp Format\nNext, these SpatialPolygonsDataFrame layers will be converted into generic spatialpolygons layers.\n\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#creating-owin-object-1",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#creating-owin-object-1",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.5.3 Creating owin Object",
    "text": "3.7.5.3 Creating owin Object\nNow, these SpatialPolygons objects will be converted into owin objects that is required by spatstat.\n\npg_owin = as(pg_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#combining-childcare-points-and-the-study-area",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#combining-childcare-points-and-the-study-area",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.5.4 Combining Childcare Points and the Study Area",
    "text": "3.7.5.4 Combining Childcare Points and the Study Area\nBy using the code chunk below, childcare centres within the specific region can be extracted for later analysis.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale() is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-kde",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-kde",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.5.5 Computing KDE",
    "text": "3.7.5.5 Computing KDE\nThe code chunk below will be used to compute the KDE of these four planning area. bw.diggle method will be used to derive the bandwidth of each area.\n\npar(mfrow = c(2,2))\nplot(density(childcare_pg_ppp.km,\n             sigma = bw.diggle,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Punggol\")\nplot(density(childcare_tm_ppp.km,\n             sigma = bw.diggle,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Tampines\")\nplot(density(childcare_ck_ppp.km,\n             sigma = bw.diggle,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km,\n             sigma = bw.diggle,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Jurong West\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-fixed-bandwidth-kde",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-fixed-bandwidth-kde",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.7.5.6 Computing Fixed Bandwidth KDE",
    "text": "3.7.5.6 Computing Fixed Bandwidth KDE\nFor comparison purposes, 250m is used as the bandwidth.\n\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#testing-spatial-point-patterns-using-clark-evans-test",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#testing-spatial-point-patterns-using-clark-evans-test",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.8.1 Testing Spatial Point Patterns using Clark-Evans Test",
    "text": "3.8.1 Testing Spatial Point Patterns using Clark-Evans Test\n\nclarkevans.test(childcareSG_ppp,\n                correction = \"none\", \n                clipregion = \"sg_owin\", \n                alternative = c(\"clustered\"), \n                nsim = 99) # Number of Monte Carlo simulations to perform\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.40435, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#clark-and-evans-test-choa-chu-kang-planning-area",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#clark-and-evans-test-choa-chu-kang-planning-area",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.8.2 Clark and Evans Test: Choa Chu Kang planning area",
    "text": "3.8.2 Clark and Evans Test: Choa Chu Kang planning area\nIn the code chunk below, clarkevans.test() of spatstat is used to performs Clark-Evans test of aggregation for childcare centre in Choa Chu Kang planning area.\n\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.764, p-value = 2.051e-05\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#clark-and-evans-test-tampines-planning-area",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#clark-and-evans-test-tampines-planning-area",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.8.3 Clark and Evans Test: Tampines planning area",
    "text": "3.8.3 Clark and Evans Test: Tampines planning area\nIn the code chunk below, the similar test is used to analyse the spatial point patterns of childcare centre in Tampines planning area.\n\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.61874, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#choa-chu-kang-planning-area",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#choa-chu-kang-planning-area",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.10.1 Choa Chu Kang Planning Area",
    "text": "3.10.1 Choa Chu Kang Planning Area"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-g-function-estimation",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-g-function-estimation",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.10.1.1 Computing G-function Estimation",
    "text": "3.10.1.1 Computing G-function Estimation\nThe code chunk below is used to compute G-function using Gest() of spatat package.\n\nG_CK = Gest(childcare_ck_ppp, correction = \"border\") \n\n\nplot(G_CK, xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.10.1.2 Performing Complete Spatial Randomness Test",
    "text": "3.10.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nMonte Carlo test with G-function\n\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_CK.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#tampines-planning-area",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#tampines-planning-area",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.10.2 Tampines Planning Area",
    "text": "3.10.2 Tampines Planning Area"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-g-function-estimation-1",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-g-function-estimation-1",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.10.2.1 Computing G-function Estimation",
    "text": "3.10.2.1 Computing G-function Estimation\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\") \n\n\nplot(G_tm)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-1",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-1",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.10.2.2 Performing Complete Spatial Randomness Test",
    "text": "3.10.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#choa-chu-kang-planning-area-1",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#choa-chu-kang-planning-area-1",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.11.1 Choa Chu Kang Planning Area",
    "text": "3.11.1 Choa Chu Kang Planning Area"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-f-function-estimation",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-f-function-estimation",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.11.1.1 Computing F-function Estimation",
    "text": "3.11.1.1 Computing F-function Estimation\nThe code chunk below is used to compute F-function using Fest() of spatat package.\n\nF_CK = Fest(childcare_ck_ppp)\n\n\n plot(F_CK)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-2",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-2",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.11.2 Performing Complete Spatial Randomness Test",
    "text": "3.11.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nMonte Carlo test with F-function\n\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180\n[1:46:41 remaining, estimate finish 2024-02-13 17:33:26]\n.........190\n[1:39:49 remaining, estimate finish 2024-02-13 17:26:34]\n.........200\n[1:33:39 remaining, estimate finish 2024-02-13 17:20:24]\n.........210\n[1:28:03 remaining, estimate finish 2024-02-13 17:14:49]\n.........220\n[1:22:59 remaining, estimate finish 2024-02-13 17:09:45]\n.........230\n[1:18:21 remaining, estimate finish 2024-02-13 17:05:07]\n.........240\n[1:14:06 remaining, estimate finish 2024-02-13 17:00:52]\n.........250\n[1:10:11 remaining, estimate finish 2024-02-13 16:56:57]\n.........260\n[1:06:35 remaining, estimate finish 2024-02-13 16:53:21]\n.........270\n[1:03:15 remaining, estimate finish 2024-02-13 16:50:01]\n.........280\n[1:00:08 remaining, estimate finish 2024-02-13 16:46:55]\n.........290\n[57:15 remaining, estimate finish 2024-02-13 16:44:02]\n.........300\n[54:34 remaining, estimate finish 2024-02-13 16:41:20]\n.........310\n[52:03 remaining, estimate finish 2024-02-13 16:38:49]\n.........320\n[49:41 remaining, estimate finish 2024-02-13 16:36:28]\n.........330\n[47:28 remaining, estimate finish 2024-02-13 16:34:15]\n.........340\n[45:23 remaining, estimate finish 2024-02-13 16:32:10]\n.........350\n[43:25 remaining, estimate finish 2024-02-13 16:30:12]\n.........360\n[41:33 remaining, estimate finish 2024-02-13 16:28:20]\n.........370\n[39:48 remaining, estimate finish 2024-02-13 16:26:35]\n.........380\n[38:08 remaining, estimate finish 2024-02-13 16:24:55]\n.........390\n[36:33 remaining, estimate finish 2024-02-13 16:23:21]\n.........400\n[35:03 remaining, estimate finish 2024-02-13 16:21:51]\n.........410\n[33:38 remaining, estimate finish 2024-02-13 16:20:25]\n.........420\n[32:16 remaining, estimate finish 2024-02-13 16:19:04]\n.........430\n[30:59 remaining, estimate finish 2024-02-13 16:17:46]\n.........440\n[29:44 remaining, estimate finish 2024-02-13 16:16:32]\n.........450\n[28:34 remaining, estimate finish 2024-02-13 16:15:21]\n.........460\n[27:26 remaining, estimate finish 2024-02-13 16:14:14]\n.........470\n[26:21 remaining, estimate finish 2024-02-13 16:13:09]\n.........480\n[25:19 remaining, estimate finish 2024-02-13 16:12:07]\n.........490\n[24:19 remaining, estimate finish 2024-02-13 16:11:07]\n.........500\n[23:22 remaining, estimate finish 2024-02-13 16:10:10]\n.........510\n[22:27 remaining, estimate finish 2024-02-13 16:09:15]\n.........520\n[21:34 remaining, estimate finish 2024-02-13 16:08:22]\n.........530\n[20:43 remaining, estimate finish 2024-02-13 16:07:31]\n.........540\n[19:54 remaining, estimate finish 2024-02-13 16:06:42]\n.........550\n[19:07 remaining, estimate finish 2024-02-13 16:05:55]\n.........560\n[18:21 remaining, estimate finish 2024-02-13 16:05:10]\n.........570\n[17:37 remaining, estimate finish 2024-02-13 16:04:26]\n.........580\n[16:55 remaining, estimate finish 2024-02-13 16:03:44]\n.........590\n[16:14 remaining, estimate finish 2024-02-13 16:03:03]\n.........600\n[15:34 remaining, estimate finish 2024-02-13 16:02:23]\n.........610\n[14:56 remaining, estimate finish 2024-02-13 16:01:45]\n.........620\n[14:19 remaining, estimate finish 2024-02-13 16:01:08]\n.........630\n[13:43 remaining, estimate finish 2024-02-13 16:00:32]\n.........640\n[13:08 remaining, estimate finish 2024-02-13 15:59:57]\n.........650\n[12:34 remaining, estimate finish 2024-02-13 15:59:24]\n.........660\n[12:02 remaining, estimate finish 2024-02-13 15:58:51]\n.........670\n[11:30 remaining, estimate finish 2024-02-13 15:58:19]\n.........680\n[10:59 remaining, estimate finish 2024-02-13 15:57:49]\n.........690\n[10:29 remaining, estimate finish 2024-02-13 15:57:19]\n.........700\n[10:00 remaining, estimate finish 2024-02-13 15:56:50]\n.........710 [9:32 remaining] .........720 [9:05 remaining] .........730 [8:38 remaining] .........740 [8:12 remaining] .........750 [7:47 remaining] .........760 [7:22 remaining] ........\n.770 [6:58 remaining] .........780 [6:35 remaining] .........790 [6:12 remaining] .........800 [5:50 remaining] .........810 [5:28 remaining] .........820 [5:07 remaining] .........830 [4:46 remaining] ......\n...840 [4:26 remaining] .........850 [4:07 remaining] .........860 [3:47 remaining] .........870 [3:29 remaining] .........880 [3:10 remaining] .........890 [2:52 remaining] .........900 [2:35 remaining] ....\n.....910 [2:18 remaining] .........920 [2:01 remaining] .........930 [1:44 remaining] .........940 [1:28 remaining] .........950 [1:13 remaining] .........960 [57 sec remaining] .........970 [42 sec remaining] ..\n.......980 [27 sec remaining] .........990 [13 sec remaining] ........\n999.\n\nDone.\n\n\n\nplot(F_CK.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#tampines-planning-area-1",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#tampines-planning-area-1",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.11.3 Tampines Planning Area",
    "text": "3.11.3 Tampines Planning Area"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-f-function-estimation-1",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-f-function-estimation-1",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.11.3.1 Computing F-function Estimation",
    "text": "3.11.3.1 Computing F-function Estimation\nMonte Carlo test with F-function\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\") \n\n\nplot(F_tm)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-3",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-3",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.11.3.2 Performing Complete Spatial Randomness Test",
    "text": "3.11.3.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#choa-chu-kang-planning-area-2",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#choa-chu-kang-planning-area-2",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.12.1 Choa Chu Kang Planning Area",
    "text": "3.12.1 Choa Chu Kang Planning Area"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-k-function-estimate",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-k-function-estimate",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.12.1.1 Computing K-function Estimate",
    "text": "3.12.1.1 Computing K-function Estimate\n\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\") \n\n\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-4",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-4",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.12.1.2 Performing Complete Spatial Randomness Test",
    "text": "3.12.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\nK_ck.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#tampines-planning-area-2",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#tampines-planning-area-2",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.12.2 Tampines Planning Area",
    "text": "3.12.2 Tampines Planning Area"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-k-function-estimation",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-k-function-estimation",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.12.2.1 Computing K-function Estimation",
    "text": "3.12.2.1 Computing K-function Estimation\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\") \n\n\nplot(K_tm, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\", xlim=c(0,1000))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-5",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-5",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.12.2.2 Performing Complete Spatial Randomness Test",
    "text": "3.12.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_tm.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#choa-chu-kang-planning-area-3",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#choa-chu-kang-planning-area-3",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.13.1 Choa Chu Kang Planning area",
    "text": "3.13.1 Choa Chu Kang Planning area"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-l-function-estimation",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-l-function-estimation",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.13.1.1 Computing L Function Estimation",
    "text": "3.13.1.1 Computing L Function Estimation\n\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\") \n\n\nplot(L_ck, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(m)\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-6",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-6",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.13.1.2 Performing Complete Spatial Randomness Test",
    "text": "3.13.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#tampines-planning-area-3",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#tampines-planning-area-3",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.13.2 Tampines Planning Area",
    "text": "3.13.2 Tampines Planning Area"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-l-function-estimation-1",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#computing-l-function-estimation-1",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.13.2.1 Computing L-function Estimation",
    "text": "3.13.2.1 Computing L-function Estimation\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\") \n\n\nplot(L_tm, . -r ~ r, ylab= \"L(d)-r\", xlab = \"d(m)\", xlim=c(0,1000))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-7",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex3.html#performing-complete-spatial-randomness-test-7",
    "title": "Hands-on Exercise 3: 1st & 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3.13.2.2 Performing Complete Spatial Randomness Test",
    "text": "3.13.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below will be used to perform the hypothesis testing.\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nThen, plot the model output by using the code below.\n\nplot(L_tm.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this exercise, I will:\n\ninstall and load of basic R packages\nhandle geospatial files\nplot geospatial data\n\n\n\n\nMaster Plan 2014 Subzone Boundary (Web)\nPre-Schools Location\nCycling Path\nSingapore Airbnb Listing CSV\n\n\n\n\nIn this exercise, we will use the tidyverse and sf packages. The p_load function from the package will assist in installing and loading these packages.\n\npacman::p_load(tidyverse, sf)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#datasets",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#datasets",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "Master Plan 2014 Subzone Boundary (Web)\nPre-Schools Location\nCycling Path\nSingapore Airbnb Listing CSV"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#installing-and-loading-r-packages",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this exercise, we will use the tidyverse and sf packages. The p_load function from the package will assist in installing and loading these packages.\n\npacman::p_load(tidyverse, sf)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#master-plan-2014-subzone-boundary",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#master-plan-2014-subzone-boundary",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.2.1 Master Plan 2014 Subzone Boundary",
    "text": "1.2.1 Master Plan 2014 Subzone Boundary\n\n\n\n\n\n\nNote\n\n\n\n\ndsn: filepath\nlayer: file name\n\n\n\n\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe projected CRS of mpsz is SVY21."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#cycling-path",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#cycling-path",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.2.2 Cycling Path",
    "text": "1.2.2 Cycling Path\n\ncyclingpath = st_read(dsn=\"data/geospatial\",\n                       layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nThe projected CRS of cyclingpath is SVY21."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#pre-schools-location",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#pre-schools-location",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.2.3 Pre-Schools Location",
    "text": "1.2.3 Pre-Schools Location\n\npreschool = st_read(dsn = \"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe Geodetic CRS of prechool is WGS 84."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#retrieve-geometries",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#retrieve-geometries",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.3.1 Retrieve Geometries",
    "text": "1.3.1 Retrieve Geometries\nst_geometry(mpsz) displays basic information of the geometries.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#view-attributes-in-simple-feature-dataframe",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#view-attributes-in-simple-feature-dataframe",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.3.2 View Attributes in simple feature DataFrame",
    "text": "1.3.2 View Attributes in simple feature DataFrame\nglimpse() will retrieve each field’s datatype.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#view-complete-information-of-feature-object",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#view-complete-information-of-feature-object",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.3.3 View Complete Information of Feature Object",
    "text": "1.3.3 View Complete Information of Feature Object\nRetrieve top 5 feature object’s complete information.\n\nhead(mpsz, n=5) \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometry-plot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geometry-plot",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.4.1 Geometry Plot",
    "text": "1.4.1 Geometry Plot\nThe code will display a plot with only the basic geometry.\n\nplot(st_geometry(mpsz))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#attribute-plot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#attribute-plot",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.4.2 Attribute Plot",
    "text": "1.4.2 Attribute Plot\nWe can also specify the column of interest to plot only the selected feature.\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#assigning-epsg-code",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#assigning-epsg-code",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.5.1 Assigning EPSG code",
    "text": "1.5.1 Assigning EPSG code\nIn MPSZ, the EPSG code is 9001 although it is projected in SVY21.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nst_set_crs() can be used for EPSG code assignment to 3414.\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#transforming-projection",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#transforming-projection",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.5.2 Transforming Projection",
    "text": "1.5.2 Transforming Projection\nThe geographic coordinate system is not suitable for distance and area measurements. Therefore, transforming from a geographic coordinate system to a projected coordinate system is necessary. To achieve this transformation, use st_transform().\n\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-aspatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-aspatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.6.1 Importing Aspatial Data",
    "text": "1.6.1 Importing Aspatial Data\nWe get a Tibble from using the function read_csv()\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\n\nlist(listings) \n\n[[1]]\n# A tibble: 3,457 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Villa in…  367042 Belinda   East Region         Tampines          1.35\n 2  71896 Home in …  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Home in …  367042 Belinda   East Region         Tampines          1.35\n 4 275343 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Home in …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 Rental u… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 369141 Place to… 1521514 Elizabeth Central Region      Newton            1.31\n# ℹ 3,447 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\nlatitude and longitude are the columns that we are interested to look at."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#converting-from-aspatial-dataframe-to-simple-feature-dataframe",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#converting-from-aspatial-dataframe-to-simple-feature-dataframe",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.6.2 Converting from Aspatial DataFrame to Simple Feature DataFrame",
    "text": "1.6.2 Converting from Aspatial DataFrame to Simple Feature DataFrame\nParameters for st_as_sf():\n\ncoords: Column names for x and y coordinates\ncrs: Coordinate system in EPSG format\n%&gt;% nests st_transform() to convert the DataFrame into SVY21 projected CRS\n\n\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nThe new sf DataFrame can be seen below.\n\nglimpse(listings_sf)\n\nRows: 3,457\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 64, 78, 220, 85, 75, 69, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.13, 0.16, 0.30, 0.15, 0.11, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 7, 51, 51, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 55, 91, 91, 183, 183, 54, 365, 183, 183…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nAs seen, longtitude and latitude columns are dropped, and a geometry column is added."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#buffering",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#buffering",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.7.1 Buffering",
    "text": "1.7.1 Buffering\n\nScenario\nThe authority intends to enhance the existing cycling path. To achieve this, they must obtain 5 meters of reserved land on both sides of the current cycling path. Your responsibility is to ascertain the extent of the land that needs to be acquired and calculate its total area.\n\nst_buffer() is used to calculate buffers.\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                            dist=5, nQuadSegs = 30)\n\nst_area() will compute the area of the buffers and generate an AREA column for buffer_cycling\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nTo derive total land, sum() is used\n\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#point-in-polygon-count",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#point-in-polygon-count",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.7.2 Point in Polygon Count",
    "text": "1.7.2 Point in Polygon Count\n\nScenario\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\n\nPreschools within each planning subzone are identified using st_intersects(). The number of preschools in each subzone is then calculated using Length(). \n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\ntop_n() will find the planning subzone with the highest number of pre-schools\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nParamters of top_n:\n\nx: DataFrame\nn: Number of rows to return\nwt: Ordering"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#density-of-pre-schools-by-planning-subzones",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#density-of-pre-schools-by-planning-subzones",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.7.3 Density of Pre-schools by Planning Subzones",
    "text": "1.7.3 Density of Pre-schools by Planning Subzones\nUse st_area() to derive the area of each planning subzone.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\nmutate() will compute density and create “PreSch Density” column.\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#histogram",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#histogram",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.8.1 Histogram",
    "text": "1.8.1 Histogram\nPlotting a histogram using hist() from base R\n\nhist(mpsz3414$`PreSch Density`)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#ggplot2-library",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#ggplot2-library",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.8.2 ggplot2 Library",
    "text": "1.8.2 ggplot2 Library\n\nggplot(data = mpsz3414,\n       aes(x = as.numeric(`PreSch Density`))) +\n    geom_histogram(bins = 20,\n                   color = \"darkgrey\",\n                   fill = \"lavender\") +\n    labs(title = \"Are pre-schools evenly distributed in Singapore?\",\n         subtitle = \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n         x = \"Pre-school density (per km sq)\",\n         y = \"Frequency\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#scatterplot",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#scatterplot",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.8.3 Scatterplot",
    "text": "1.8.3 Scatterplot\n\nggplot(data = mpsz3414,\n       aes(y = `PreSch Count`,\n            x = as.numeric(`PreSch Density`))) +\n    geom_point(color = \"darkorange\",\n               fill = \"mintcream\") +\n    xlim(0, 40) +\n    ylim(0, 40) +\n    labs(title = \"Are pre-schools evenly distributed in Singapore?\",\n         x = \"Pre-school density (per km sq)\",\n         y = \"Pre-school count\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me!",
    "section": "",
    "text": "Hello! I am Fathima, a Y4 Information Systems from Singapore Management University 😸"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "",
    "text": "In this exercise, I will:\n\nplot choropleth maps using the tmap package\n\n\n\n\nGeospatial: Master Plan 2014 Subzone Boundary (Web)\nAspatial: Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020\n\n\n\n\nThe following packages will be used in this exercise:\n\nreadr for importing delimited text file\ntidyr for tidying data\ndplyr for wrangling data\nsf for handling geospatial data\ntmap for plotting choropleth maps\n\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#datasets",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#datasets",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "",
    "text": "Geospatial: Master Plan 2014 Subzone Boundary (Web)\nAspatial: Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#r-packages",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "",
    "text": "The following packages will be used in this exercise:\n\nreadr for importing delimited text file\ntidyr for tidying data\ndplyr for wrangling data\nsf for handling geospatial data\ntmap for plotting choropleth maps\n\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-geospatial-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-geospatial-data-into-r",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.2.1 Importing Geospatial Data into R",
    "text": "2.2.1 Importing Geospatial Data into R\nUse the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz\n\nmpsz &lt;- st_read(dsn=\"data/geospatial\",\n                layer=\"MP14_SUBZONE_WEB_PL\")\n\n\nmpsz"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-attribute-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-attribute-data-into-r",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.2.2 Importing Attribute Data into R",
    "text": "2.2.2 Importing Attribute Data into R\nUsing read_csv() function of readr package, we will read the DOS csv file.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#preparing-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#preparing-data",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.2.3 Preparing Data",
    "text": "2.2.3 Preparing Data\nA data table containing values for the year 2020 is needed for creating the thematic map. This table should include variables such as PA (Planning Area), SZ (Subzone), and AG (Age Group), which consists of ECONOMY ACTIVE, AGED, TOTAL, and DEPENDENCY categories.\n\nYOUNG: age group 0 to 4 until age group 20 to 24\nECONOMY ACTIVE: age group 25-29 until age group 60-64\nAGED: age group 65 and above\nTOTAL: all age group\nDEPENDENCY: the ratio between young and aged against economy active group"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#wrangling-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#wrangling-data",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.2.3.1 Wrangling Data",
    "text": "2.2.3.1 Wrangling Data\nTo wrangle the data, the following functions will be used:\npivot_wider() of tidyr package, and mutate(), filter(), group_by() and select() of dplyr package\n\n# Filter data for the year 2020 and group by Planning Area (PA), Subzone (SZ), and Age Group (AG)\npopdata2020 &lt;- popdata %&gt;%\n  filter(`Time` == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;% \n  \n  # Create a new data frame with total population for each group\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  \n  # Ungroup after grouping to create summaries for each grouping\n  ungroup() %&gt;%\n  \n  # Pivot the data to increase the number of columns and decrease the number of rows\n  pivot_wider(names_from = AG, values_from = POP) %&gt;%\n  \n  # Add new variables and preserve existing ones\n  mutate(\n    YOUNG = rowSums(.[3:6]) + rowSums(.[12]),\n    `ECONOMY ACTIVE` = rowSums(.[7:11]) + rowSums(.[13:15]),\n    `AGED` = rowSums(.[16:21]),\n    `TOTAL` = rowSums(.[3:21]),\n    `DEPENDENCY` = (`YOUNG` + `AGED`) / `ECONOMY ACTIVE`\n  ) %&gt;%\n  \n  # Select specific columns for the final data frame\n  select(`PA`, `SZ`, `YOUNG`, `ECONOMY ACTIVE`, `AGED`, `TOTAL`, `DEPENDENCY`)\n\n# View the resulting data frame\npopdata2020"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#joining-the-attribute-data-and-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#joining-the-attribute-data-and-geospatial-data",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.2.3.2 Joining the Attribute Data and Geospatial Data",
    "text": "2.2.3.2 Joining the Attribute Data and Geospatial Data\nIt is necessary to convert the PA and SZ fields to uppercase. This is because these fields contain a mix of upper- and lowercase values, unlike the SUBZONE_N and PLN_AREA_N fields, which consistently use uppercase characters.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n                mutate_at(.vars = vars(PA, SZ),\n                          .funs = funs(toupper)) %&gt;%\n                filter(`ECONOMY ACTIVE` &gt; 0)\npopdata2020\n\nleft_join() of dplyr is used to join the attribute data and geospatial data using planning subzone name.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\nmpsz_pop2020\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-a-choropleth-map-with-qtm",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-a-choropleth-map-with-qtm",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.3.1 Plotting a choropleth map with qtm()",
    "text": "2.3.1 Plotting a choropleth map with qtm()\n\ntmap_mode(\"plot\") \nqtm(mpsz_pop2020,\n    fill = \"DEPENDENCY\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-a-choropleth-map-with-tmaps-elements",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-a-choropleth-map-with-tmaps-elements",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.3.2 Plotting a choropleth map with tmap’s elements",
    "text": "2.3.2 Plotting a choropleth map with tmap’s elements\nOne disadvantge of qtm() is that it makes aesthetics of individual layers harder to control.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) + \n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\",\n             position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-a-base-map",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-a-base-map",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.3.2.1 Plotting a base map",
    "text": "2.3.2.1 Plotting a base map\nThe core of tmap is built using tm_shape(), and additional layer elements like tm_fill() and tm_polygons() are used. In the given code snippet, tm_shape() defines the input data (mpsz_pop2020), and tm_polygons() is used to draw the planning subzone polygons.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#drawing-a-choropleth-map-using-tm_polygons",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#drawing-a-choropleth-map-using-tm_polygons",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.3.2.2 Drawing a choropleth map using tm_polygons()",
    "text": "2.3.2.2 Drawing a choropleth map using tm_polygons()\nTo make a choropleth map displaying the variable distribution across planning subzones, assign the target variable (e.g., Dependency) to tm_polygons().\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#drawing-a-choropleth-map-using-tm_fill-and-tm_border",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#drawing-a-choropleth-map-using-tm_fill-and-tm_border",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.3.2.3 Drawing a choropleth map using tm_fill() and tm_border()",
    "text": "2.3.2.3 Drawing a choropleth map using tm_fill() and tm_border()\ntm_polygons() wraps tm_fill() for shading polygons and tm_borders() for adding shapefile borders to the choropleth map.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\")\n\nTo add the boundary of the planning subzones, tm_borders() is used.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1, alpha = 1)\n\nIn addition to the alpha argument, tm_borders() has three other options:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-classification-methods-of-tmap",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.3.3 Data classification methods of tmap",
    "text": "2.3.3 Data classification methods of tmap\nChoropleth maps often use data classification methods to group numerous observations into ranges or classes.\ntmap offers ten classification methods: pretty (default), equal, jenks, fixed, sd, quantile, kmeans, hclust, bclust, and fisher.\nTo specify a data classification method, use the style argument in tm_fill() or tm_polygons()."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-choropleth-maps-with-built-in-classification-methods",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-choropleth-maps-with-built-in-classification-methods",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.3.1 Plotting choropleth maps with built-in classification methods",
    "text": "2.4.3.1 Plotting choropleth maps with built-in classification methods\nThe following shows quantile data classification.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\nThe following shows an equal data classification method.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\nThe following shows fixed data classification method.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"fixed\",\n          breaks = c(0, 2, 6, 13, 19)) + \n  tm_borders(alpha = 0.5)\n\nThe following shows standard deviation method.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\nThe following shows quantile method.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\nThe following shows KMeans method.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\nThe following shows hclust method.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\nThe following shows bclust method.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"bclust\") +\n  tm_borders(alpha = 0.5)\n\nThe following shows fisher method.\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#experimenting-with-different-number-of-classes",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#experimenting-with-different-number-of-classes",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.3.2 Experimenting with Different Number of Classes",
    "text": "2.4.3.2 Experimenting with Different Number of Classes\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"pretty\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-choropleth-map-with-custom-break",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#plotting-choropleth-map-with-custom-break",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.3.3 Plotting choropleth map with custom break",
    "text": "2.4.3.3 Plotting choropleth map with custom break\nTo customize category breaks in tmap, use the breaks argument in tm_fill(). Specify n+1 elements in breaks for n categories (values in increasing order).\nThe provided code calculates and displays descriptive statistics for the DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\nNow, we will plot the choropleth map by using the values we obtained from the previous output.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#colour-scheme",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#colour-scheme",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.4 Colour Scheme",
    "text": "2.4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#using-colourbrewer-palette",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#using-colourbrewer-palette",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.4.1 Using ColourBrewer palette",
    "text": "2.4.4.1 Using ColourBrewer palette\nThe chosen colour will be assigned to the palette argument of tm_fill().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#map-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#map-layouts",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.5 Map Layouts",
    "text": "2.4.5 Map Layouts"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#map-legend",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#map-legend",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.5.1 Map Legend",
    "text": "2.4.5.1 Map Legend\ntmap offers various legend options that allow you to modify the placement, format, and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#map-style",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#map-style",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.5.2 Map style",
    "text": "2.4.5.2 Map style\ntmap enables the alteration of numerous layout settings, accessible through tmap_style().\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#cartographic-furniture",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#cartographic-furniture",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.5.3 Cartographic Furniture",
    "text": "2.4.5.3 Cartographic Furniture\ntmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#drawing-small-multiple-choropleth-maps",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.6 Drawing Small Multiple Choropleth Maps",
    "text": "2.4.6 Drawing Small Multiple Choropleth Maps\nFacet maps consist of multiple maps arranged either side-by-side or stacked vertically. They are useful for visualising changes in spatial relationships concerning another variable, like time.\nIn tmap, facet maps can be created in three ways:\n\nBy assigning multiple values to at least one aesthetic argument\nBy defining a group-by variable in tm_facets()\nBy creating multiple stand-alone maps with tmap_arrange()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#by-assigning-multiple-values-to-at-least-one-aesthetic-argument",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#by-assigning-multiple-values-to-at-least-one-aesthetic-argument",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.6.1 By assigning multiple values to at least one aesthetic argument",
    "text": "2.4.6.1 By assigning multiple values to at least one aesthetic argument\nFacet maps are created by defining ncols in tm_fill().\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#by-defining-a-group-by-variable-in-tm_facets",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#by-defining-a-group-by-variable-in-tm_facets",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.6.2 By defining a group-by variable in tm_facets()",
    "text": "2.4.6.2 By defining a group-by variable in tm_facets()\nFacet maps are created using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#by-creating-multiple-stand-alone-maps-with-tmap_arrange",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#by-creating-multiple-stand-alone-maps-with-tmap_arrange",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.6.3 By creating multiple stand-alone maps with tmap_arrange()",
    "text": "2.4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nFacet maps are created by creating multiple stand-alone maps with tmap_arrange()\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#mapping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#mapping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2.4.7 Mapping Spatial Object Meeting a Selection Criterion",
    "text": "2.4.7 Mapping Spatial Object Meeting a Selection Criterion\nWe can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to compute spatial weights using R. By the end to this hands-on exercise, we will be able to:\n\nimport geospatial data using appropriate function(s) of sf package\nimport csv file using appropriate function of readr package\nperform relational join using appropriate join function of dplyr package\ncompute spatial weights using appropriate functions of spdep package\ncalculate spatially lagged variables using appropriate functions of spdep package"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#overview",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to compute spatial weights using R. By the end to this hands-on exercise, we will be able to:\n\nimport geospatial data using appropriate function(s) of sf package\nimport csv file using appropriate function of readr package\nperform relational join using appropriate join function of dplyr package\ncompute spatial weights using appropriate functions of spdep package\ncalculate spatially lagged variables using appropriate functions of spdep package"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#the-study-area-and-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#the-study-area-and-data",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "4.2 The Study Area and Data",
    "text": "4.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer\nHunan_2012.csv\n\n\n4.2.1 Getting Started\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-the-data-into-r-environment",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "4.3 Getting the Data Into R Environment",
    "text": "4.3 Getting the Data Into R Environment\n\n4.3.1 Import Shapefile into R Environment\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex04\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n4.3.2 Import csv File into R Environment\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n4.3.3 Performing Relational Join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-regional-development-indicator",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-regional-development-indicator",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "4.4 Visualising Regional Development Indicator",
    "text": "4.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap &lt;- tm_shape(hunan) + tm_polygons() + tm_text(\"NAME_3\", size=0.5)\n\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-contiguity-spatial-weights",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "4.5 Computing Contiguity Spatial Weights",
    "text": "4.5 Computing Contiguity Spatial Weights\nIn this section, we will use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.\n\n4.5.1 Computing (QUEEN) Contiguity Based Neighbours\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE) \nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nFor each polygon in our polygon object, wm_q lists all neighboring polygons.\nTo see the neighbors for the first polygon in the object:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors.\nWe can retrive the county name of Polygon ID=1 by using the code chunk below:\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\nTo reveal the county names of the five neighboring polygons:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nWe can display the complete weight matrix by using str().\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\n4.5.2 Creating (ROOK) Contiguity Based Neighbours\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one neighbour.\n\n\n4.5.3 Visualising Contiguity Weights\nThe mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid. We will be using map_dbl variation of map from the purrr package.\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nWe check the first few observations to see if things are formated correctly.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n\n4.5.3.1 Plotting Queen Contiguity Based Neighbours Map\n\n plot(hunan$geometry, border=\"lightgrey\")\n plot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\n\n\n4.5.3.2 Plotting Rook Contiguity Based Neighbours Map\n\n plot(hunan$geometry, border=\"lightgrey\")\n plot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n4.5.3.3 Plotting both Queen and Rook Contiguity Based Neighbours Maps\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-distance-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-distance-based-neighbours",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "4.6 Computing Distance Based Neighbours",
    "text": "4.6 Computing Distance Based Neighbours\nHere we will derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument.\n\n4.6.1 Determine the Cut-off Distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb()\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise\nRemove the list structure of the returned object by using unlist()\n\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n4.6.2 Computing Fixed Distance Weight Matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nNote\n\n\n\nQuiz: What is the meaning of “Average number of links: 3.681818” shown above?\nAns: On average, each region has about 3 to 4 neighboring regions within a distance of 62 km.\n\n\nNext, we will use str() to display the content of wm_d62 weight matrix.\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\n n_comp &lt;- n.comp.nb(wm_d62)\n n_comp$nc\n\n[1] 1\n\n\n\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\n\n\n4.6.2.1 Plotting Fixed Distance Weight Matrix\nNext, we will plot the distance weight matrix by using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nWe can also plot both of them next to each other by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n4.6.3 Computing Adaptive Distance Weight Matrix\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\n\n\n4.6.3.1 Plotting Distance Based Neighbours\nWe can plot the weight matrix using the code chunk below\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#weights-based-on-idw",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#weights-based-on-idw",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "4.7 Weights Based on IDW",
    "text": "4.7 Weights Based on IDW\nIn this section, we will derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\nids\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034\n\n\n\n4.7.1 Row-standardised Weights Matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values.\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nTo see the weight of the first polygon’s eight neighbors type:\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.125 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\n rswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#application-of-spatial-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#application-of-spatial-weight-matrix",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "4.8 Application of Spatial Weight Matrix",
    "text": "4.8 Application of Spatial Weight Matrix\nIn this section, we will create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights\nspatial lag as a sum of neighbouring values\nspatial window average\nspatial window sum\n\n\n4.8.1 Spatial Lag with Row-standardized Weights\nWe will compute the average neighbor GDPPC value for each polygon.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nIn the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n4.8.2 Spatial Lag as a Sum of Neighboring Values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. We start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, glist = b_weights, style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nFirst, let us examine the result by using the code chunk below.\n\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\nhunan &lt;- left_join(hunan, lag.res)\n\nNow, We can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n4.8.3 Spatial Window Average\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\nLook at the neighbour list of area [1] by using the code chunk below.\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \"lag GDPPC\", \"lag_window_avg GDPPC\") %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n4.8.4 Spatial Window Sum\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now [1] has six neighbours instead of five.\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, glist = b_weights, style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable list object into a data.frame by using as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html",
    "title": "Hands-on Exercise 5: Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, wewill learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package.\nWe will:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#overview",
    "title": "Hands-on Exercise 5: Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, wewill learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package.\nWe will:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#getting-started",
    "title": "Hands-on Exercise 5: Local Measures of Spatial Autocorrelation",
    "section": "5.2 Getting Started",
    "text": "5.2 Getting Started\n\n5.2.1 Analytical Question\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.\nThe anaytical questions are:\n\nAre the development distributed geographically?\nIf no, are there sign of spatial clustering?\nIf yes, where are these clusters?\n\n\n\n5.2.2 Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n5.2.3 Load Relevant R Packages\nThe packages are\n\nsf for importing and handling geospatial data in R\ntidyverse for wrangling attribute data in R\nspdep to compute spatial weights, global and local spatial autocorrelation statistics\ntmap to prepare cartographic quality chropleth map\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#load-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#load-data",
    "title": "Hands-on Exercise 5: Local Measures of Spatial Autocorrelation",
    "section": "5.3 Load Data",
    "text": "5.3 Load Data\n\n5.3.1 Import Shapefiles into R\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\",layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex05\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n5.3.2 Import csv File into R\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n5.3.3 Relational Join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%   select(1:4, 7, 15)\n\n\n\n5.3.4 Visualising Regional Development Indicator\nLets prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#global-measures-of-spatial-autocorrelation",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#global-measures-of-spatial-autocorrelation",
    "title": "Hands-on Exercise 5: Local Measures of Spatial Autocorrelation",
    "section": "5.4 Global Measures of Spatial Autocorrelation",
    "text": "5.4 Global Measures of Spatial Autocorrelation\nHere we will compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n5.4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. In the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. \n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n5.4.2 Row-standardised Weights Matrix\nWe need to assign weights to each neighboring polygon. This is done by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#global-measures-of-spatial-autocorrelation-morans-i",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#global-measures-of-spatial-autocorrelation-morans-i",
    "title": "Hands-on Exercise 5: Local Measures of Spatial Autocorrelation",
    "section": "5.5 Global Measures of Spatial Autocorrelation: Moran’s I",
    "text": "5.5 Global Measures of Spatial Autocorrelation: Moran’s I\nHere we will perform Moran’s I statistics testing by using moran.test() of spdep.\n\n5.5.1 Maron’s I Test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC,             listw=rswm_q,             zero.policy = TRUE,             na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer:\n\n\n\n5.5.2 Computing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n5.5.3 Visualising Monte Carlo Moran’s I\nIn the code chunk below hist() and abline() of R Graphics are used.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\nQuestion: What statistical observation can you draw fro mthe output above?\n\n\nChallenge: Instead of using Base Graph to plot the values, plot the values by using ggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "title": "Hands-on Exercise 5: Local Measures of Spatial Autocorrelation",
    "section": "5.6 Global Measures of Spatial Autocorrelation: Geary’s C",
    "text": "5.6 Global Measures of Spatial Autocorrelation: Geary’s C\nHere we will perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n5.6.1 Geary’s C Test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer:\n\n\n\n5.6.2 Computing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer:\n\n\n\n5.6.3 Visualising the Monte Carlo Geary’s C\nWe will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\") \n\n\n\n\n\nQuestion: What statistical observation can you draw from the output?\nAnswer:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#spatial-correlogram",
    "title": "Hands-on Exercise 5: Local Measures of Spatial Autocorrelation",
    "section": "5.7 Spatial Correlogram",
    "text": "5.7 Spatial Correlogram\nSpatial correlograms show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.\n\n5.7.1 Compute Moran’s I Correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. \n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nQuestion: What statistical observation can you draw from the plot above?\nAnswer:\n\n\n\n5.7.2 Compute Geary’s C Correlogram and Plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#cluster-and-outlier-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#cluster-and-outlier-analysis",
    "title": "Hands-on Exercise 5: Local Measures of Spatial Autocorrelation",
    "section": "5.8 Cluster and Outlier Analysis",
    "text": "5.8 Cluster and Outlier Analysis\nHere we will apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran I to detect cluster and outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n5.8.1 Computing Local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n\n5.8.1.1 Mapping the Local Moran’s I\nWe need to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n5.8.1.2 Mapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chunks below.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n5.8.1.3 Mapping Local Moran’s I P-values\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n5.8.1.4 Mapping Both Local Moran’s I Values and P-values\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#creating-a-lisa-cluster-map",
    "title": "Hands-on Exercise 5: Local Measures of Spatial Autocorrelation",
    "section": "5.9 Creating a LISA Cluster Map",
    "text": "5.9 Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n5.9.1 Plotting Moran scatterplot\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n5.9.2 Plotting Moran Scatterplot with Standardised Variable\nFirst we will use scale() to centers and scales the variable.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n5.9.3 Preparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, we derive the spatially lagged variable of interest (i.e. GDPPC) and center the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)\n\nThis is followed by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05       \n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, place non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\nWe can combined all the steps into a single code chunk as shown below\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n5.9.4 Plotting LISA map\nNow, we can build the LISA map by using the code chunk below.\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such a visualisation.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-on Exercise 5: Local Measures of Spatial Autocorrelation",
    "section": "5.10 Hot Spot and Cold Spot Area Analysis",
    "text": "5.10 Hot Spot and Cold Spot Area Analysis\nBesides detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\n\n5.10.1 Getis and Ord’s G-Statistics\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n5.10.2 Deriving Distance-based Weight Matrix\nFirst, we need to define a new set of neighbours.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n\n5.10.2.1 Deriving the Centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n5.10.2.2 Determine the Cut-off Distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\n\n\n5.10.2.3 Computing Fixed Distance Weight Matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\nThe output spatial weights object is called wm62_lw.\n\n\n5.10.3 Computing Adaptive Distance Weight Matrix\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#computing-gi-statistics",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05b.html#computing-gi-statistics",
    "title": "Hands-on Exercise 5: Local Measures of Spatial Autocorrelation",
    "section": "5.11 Computing Gi statistics",
    "text": "5.11 Computing Gi statistics\n\n5.11.1 Gi Statistics Using Fixed Distance\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nthe code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n5.11.2 Mapping Gi Values with Fixed Distance Weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\n\n5.11.3 Gi statistics Using Adaptive Distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n5.11.4 Mapping Gi values with Adaptive Distance Weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-NKDE.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-NKDE.html",
    "title": "In-class Exercise 3: Network Kernel Density Estimation",
    "section": "",
    "text": "Code\npacman::p_load(sf, spNetwork, tmap, classInt, viridis, tidyverse)\n\n\n##Data Import and Preparation\n\n\nCode\nnetwork &lt;- st_read(dsn=\"data/geospatial\", \n                   layer=\"Punggol_St\")\n\nchildcare &lt;- st_read(dsn=\"data/geospatial\",\n                     layer=\"Punggol_CC\")\n\n\n\n\nCode\ntmap_mode('view') #interactive\ntm_shape(childcare) + \n  tm_dots() + \n  tm_shape(network) +\n  tm_lines()\ntmap_mode('plot')\n\n\n\n\nCode\nlixels &lt;- lixelize_lines(network, \n                         750, \n                         mindist = 375) \n\n\n\n\nCode\nsamples &lt;- lines_center(lixels)\n\n\n\n\nCode\ndensities &lt;- nkde(network, \n                  events = childcare,\n                  w = rep(1,nrow(childcare)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, #we aggregate events within a 5m radius (faster calculation)\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\n\n\nCode\n#samples$density &lt;- samples$density*1000\n#lixels$density &lt;- lixels$density*1000\n\n\n\n\nCode\n#tmap_mode('view')\n#tm_shape(lixels)+\n  #tm_lines(col=\"density \")+\n#tm_shape(childcare)+\n  #tm_dots()\n#tmap_mode('plot')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class Exercise 4: Spatial Weights and Application: sf methods",
    "section": "",
    "text": "Code\npacman::p_load(sf, spdep, tmap, tidyverse, knitr, GWmodel)\n\n\n\n\nCode\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\n\n\nCode\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\nCode\nhunan &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\nCode\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5) #size of text \n\ngdppc &lt;- qtm(hunan, \"GDPPC\") #qtm is a faster way to plot maps \ntmap_arrange(basemap, gdppc, asp=1, ncol=2) #organise both maps into 1 row 2 columns\n\n\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\nCode\nhunan_sp &lt;-hunan %&gt;%\n  as_Spatial()\n\n\n\n\nCode\n#gwstat &lt;- gwss(data = hunan_sp,\n         #      vars = \"GDPPC\",\n         #      bw = 6,\n          #     kernel = \"bisquare\",\n           #    adaptive = TRUE,\n           #    longlat = T)\n\n\n\n\nCode\nwm_q &lt;- poly2nb(hunan, queen =TRUE)\nsummary(wm_q)\n#nonzero links = neighbors\n#1st row is no of neighbors\n#2nd row is no of geographical areas\n\n\n\n\nCode\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\n\n\nCode\ncoords &lt;- cbind(longitude, latitude)\n#binds without any identifiers\n\n\n\n\nCode\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\n\n\n\nCode\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\nCode\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n\n\n\nCode\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\n\n\n\nCode\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\n\n\n\nCode\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\n\n\n\nCode\nhunan &lt;- left_join(hunan, lag.res)\n\n\n\n\nCode\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fathima’s IS415 Journey",
    "section": "",
    "text": "Welcome to IS415 Geospatial Analytics and Applications.\nThis is the course website of IS415 for this term. You will find my course work on this website."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#setting-the-scene",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#setting-the-scene",
    "title": "Take-home Exercise 1a [DATA PREPARATION]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "1.1 Setting the scene",
    "text": "1.1 Setting the scene\nUnderstanding how people move around in a city is like figuring out its heartbeat—it shows us the rhythms that shape our urban lives. Thanks to smartphones and technology, we now have a bunch of data about how people move. When we use smart analysis tools like GIS, we can unlock valuable insights that help us plan cities better.\nIn 2020, GRAB shared a set of data called Grab Posisi, all about how people move around in Singapore. This kind of information isn’t just interesting; it’s super helpful for businesses, people who make decisions about the city, and those who plan how cities work. It’s like having a dynamic picture of how people move, helping us create cities that work well for everyone."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#objectives",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#objectives",
    "title": "Take-home Exercise 1a [DATA PREPARATION]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "1.2 Objectives",
    "text": "1.2 Objectives\nThe objectives of this take-home exercise are to:\n\nApply geospatial analytics to address societal challenges\nUse spatial point patterns analysis methods to explore Grab hailing services distribution in Singapore\nOrganise geospatial data into sf tibble data.frames using sf and tidyverse functions\nFocus on Grab taxi location points, road layer within Singapore, and Singapore coastal boundary layer\nGenerate traditional Kernel Density Estimation layers\nCreate Network Kernel Density Estimation (NKDE)\nUtilise tmap functions to display kernel density layers on OSM\nDescribe spatial patterns revealed by the kernel density maps\n\nBy this exercise, I will:\n\nEnhance my understanding of geospatial analytics applications\nDevelop proficiency in spatial point patterns analysis\nGain hands-on experience in dealing with geospatial data\nExplore Grab hailing services distribution patterns in Singapore\nGenerate and interpret Kernel Density Estimation layers\nUnderstand the nuances of Network Kernel Density Estimation (NKDE)\nLearn about the visualisation of spatial patterns using tmap functions on OSM"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#getting-started",
    "title": "Take-home Exercise 1a [DATA PREPARATION]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "2 Getting Started",
    "text": "2 Getting Started"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#data-acquisition",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#data-acquisition",
    "title": "Take-home Exercise 1a [DATA PREPARATION]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "2.1 Data Acquisition",
    "text": "2.1 Data Acquisition\nThe study will utilise the following datasets to explore spatial point patterns analysis methods and reveal the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore.\n\n\n\nDataset Name\nType\nSource\nPath\n\n\n\n\nGrab-Posisi\nAspatial (.parquet)\nhttps://engineering.grab.com/grab-posisi\ndata/aspatial/grabPosisi\n\n\nMaster Plan 2019 Subzone Boundary (No Sea)\nGeospatial (.shp)\nhttps://beta.data.gov.sg/collections/2104/view\ndata/geospatial/MPSZ-2019\n\n\nOpen Street Map Road Data\nGeospatial (.shp)\nhttps://download.geofabrik.de/asia/malaysia-singapore-brunei.html\ndata/geospatial/OSM/gis_osm_roads_free_1"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#importing-relevant-r-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#importing-relevant-r-packages",
    "title": "Take-home Exercise 1a [DATA PREPARATION]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "2.2 Importing Relevant R Packages",
    "text": "2.2 Importing Relevant R Packages\nThe R packages used in this project are:\n\narrow: for reading and writing Parquet files\ndplyr: for data manipulation\nlubridate: for working with date-time data\nmaptools: set of tools for reading and manipulating spatial data formats, such as shapefiles\nraster: reads, writes, manipulates, analyses, and models gridded spatial data\nrgdal: from CRAN, enables users to import, export, and manipulate spatial data within the R environment\nRcolorBrewer: package providing color schemes for maps and other visualizations\nrmapshaper: a package for simplifying and modifying geographic shapes in R\nsf: for importing, managing, and processing geospatial data\nspNetwork: to perform spatial analysis for NKDE\nspatstat: for performing spatial point patterns analysis\ntidyverse: a family of other R packages for performing data science tasks such as importing, wrangling, and visualizing data\ntmap: creating static and interactive maps\nggplot2: used for data visualization\nplotly: interactive graphing library for R\n\nPacman assists us by helping us load R packages that we require.\n\npacman::p_load(arrow, dplyr, lubridate, maptools, raster, rgdal, RColorBrewer, rmapshaper, sf, sp, spNetwork, spatstat, tidyverse, tmap, ggplot2, plotly)\n\n#update.packages(ask = FALSE, dependencies = TRUE)\n## install.packages(\"maptools\", repos = \"https://packagemanager.posit.co/cran/2023-10-13\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#importing-geospatial-datasets",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#importing-geospatial-datasets",
    "title": "Take-home Exercise 1a [DATA PREPARATION]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "2.3 Importing Geospatial Datasets",
    "text": "2.3 Importing Geospatial Datasets\n\n2.3.1 Master Plan 2019 Subzone Boundary (No Sea)\nFor shapefile format, two arguments are required: dsn to define the data path, and layer to provide the shapefile name. \n\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MPSZ-2019\") %&gt;%\n    st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Take-home_Ex\\Take-home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nImportant\n\n\n\nProject Transformation\nGiven that our dataset corresponds to the geographical boundaries of Singapore, it is necessary to specify the appropriate CRS for accurate spatial analysis. To achieve this, the st_transform() function is used to convert the CRS of mpsz_sf to SVY21 (EPSG: 3414)\n\n\n\n\n2.3.2 Coastal Outline\nIn order to create a costal outline of singapore, we will use the st_union function to consolidate all subzone boundaries from mpsz_sf into a single polygon.\n\noutline = mpsz_sf %&gt;% st_union()\n\n\nplot(outline)\n\n\n\n\n\n\n\n\n\n2.3.2.1 Extracting Outer Islands\nAs seen in the figure above, the coastal outline includes outer islands where Grab service is unavailable. Through the code chunk below, we use the subset function to select subzones from the mpsz_sf dataset to exclude. These excluded rows of data are stored in new dataframes.\n\nsemakau &lt;- subset(mpsz_sf,mpsz_sf$SUBZONE_N == \"SEMAKAU\") #western island\nsudong &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"SUDONG\") #western island\nbukom &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N ==  \"JURONG ISLAND AND BUKOM\")\nnorth &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"NORTH-EASTERN ISLANDS\")\nsouth &lt;- subset(mpsz_sf, mpsz_sf$SUBZONE_N == \"SOUTHERN GROUP\")\n\nWe combine the newly created dataframes using the bind_rows function and store the data in another dataframe called outer.\n\nouter &lt;- dplyr::bind_rows(list(semakau,sudong,bukom,north,south))\n\n\n\n2.3.2.2 Rendering a Coastal Boundary Excluding Outer Islands\nWe use the st_union function again to merge the geometries of mpsz_sf and outer, and the st_difference function to eliminate the overlap between the two layers. This results in a coastal boundary, sg_sf, which excludes the outer islands.\n\nsg_sf &lt;- st_difference(st_union(mpsz_sf), st_union(outer))\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have obtained the coastal boundary layer of Singapore, excluding the outer islands.\n\n\n\n#write_rds(sg_sf, \"data/rds/sg_sf.rds\")\nsg_sf &lt;- read_rds(\"data/rds/sg_sf.rds\")\n\n\nplot(sg_sf)\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe’ll save the output in .rds file format to save loading time.\n\n\n\n\n2.3.3 Open Street Map Road Data\nLet’s import the road data obtained from OSM.\n\nallroads = st_read(dsn = \"data/geospatial/OSM\", \n                         layer = \"gis_osm_roads_free_1\")  %&gt;% st_transform(crs = 3414)\n\nReading layer `gis_osm_roads_free_1' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Take-home_Ex\\Take-home_Ex01\\data\\geospatial\\OSM' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1759836 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 99.66041 ymin: 0.8021131 xmax: 119.2601 ymax: 7.514393\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nImportant\n\n\n\nProject Transformation\nGiven that our dataset corresponds to the geographical boundaries of Singapore, it’s necessary to specify the appropriate CRS for accurate spatial analysis. To achieve this, the st_transform() function is used to convert the CRS of allroads to SVY21 (EPSG: 3414)\n\n\nThis dataset encompasses road networks spanning Singapore, Malaysia, and Brunei. To narrow our focus, we will extract roads exclusively within the Singapore boundary using the st_intersection function to check the intersection between sg_sf and allroads.\n\nsg_roads_all &lt;- st_intersection(allroads,sg_sf)\n\n\n#write_rds(sg_roads_all, \"data/rds/sg_roads_all.rds\")\nsg_roads_all &lt;- read_rds(\"data/rds/sg_roads_all.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\nWe’ll save the output in .rds file format to save loading time.\n\n\nThe next step involves examining the various road types within the road network.\n\nunique(sg_roads_all$fclass)\n\n [1] \"primary\"        \"residential\"    \"tertiary\"       \"footway\"       \n [5] \"service\"        \"secondary\"      \"motorway\"       \"motorway_link\" \n [9] \"trunk\"          \"trunk_link\"     \"primary_link\"   \"pedestrian\"    \n[13] \"living_street\"  \"unclassified\"   \"steps\"          \"track_grade2\"  \n[17] \"track\"          \"secondary_link\" \"cycleway\"       \"path\"          \n[21] \"tertiary_link\"  \"track_grade1\"   \"track_grade3\"   \"unknown\"       \n[25] \"track_grade5\"   \"bridleway\"      \"track_grade4\"  \n\n\nGiven our focus on Grab services, which primarily operate on roads excluding expressways (it is not possible for Grab to pick-up or drop off passengers along expressways), we will extract the relevant road types from the road network. To determine which roads are applicable, we can look at the OSM fclass. The image below provides descriptions for each road type, and for our analysis, we will focus on the most relevant types: primary, residential, tertiary, service, secondary, primary_link, secondary_link, and tertiary_link\n\nWe store the selected road types in a dataframe called sg_roads_filtered.\n\nsg_roads_filtered &lt;- c(\"primary\", \"residential\", \"tertiary\", \"service\", \"secondary\", \"primary_link\", \"secondary_link\", \"tertiary_link\")\n\n\nsg_roads &lt;- sg_roads_all[sg_roads_all$fclass %in% sg_roads_filtered, ]\n\nunique(sg_roads$fclass)\n\n[1] \"primary\"        \"residential\"    \"tertiary\"       \"service\"       \n[5] \"secondary\"      \"primary_link\"   \"secondary_link\" \"tertiary_link\" \n\n\n\nwrite_rds(sg_roads, \"data/rds/sg_roads.rds\")\n#sg_roads &lt;- read_rds(\"data/rds/sg_roads.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\nWe’ll save the output in .rds file format to save loading time.\n\n\nWe will now visualise the selected road types within the boundaries of Singapore using tmap.\n\nroad_type_palette &lt;- brewer.pal(12, \"Set3\")\ntmap_mode('plot')\ntm_shape(sg_sf) + \n  tm_borders(lwd = 2, col = 'grey') +\n  tm_shape(sg_roads) + \n  tm_lines(col = \"fclass\", palette = road_type_palette)\n  tm_layout(frame = FALSE, main.title = \"Types of Road Networks in Singapore\")\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have extracted the road layer within Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#importing-aspatial-datasets",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#importing-aspatial-datasets",
    "title": "Take-home Exercise 1a [DATA PREPARATION]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "2.4 Importing Aspatial Datasets",
    "text": "2.4 Importing Aspatial Datasets\n\n2.4.1 Grab-Posisi\nGrab-Posisi, is a GPS trajectory dataset. Each trajectory is serialised in a file in Apache Parquet format.\nWe will use the read_parquet function from the arrow package, to read Parquet files.\n\ngrab0 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00000.parquet\", as_data_frame = TRUE)\ngrab1 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00001.parquet\", as_data_frame = TRUE)\ngrab2 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00002.parquet\", as_data_frame = TRUE)\ngrab3 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00003.parquet\", as_data_frame = TRUE)\ngrab4 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00004.parquet\", as_data_frame = TRUE)\ngrab5 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00005.parquet\", as_data_frame = TRUE)\ngrab6 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00006.parquet\", as_data_frame = TRUE)\ngrab7 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00007.parquet\", as_data_frame = TRUE)\ngrab8 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00008.parquet\", as_data_frame = TRUE)\ngrab9 &lt;- read_parquet(\"data/aspatial/GrabPosisi/part-00009.parquet\", as_data_frame = TRUE)\n\nThen we join all the read files into one dataframe.\n\ngrab &lt;- bind_rows(grab0,grab1, grab2,grab3,grab4,grab5,grab6,grab7,grab8,grab9) \n\n\nhead(grab)\n\n# A tibble: 6 × 9\n  trj_id driving_mode osname  pingtimestamp rawlat rawlng speed bearing accuracy\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 70014  car          android    1554943236   1.34   104.  18.9     248      3.9\n2 73573  car          android    1555582623   1.32   104.  17.7      44      4  \n3 75567  car          android    1555141026   1.33   104.  14.0      34      3.9\n4 1410   car          android    1555731693   1.26   104.  13.0     181      4  \n5 4354   car          android    1555584497   1.28   104.  14.8      93      3.9\n6 32630  car          android    1555395258   1.30   104.  23.2      73      3.9\n\n\nThe head function reveals that there are 9 columns in the dataframe.\nThe field pingtimestamp is not in proper date-time format. It is stored as an int value. The following code chunk converts the data type of pingtimestamp from int to date-time format.\n\ngrab$pingtimestamp &lt;- as_datetime(grab$pingtimestamp)\n\n\n\n2.4.1.2 Converting Aspatial Data Frame into a Simple Feature Data Frame\nWe will proceed to convert the grab dataset, currently in an aspatial data frame, into an sf tibble dataframe.\n\ngrab_sf &lt;- st_as_sf(grab, \n                       coords = c(\"rawlng\", \"rawlat\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n# write_rds(grab_sf, \"data/rds/grab_sf.rds\")\ngrab_sf &lt;- read_rds(\"data/rds/grab_sf.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\nWe’ll save the output in .rds file format to save loading time.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nProject Transformation\nAssuming the dataset is initially in the WGS84 Geographic Coordinate System, as indicated by the latitude/longitude fields, we need to define the suitable CRS for spatial analysis within Singapore. The st_transform() function is utilised to convert the CRS of the grab dataset to SVY21 (EPSG: 3414).\n\n\nThis gives us the new simple feature data frame, grab_sf"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#data-wrangling",
    "title": "Take-home Exercise 1a [DATA PREPARATION]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "2.5 Data Wrangling",
    "text": "2.5 Data Wrangling\n\n2.5.1 Extracting Grab Trips Starting Locations\nWe will extract trip starting point for all unqiue trajectories and store them to a new df named grab_origin. To isolate the origin locations, we use the following methodology:\n\nGrouping by Trajectory ID:\n\nThe dataset is grouped by the unique trajectory identifier (trj_id).\n\nArranging by Timestamp:\n\nWithin each trajectory group, records are arranged in ascending order based on the timestamp (pingtimestamp).\n\nFiltering for the First Row:\n\nBy selecting the first row within each grouped trajectory (row_number() == 1), we identify the earliest recorded location for each trip. This is indicative of the trip’s starting point.\n\nAdding Temporal Information:\n\nAdditional temporal context is provided by introducing new columns:\n\nweekday: Day of the week based on the timestamp\nstart_hr: Starting hour of the trip\nday: Day of the month when the trip started\n\n\n\n\ngrab_origin &lt;- grab_sf %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(pingtimestamp) %&gt;%\n  filter(row_number()==1) %&gt;% #after sorting by timestamp, first row gives origin location \n  mutate(weekday = wday(pingtimestamp, #define workday\n                        label = TRUE,\n                        abbr = TRUE), #Monday = MON \n        start_hr = factor(hour(pingtimestamp)),\n        day = factor(mday(pingtimestamp))) #to change to ordinal scale\n\n\n#write_rds(grab_origin, \"data/rds/grab_origin.rds\")\ngrab_origin &lt;- read_rds(\"data/rds/grab_origin.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\nWe’ll save the output in .rds file format to save loading time.\n\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have retrieved the origin location coordinates for Grab services.\n\n\n\n\n2.5.2 Extracting Grab Trips Ending Locations\nWe will extract trip ending point for all unique trajectories and store them to a new df named grab_dest. We employ a similar methodology to extracting a trips origin location. Except here, within each trajectory group, records are arranged in descending order based on the timestamp (pingtimestamp). By selecting the first row within each grouped trajectory (row_number() == 1), we identify the latest recorded location for each trip. This corresponds to the trip’s ending point.\n\ngrab_dest &lt;- grab_sf %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(desc(pingtimestamp)) %&gt;% #function from dplyr\n  filter(row_number()==1) %&gt;% #first row after arranging in desc order gives dest  \n  mutate(weekday = wday(pingtimestamp, #define workday\n                        label = TRUE,\n                        abbr = TRUE), #Monday = MON \n        end_hr = factor(hour(pingtimestamp)),\n        day = factor(mday(pingtimestamp))) #to change to ordinal scale\n\n\n#write_rds(grab_dest, \"data/rds/grab_dest.rds\")\ngrab_dest &lt;- read_rds(\"data/rds/grab_dest.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\nWe’ll save the output in .rds file format to save loading time.\n\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have retrieved the destination location coordinates for Grab services.\n\n\n\n\n2.5.3 Converting Simple Features to Planar Point Pattern Objects\nTo perform spatial point pattern analysis, firstly, we’ll need to convert the simple features objects (grab_orign and grab_dest) into Spatial* classes. For this we use as.ppp from spatstat package. The st_coordinates function extracts the coordiates from our sf objects and st_bbox will extract the minimum and maximum coordinates of the object along each axis.\n\ngrab_origin_ppp &lt;- as.ppp(st_coordinates(grab_origin), st_bbox(grab_origin))\n\n\npar(mar = c(1,1,1,1))\nplot(grab_origin_ppp, main = \"Grab Origin Points as PPP Objects\")\n\n\n\n\n\n\n\ngrab_dest_ppp &lt;- as.ppp(st_coordinates(grab_dest), st_bbox(grab_dest))\n\n\npar(mar = c(1,1,1,1))\nplot(grab_dest_ppp, main = \"Grab Destination Points as PPP Objects\")\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is not neccessary to change sg_sf into a ppp object as it will be converted to owin instead.\n\n\n\n\n2.5.4 Check for Duplicates and Handle Data Errors\nLet’s have a look at the ppp objects we have created to make sure that there is no duplicates or errors in our data.\n\nsummary(grab_origin_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.473666e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3628.24, 49845.23] x [25198.14, 49689.64] units\n                    (46220 x 24490 units)\nWindow area = 1131920000 square units\n\nsummary(grab_dest_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.493661e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3637.21, 49870.63] x [25221.3, 49507.79] units\n                    (46230 x 24290 units)\nWindow area = 1122850000 square units\n\n\n\n\n\n\n\n\nNote\n\n\n\ngrab_origin_ppp and grab_dest_ppp objects have no duplicated points, but just to be sure, we check again using the any(duplicated() function.\n\n\n\nany(duplicated(grab_origin_ppp)) \n\n[1] FALSE\n\nany(duplicated(grab_dest_ppp)) \n\n[1] FALSE\n\n\nThe output is false for both objects so we move on to the next step.\n\n\n2.5.5 Introducing OWIN\nWhen analysing spatial point patterns, we’ll confine our analysis within a certain geographical area - such as the Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\n\n\n2.5.5.1 Creating OWIN Object\nTo create a two dimensional observation window using sg_sf coastal boundary we created earlier, we use the as.owin function.\n\nsg_owin &lt;- as.owin(sg_sf)\n\n\n#write_rds(sg_owin, \"data/rds/sg_owin\")\nsg_owin &lt;- read_rds(\"data/rds/sg_owin\")\n\n\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe’ll save the output in .rds file format to save loading time.\n\n\n\n\n2.5.5.2 Combining Point Events and OWIN Object\nNow, we’ll extract the relevant point events that are located within Singapore.\n\n\n2.5.5.2.1 Origin Points in OWIN Object\n\ngrab_origin_ppp_sg &lt;- grab_origin_ppp[sg_owin]\n\n\n#write_rds(grab_origin_ppp_sg, \"data/rds/grab_origin_ppp_sg.rds\")\ngrab_origin_ppp_sg &lt;- read_rds(\"data/rds/grab_origin_ppp_sg.rds\")\n\n\npar(mar = c(1,1,1,1))\nplot(grab_origin_ppp_sg, main = '[OWIN] Grab Origin Points' )\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe’ll save the output in .rds file format to save loading time.\n\n\n\n\n2.5.5.2.1 Destination Points in OWIN Object\n\ngrab_dest_ppp_sg &lt;- grab_dest_ppp[sg_owin]\n\n\n#write_rds(grab_dest_ppp_sg, \"data/rds/grab_dest_ppp_sg.rds\")\ngrab_dest_ppp_sg &lt;- read_rds(\"data/rds/grab_dest_ppp_sg.rds\")\n\n\npar(mar = c(1,1,1,1))\nplot(grab_dest_ppp_sg, main = '[OWIN] Grab Destination Points' )\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe’ll save the output in .rds file format to save loading time."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#exploratory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#exploratory-data-analysis",
    "title": "Take-home Exercise 1a [DATA PREPARATION]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "3.0 Exploratory Data Analysis",
    "text": "3.0 Exploratory Data Analysis\nTo better understand the spatial data, we employ a series of exploratory techniques.\n\n3.1 Spatial Relationship between Origin and Destination Points\nThe map below shows the geographical distribution of origin and destination points. By overlaying both sets of points, we can observe areas where origin and destination locations overlap. These overlapping areas indicate regions with bidirectional travel.\n\ncombined_map &lt;- ggplot() +\n  geom_sf(data = grab_origin, aes(color = \"Origin\"), alpha = 0.7) +\n  geom_sf(data = grab_dest, aes(color = \"Destination\"), alpha = 0.7) +\n  ggtitle(\"Spatial Relationship Between Origin and Destination Locations\") +\n  labs(color = \"Location Type\") +\n  scale_color_manual(values = c(\"Origin\" = \"blue\", \"Destination\" = \"red\"), \n                     name = \"Location Type\", \n                     labels = c(\"Origin\", \"Destination\")) +\n  theme_minimal()\n\nprint(combined_map)\n\n\n\n\n\n\n\n\n3.2 Spatio-Temporal Visualisations\nSpatio-temporal visualisations show us a view of how data evolves over both space and time. These visualisations are particularly useful for analysing patterns, trends, and relationships within datasets.\n\n\n3.2.1 Frequency of Trip by Hour\nLet’s create interactive plots to provide an insightful representation of the distribution of origin/destination locations and emphasise the specific hours that stand out in terms of pickup/drop-off frequency.\n\n\n3.2.1.1 Origin or Pickup Frequency\nFor creating a bar plot with ggplot, the pickup hours are first converted to numeric values for analysis. TThe x-axis represents the pickup hours, and the y-axis represents the count of origin locations. To highlight the hours with the highest count with a visual indication, we add a red dashed vertical line on the plot. The ggplot object is then converted into an interactive plot using ggplotly, allowing users to hover over bars for detailed information.\n\ngrab_origin$start_hr &lt;- as.numeric(grab_origin$start_hr)\norg&lt;- ggplot(data = grab_origin, aes(x = start_hr)) +\n  geom_bar() +\n  labs(title = \"Distribution of Origin Locations by Pickup Hour\",\n       x = \"Pickup Hour\",\n       y = \"Count\") +\n  scale_x_continuous(breaks = seq(0, 24, by = 1)) +\n  theme_minimal()\nmax_hour &lt;- which.max(table(grab_origin$start_hr))\norg &lt;- org + geom_vline(xintercept = max_hour, linetype = \"dashed\", color = \"red\")\n\norg &lt;- ggplotly(org)\norg\n\n\n\n\n\n\nThe chart above indicates that most pickups occur at 2-3pm, a potential reason could include:\n\nAfter-School Activities: this time period may coincide with school release times and after-school activities. Parents might be picking up children or transporting them to various activities.\n\n\n\n3.2.1.2 Destination or Drop-off Frequency\nFor creating a bar plot with ggplot, the drop-off hours are first converted to numeric values for analysis. TThe x-axis represents the drop-off hours, and the y-axis represents the count of destination locations. To highlight the hours with the highest count with a visual indication, we add a red dashed vertical line on the plot. The ggplot object is then converted into an interactive plot using ggplotly, allowing users to hover over bars for detailed information.\n\ngrab_dest$end_hr &lt;- as.numeric(grab_dest$end_hr)\ndest&lt;- ggplot(data = grab_dest, aes(x = end_hr)) +\n  geom_bar() +\n  labs(title = \"Distribution of Destinatioon Locations by Drop-off Hour\",\n       x = \"Drop-off Hour\",\n       y = \"Count\") +\n  scale_x_continuous(breaks = seq(0, 24, by = 1)) +\n  theme_minimal()\nmax_hour &lt;- which.max(table(grab_dest$end_hr))\ndest &lt;- dest + geom_vline(xintercept = max_hour, linetype = \"dashed\", color = \"red\")\n\ndest &lt;- ggplotly(dest)\ndest\n\n\n\n\n\n\nThe chart above indicates that most drop-offs occur at 2pm, a potential reason could include:\n\nLunchtime Rush: 2pm is often around the time people finish their lunch breaks. This could result in increased travel demand as people return to work or resume their activities.\n\n\n\n3.2.2 Frequency of Trip by Day of Week\nLet’s create a bar chart to explore the frequency of origins/destinations across different days and identify any noteworthy patterns.\n\n\n3.2.2.1 Origin or Pickup Frequency by Days\nThe code uses ggplot to create a bar plot, where each bar represents the frequency of pickups on a specific day. This code produces a informative visualisation to explore patterns in pickup frequency throughout the week.\n\norigin_day &lt;- ggplot(data = grab_origin, aes(x = weekday))  +\n              geom_bar(fill = \"#4C78A8\", color = \"#4C78A8\", alpha = 0.8) +\n              labs(title = \"Frequency of Pickup by Day of Week\",\n                   x = \"Day\",\n                   y = \"Count\") +\n              theme_minimal()\norigin_day &lt;- ggplotly(origin_day)\norigin_day\n\n\n\n\n\n\n\nThe distribution of trips across days appears generally uniform, with a subtle increase observed on Wednesdays.\n\n\n\n3.2.2.2 Destination or Drop-off Frequency by Days\nThe code uses ggplot to create a bar plot, where each bar represents the frequency of drop-offs on a specific day. This code produces a informative visualisation to explore patterns in drop-off frequency throughout the week.\n\ndest_day &lt;- ggplot(data = grab_dest, aes(x = weekday))  +\n              geom_bar(fill = \"#FF0000\", color = \"#FF0000\", alpha = 0.8) +\n              labs(title = \"Frequency of Drop-off by Day of Week\",\n                   x = \"Day\",\n                   y = \"Count\") +\n              theme_minimal()\ndest_day &lt;- ggplotly(dest_day)\ndest_day\n\n\n\n\n\n\n\nIf we plot by destination, we see the same result. This is expected, considering that each trip’s origin corresponds to a destination."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#first-order-spatial-point-patterns-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01a.html#first-order-spatial-point-patterns-analysis",
    "title": "Take-home Exercise 1a [DATA PREPARATION]: Application of Spatial Point Patterns Analysis to Discover the Geographical Distribution of Grab Hailing Services in Singapore",
    "section": "4.0 First-Order Spatial Point Patterns Analysis",
    "text": "4.0 First-Order Spatial Point Patterns Analysis\nFirst-order spatial point pattern analysis focuses on the distribution of individual points in a given spatial domain. It involves examining the basic characteristics and properties of the point pattern itself, without considering the interactions or relationship between points.\n\n4.1 Rescale grab_original_ppp_sg and grab_dest_ppp_sg\nFor further analysis, it is necessary to convert our data to kilometers, and we can achieve this by utilising the rescale function on grab_original_ppp_sg and grab_dest_ppp_sg which are in metres.\n\ngrab_origin_ppp_sg_km &lt;- rescale(grab_origin_ppp_sg, 1000, 'km')\ngrab_dest_ppp_sg_km &lt;- rescale(grab_dest_ppp_sg, 1000, 'km')\n\n\n#write_rds(grab_origin_ppp_sg_km, \"data/rds/grab_origin_ppp_sg_km.rds\")\ngrab_origin_ppp_sg_km &lt;- read_rds(\"data/rds/grab_origin_ppp_sg_km.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\nWe’ll save the output in .rds file format to save loading time.\n\n\n\n#write_rds(grab_dest_ppp_sg_km, \"data/rds/grab_dest_ppp_sg_km.rds\")\ngrab_dest_ppp_sg_km &lt;- read_rds(\"data/rds/grab_dest_ppp_sg_km.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\nWe’ll save the output in .rds file format to save loading time.\n\n\n\nProceed to Take-home Exercise 1b: Analysis"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/data/geospatial/TAINAN_VILLAGE.html",
    "href": "Take-home_Ex/Take-home_Ex02/data/geospatial/TAINAN_VILLAGE.html",
    "title": "IS415 - GAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“TWD97”,DATUM[“Taiwan Datum 1997”,ELLIPSOID[“GRS 1980”,6378137,298.257222101,LENGTHUNIT[“metre”,1]]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“Taiwan, Republic of China - onshore and offshore - Taiwan Island, Penghu (Pescadores) Islands.”],BBOX[17.36,114.32,26.96,123.61]],ID[“EPSG”,3824]] +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs 27230 3824 EPSG:3824 TWD97 longlat EPSG:7019 true"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "",
    "text": "Reading layer `TAINAN_VILLAGE' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Take-home_Ex\\Take-home_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 649 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0269 ymin: 22.88751 xmax: 120.6563 ymax: 23.41374\nGeodetic CRS:  TWD97"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#choropleth-maps",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#choropleth-maps",
    "title": "Take-home Exercise 2 [DATA PREPARATION]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.3.5 Choropleth Maps",
    "text": "2.3.5 Choropleth Maps\nNow we will create a series of choropleth maps to visualise the distribution of dengue cases across various counties through August to December.\n\ntmap_mode(\"plot\")\ntm_shape(dengue_join) +\n  tm_fill(\"case_count\",\n          style = \"quantile\",\n          palette = \"Reds\",\n          title = \"Number of Dengue Cases\",\n          legend.show = TRUE,\n          popup.vars = c(\"case_count\")) +\n  tm_layout(main.title = \"Dengue Cases in Tainan\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.5,\n            legend.width = 0.4,\n            frame = TRUE) +\n  tm_borders(alpha = 0.8) +\n  tm_compass(type = \"arrow\", size = 1.5) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\nLet’s visualise the case distribution by month to see if we can gather any insight about dengue cases and month of the year.\nFirst classify the weeks into their respective months.\n\nAugust 2023: 31, 32, 33, 34, 35\nSeptember 2023: 36, 37, 38, 39\nOctober 2023: 40, 41, 42, 43\nNovember 2023: 44, 45, 46, 47\nDecember 2023: 48, 49, 50\n\n\ncreate_dengue_map &lt;- function(data, title, start_week, end_week) {\n  map_data &lt;- data %&gt;%\n    filter(week_number &gt;= start_week & week_number &lt;= end_week)\n  \n  tmap_mode(\"plot\")\n  tm_shape(map_data) +\n    tm_fill(\"case_count\",\n            style = \"quantile\",\n            palette = \"Reds\",\n            title = paste(\"Count\")) +\n        tm_layout(main.title = paste(title),\n              main.title.position = \"center\",\n              main.title.size = 1,  \n              legend.height = 0.5,  \n              legend.width = 0.4,  \n              frame = TRUE) +\n    tm_borders(alpha = 0.8) + \n        tm_grid(alpha = 0.2)\n}\n\naug_map &lt;- create_dengue_map(dengue_join, \"August\", 31, 35)\nsep_map &lt;- create_dengue_map(dengue_join, \"September\", 36, 39)\noct_map &lt;- create_dengue_map(dengue_join, \"October\", 40, 43)\nnov_map &lt;- create_dengue_map(dengue_join, \"November\", 44, 47)\ndec_map &lt;- create_dengue_map(dengue_join, \"December\", 48, 50)\n\ntmap_arrange(aug_map, sep_map, oct_map, nov_map, dec_map)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThroughout the months from August to December, a noticeable trend emerges in Tainan’s various counties: the intensity of red on the maps steadily diminishes. This reduction suggests a decline in the number of reported dengue cases each month. Several factors could contribute to this decline, including effective control measures such as prompt medical treatment and public health interventions. Additionally, the decreasing temperatures towards December might play a role, as mosquitoes, responsible for dengue transmission, thrive in warmer environments.\nMoreover, another observation is the spatial clustering of high dengue case numbers in neighboring counties. Understanding these geographical patterns becomes important for targeted interventions and collaborative efforts among neighboring regions to address and mitigate the prevalence of the disease.\n\n\n\nProceed to Take-home Exercise 2b: Analysis"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#deriving-contiguity-weights-queens-method",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#deriving-contiguity-weights-queens-method",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.4.1 Deriving Contiguity Weights: Queen’s Method",
    "text": "2.4.1 Deriving Contiguity Weights: Queen’s Method\nQueen’s method is one of the techniques used to derive contiguity weights for spatial data. Contiguity weights define the relationships between spatial units based on their contiguity. The Queen’s method considers two spatial units as neighbors if they share any common boundary point.\n\nwm_q &lt;- dengue_join %&gt;%\n  mutate(nb = st_contiguity(geometry, queen=TRUE),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\n\nglimpse(wm_q)\n\nRows: 258\nColumns: 14\n$ nb         &lt;nb&gt; &lt;2, 43, 45, 203, 206&gt;, &lt;1, 3, 10, 43, 45&gt;, &lt;2, 4, 6, 10, 45&gt;…\n$ wt         &lt;list&gt; &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0.2…\n$ VILLCODE   &lt;chr&gt; \"67000320003\", \"67000320041\", \"67000320035\", \"67000320012\",…\n$ COUNTYNAME &lt;chr&gt; \"臺南市\", \"臺南市\", \"臺南市\", \"臺南市\", \"臺南市\", \"臺南市\",…\n$ TOWNNAME   &lt;chr&gt; \"東區\", \"東區\", \"東區\", \"東區\", \"東區\", \"東區\", \"東區\", \"東…\n$ VILLNAME   &lt;chr&gt; \"大智里\", \"崇文里\", \"崇成里\", \"崇明里\", \"大福里\", \"崇德里\",…\n$ VILLENG    &lt;chr&gt; \"Dazhi Vil.\", \"Chongwen Vil.\", \"Chongcheng Vil.\", \"Chongmin…\n$ COUNTYID   &lt;chr&gt; \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\", \"D\",…\n$ COUNTYCODE &lt;chr&gt; \"67000\", \"67000\", \"67000\", \"67000\", \"67000\", \"67000\", \"6700…\n$ TOWNID     &lt;chr&gt; \"D01\", \"D01\", \"D01\", \"D01\", \"D01\", \"D01\", \"D01\", \"D01\", \"D0…\n$ TOWNCODE   &lt;chr&gt; \"67000320\", \"67000320\", \"67000320\", \"67000320\", \"67000320\",…\n$ NOTE       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ case_count &lt;dbl&gt; 83, 89, 52, 51, 35, 198, 46, 124, 110, 173, 28, 166, 132, 3…\n$ geometry   &lt;POLYGON [°]&gt; POLYGON ((120.2332 22.9659,..., POLYGON ((120.2307 …\n\n\n\nOur hypothesis are:\n\nH0: The spatial distribution of dengue cases per villages are randomly distributed.\nH1: The spatial distribution of dengue cases per villages are not randomly distributed."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#computing-global-morans-i",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#computing-global-morans-i",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.4.2 Computing Global Moran’s I",
    "text": "2.4.2 Computing Global Moran’s I\nGlobal Moran’s I helps us assess the overall spatial autocorrelation of a variable across an entire study area. It helps identify whether there is a significant spatial pattern of similarity or dissimilarity in the distribution of values. Here we use global_moran() to compute the Moran’s I value.\n\nmoranI &lt;- global_moran(wm_q$case_count,\n                       wm_q$nb,\n                       wm_q$wt)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.396\n $ K: num 4.88"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#performing-global-morans-i-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#performing-global-morans-i-test",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.4.3 Performing Global Moran’s I Test",
    "text": "2.4.3 Performing Global Moran’s I Test\nMoran’s I test can be performed by using global_moran_test(). This can check whether there is spatial autocorrelation in case_count.\n\nglobal_moran_test(wm_q$case_count,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 10.886, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.396194784      -0.003891051       0.001350842"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#performing-global-morans-i-permutation-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#performing-global-morans-i-permutation-test",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.4.4 Performing Global Moran’s I Permutation Test",
    "text": "2.4.4 Performing Global Moran’s I Permutation Test\nWe use the Monte Carlo simulation to perform the statistical tests.\n\n\n\n\n\n\nNote\n\n\n\nOur hypothesis are:\n\nH0: The spatial distribution of dengue cases per counties are randomly distributed.\nH1: The spatial distribution of dengue cases per counties are not randomly distributed.\n\n\n\nWe need to use set.seed() before executing the simulation to ensure consistency and reproducibility in the simulation process.\n\nset.seed(1234)\n\nThen we use global_moran_perm() to perform the Monte Carlo simulation.\n\nglobal_moran_perm(wm_q$case_count,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.39619, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nSince the p-value is smaller than the alpha value of 0.05, we will reject the null hypothesis. The spatial distribution of dengue cases per county are not randomly distributed. The Moran I statistics has a value of 0.397, since it is greater than 0, we can conclude that the spatial distribution of dengue cases does show signs of clustering.\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have utilised the extracted data to conduct a global spatial autocorrelation analysis."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#computing-local-morans-i",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#computing-local-morans-i",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.5.1 Computing Local Moran’s I",
    "text": "2.5.1 Computing Local Moran’s I\nWe use local_moran() to compute Local Moran’s I statistic to test for spatial autocorrelation in the data.\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    case_count, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-local-morans-i",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-local-morans-i",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.5.2 Visualising Local Moran’s I",
    "text": "2.5.2 Visualising Local Moran’s I\nHere, we use tmap to plot a choropleth map by using value in the ii field (Local Moran statistic)\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.8) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Local Moran's I of Dengue Cases in Tainan\",\n            main.title.size = 0.9)\n\n\n\n\nWe can see two distinct colour families (orange and green), representing two types of clusters.\n\nHigh-High Clusters 🟠\nThere is a high number of dengue cases and the neighbouring villages around it also have a high number of dengue cases\nLow-High Outliers 🟢\nThe county itself has a low number of dengue cases and the neighbouring villages around it has a high number of dengue cases"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-p-value-of-local-morans-i",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-p-value-of-local-morans-i",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.5.3 Visualising p-value of Local Moran’s I",
    "text": "2.5.3 Visualising p-value of Local Moran’s I\nHere, we use tmap to plot a choropleth map by using value in the p_ii_sim field\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\", palette = \"Reds\") + \n  tm_borders(alpha = 0.8) +\n  tm_layout(main.title = \"p-value of Local Moran's I\",\n            main.title.size = 1)\n\n\n\n\nThere are also 2 distinct cluster classes here.\n\nLight Red⚪\nThese classes have values closer to 0. They do not have significant clustering.\nDark Red 🔴\nThese classes have values closer to 1. They have significant clustering and neighboring areas exhibit similar values."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-local-morans-i-and-p-value",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-local-morans-i-and-p-value",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.5.4 Visualising Local Moran’s I and p-value",
    "text": "2.5.4 Visualising Local Moran’s I and p-value\nLet’s plot the two maps next to each to compare.\n\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.8) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Local Moran's I of Dengue Cases in Tainan\",\n      main.title.size = 0.6)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii\", palette = \"Reds\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.8) +\n  tm_layout(main.title = \"p-value of Local Moran's I of Dengue Cases in Tainan\",\n            main.title.size = 0.6)\n\ntmap_arrange(map1, map2, ncol = 2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-lisa-map",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-lisa-map",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.5.5 Visualising LISA map",
    "text": "2.5.5 Visualising LISA map\nA LISA (Local Indicators of Spatial Association) map is a graphical representation of local spatial autocorrelation analysis results. It is used to visualise spatial patterns of clustering for a variable across different geographic locations within a study area. A LISA map is able to reveal both clusters and outliers. A LISA map is able to identifie High-High and Low-Low clusters and High-Low and Low-High outliers.\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\nHigh-High Clusters 🔴\nThere is a high number of dengue cases and the neighbouring villages around it also have a high number of dengue cases. This information could be useful for local authorities who should focus their efforts on these villages that are experiencing a higher number of cases and are more prone to outbreaks.\nLow-Low Outliers 🟢\nThe county itself has a low number of dengue cases and the neighbouring villages around it has a low number of dengue cases. This could indicate that the prevention measures taken here are adequate or working well.\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have utilised the extracted data to conduct a local spatial autocorrelation analysis"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#computing-local-gi-statistics",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#computing-local-gi-statistics",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.6.1 Computing Local Gi* Statistics",
    "text": "2.6.1 Computing Local Gi* Statistics\nThe local_gi() function calculates the local Gi* statistic using a contiguity relationship matrix, assessing spatial autocorrelation in the data. The computed values and associated p-values help determine if the observed spatial pattern significantly deviates from what would be expected under spatial randomness.\n\nwm_idw &lt;- dengue_join %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    case_count, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 258 features and 21 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0627 ymin: 22.89401 xmax: 120.2925 ymax: 23.09144\nGeodetic CRS:  TWD97\n# A tibble: 258 × 22\n   gi_star    e_gi   var_gi p_value   p_sim p_folded_sim skewness kurtosis nb   \n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;nb&gt; \n 1   1.47  0.00398  1.93e-6   1.47  1.42e-1         0.2      0.1     0.454 &lt;int&gt;\n 2   0.986 0.00412  1.95e-6   0.855 3.92e-1         0.46     0.23    0.389 &lt;int&gt;\n 3   1.35  0.00355  1.71e-6   1.75  7.93e-2         0.16     0.08    0.227 &lt;int&gt;\n 4   0.999 0.00397  1.45e-6   0.900 3.68e-1         0.32     0.16    0.357 &lt;int&gt;\n 5  -0.378 0.00377  1.59e-6  -0.320 7.49e-1         0.76     0.38    0.671 &lt;int&gt;\n 6   2.33  0.00487  1.90e-6   1.55  1.21e-1         0.16     0.08    0.716 &lt;int&gt;\n 7  -0.407 0.00377  2.32e-6  -0.355 7.22e-1         0.8      0.4     0.480 &lt;int&gt;\n 8   2.61  0.00446  1.66e-6   2.09  3.70e-2         0.08     0.04    0.500 &lt;int&gt;\n 9   2.98  0.00435  1.54e-6   3.11  1.89e-3         0.02     0.01    0.230 &lt;int&gt;\n10   3.63  0.00427  9.92e-7   3.48  5.03e-4         0.02     0.01    0.668 &lt;int&gt;\n# ℹ 248 more rows\n# ℹ 13 more variables: wts &lt;list&gt;, VILLCODE &lt;chr&gt;, COUNTYNAME &lt;chr&gt;,\n#   TOWNNAME &lt;chr&gt;, VILLNAME &lt;chr&gt;, VILLENG &lt;chr&gt;, COUNTYID &lt;chr&gt;,\n#   COUNTYCODE &lt;chr&gt;, TOWNID &lt;chr&gt;, TOWNCODE &lt;chr&gt;, NOTE &lt;chr&gt;,\n#   case_count &lt;dbl&gt;, geometry &lt;POLYGON [°]&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-gi",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-gi",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.6.2 Visualising Gi*",
    "text": "2.6.2 Visualising Gi*\nLet’s plot a map to visualise the Gi*.\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.8) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\nWe are able to observe clustering in the map above. The areas that have a higher Gi* value (green), indicate a higher number of dengue cases and the areas with a lower Gi* value (orange) indicate randomness in the distribution of dengue cases or a low number of cases."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-p-value-of-hcsa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-p-value-of-hcsa",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.6.3 Visualising p-value of HCSA",
    "text": "2.6.3 Visualising p-value of HCSA\nLet’s plot a map to visualise the p-values.\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") + \n  tm_borders(alpha = 0.8)\n\n\n\n\nHere we observe the significant level of the areas."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-local-hcsa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-local-hcsa",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.6.4 Visualising Local HCSA",
    "text": "2.6.4 Visualising Local HCSA\nLet’s plot the two maps next to each to compare.\n\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of Dengue\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\nAreas with fewer instances of dengue clusters, indicated by lower Gi* density, show a low p-value. Therefore, these areas have a higher significance level, sugegsting that dengue cases here are infleunced by geographical proximity."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-hot-spot-and-cold-spot-areas",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#visualising-hot-spot-and-cold-spot-areas",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.6.5 Visualising Hot Spot and Cold Spot Areas",
    "text": "2.6.5 Visualising Hot Spot and Cold Spot Areas\nThe HCSA map categorically displays both hot spots and cold spots in a dataset, highlighting High-High and Low-Low outliers. It integrates local Gi* values and their corresponding p-values for geographical regions. Next, we’ll use tmap functions to plot significant (p-values &lt; 0.05) hot spot and cold spot areas.\n\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.8) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\nHot Spot 🟢: We can see several hotspots (higher dengue cluster) and it is surrounded by hotspots as well.\nCold Spot 🟠: We can see several coldspots on the borders.\n\n\n\n\n\n\n\nNote\n\n\n\nA positive Gi* indicates clustering or a hot spot and a negative Gi* indicates dispersion or a cold spot.\n\n\nLet’s retrieve the top 3 hot spots village for subsequent tests.\n\nprint(head((HCSA_sig[HCSA_sig$gi_star &gt; 4, ]), 3)$TOWNNAME)\n\n[1] \"安南區\" \"安南區\" \"永康區\"\n\n\n\nprint(head((HCSA_sig[HCSA_sig$gi_star &gt; 4, ]), 3)$VILLNAME)\n\n[1] \"溪墘里\" \"大安里\" \"三合里\"\n\n\n\n\n\n\n\nSimilarly the top 3 cold spots village for subsequent tests.\n\nprint(head((HCSA_sig[HCSA_sig$gi_star &lt; -2, ]), 3)$TOWNNAME)\n\n[1] \"安南區\" \"安南區\" \"安南區\"\n\n\n\nprint(head((HCSA_sig[HCSA_sig$gi_star &gt; -2, ]), 3)$VILLNAME)\n\n[1] \"崇學里\" \"和平里\" \"崇善里\"\n\n\nInterestingly, the top 3 cold spots are all from the same town.\nIf we refer to the Local Moran’s I map that we got from the previous section,\n\n\n\n\n\nwe can see that the cold spots co-incide with the Low-Low clusters and the hot spots co-incide with the High-High clusters. This means that we can conclude that the hot and cold spots are statistically significant and not randomly distributed."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#creating-a-time-series-cube",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#creating-a-time-series-cube",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.7.1 Creating a Time Series Cube",
    "text": "2.7.1 Creating a Time Series Cube\nA time series cube serves as a structured repository of spatial data layers, where each layer portrays the geographical distribution of a phenomenon at a distinct temporal point. In this context, our objective is to construct a time series cube showing the weekly dynamics of dengue fever case numbers in Tainan City, Taiwan, for 2023.\nBefore we begin our Global and Local Spatial Autocorrelation Analysis, we need to once again retrieve our original dengue dataframe and Tainan dataframe. Then we will convert to a spacetime cube.\n\n\nReading layer `TAINAN_VILLAGE' from data source \n  `C:\\fathimak2020\\IS415-GAA\\Take-home_Ex\\Take-home_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 649 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0269 ymin: 22.88751 xmax: 120.6563 ymax: 23.41374\nGeodetic CRS:  TWD97\n\n\nTo fill in the missing week values, we will iterate through 20 weeks from Aug to Dec, starting with week 31. This is to ensure that the dengue_grouped dataset includes entries for all locations (loc) for each week. We will fill in missing locations for each week with zero values for case_count.\n\nfor (i in 31:(31 + 20 - 1)) {\n  empty &lt;- i\n  \n  yes_loc &lt;- dengue_grouped$loc[dengue_grouped$week == empty]\n  no_loc &lt;- setdiff(diff_loc, yes_loc)\n  \n  fill &lt;- data.frame(loc = no_loc, week = empty, case_count = 0)\n  \n  dengue_grouped &lt;- rbind(dengue_grouped, fill)\n}\n\ndengue_grouped &lt;- na.omit(dengue_grouped)\n\n\ndengue_grouped &lt;- as_tibble(dengue_grouped)\ndengue_st &lt;- spacetime(dengue_grouped, tainan,\n                      .loc_col = \"loc\",\n                      .time_col = \"week\")\n\nCheck if spacetime cube has been created.\n\nis_spacetime_cube(dengue_st)\n\n[1] TRUE"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#computing-gi",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#computing-gi",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.7.2 Computing Gi*",
    "text": "2.7.2 Computing Gi*\nNext, we’ll figure out the local Gi* statistics. This will help us spot areas where the spread of dengue fever cases is changing significantly over time.\n\n2.7.2.1 Deriving the spatial weights\nFirst we will compute spatial weights by using the inverse distance weights to compute the Gi* statistics. From this we get our neighbours and weights.\n\ndengue_nb &lt;- dengue_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nThen we will use local_gstar_perm() of group by week to calculate the local GI* statistic for each village. \n\ngi_stars &lt;- dengue_nb %&gt;% \n  group_by(week) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    case_count, nb, wt)) %&gt;% \n  tidyr::unnest(gi_star)\n\n\n\n2.8 Mann-Kendall Test\n\n\n\nSpot\nChinese\nEnglish\n\n\n\n\nHot\n安南區-溪墘里\nAnnan District-Xiqianli\n\n\nHot\n安南區-大安里\nAnnan District-Da’anli\n\n\nHot\n永康區-三合里\nYongkang District-Sanhe\n\n\nCold\n安南區-崇學里\nAnnan District-Chongxueli\n\n\nCold\n安南區-和平里\nAnnan District - Hepingli\n\n\nCold\n安南區-崇善里\nAnnan District-Chongshan Lane\n\n\n\nHot Spots\nThe Mann-Kendall test checks for trends (upward or downward) in a dataset over time. It helps determine if the trend is statistically significant. With this, we should be able to tell if the dengue cases are increasing or decreasing with time.\n\ncbg_1 &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(loc == \"安南區-溪墘里\") |&gt; \n  select(loc, week, gi_star)\n\np &lt;- ggplot(data = cbg_1, \n       aes(x = week, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\ncbg_1 %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau       sl     S     D  varS\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.632 0.000113  -120  190.   950\n\n\n\ncbg_2 &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(loc == \"安南區-大安里\") |&gt; \n  select(loc, week, gi_star)\n\np &lt;- ggplot(data = cbg_2, \n       aes(x = week, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\ncbg_2 %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau      sl     S     D  varS\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.526 0.00132  -100  190.   950\n\n\n\ncbg_3 &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(loc == \"永康區-三合里\") |&gt; \n  select(loc, week, gi_star)\n\np &lt;- ggplot(data = cbg_3, \n       aes(x = week, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\ncbg_3 %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n# A tibble: 1 × 5\n     tau       sl     S     D  varS\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -0.579 0.000406  -110  190.   950\n\n\n\nggplot() +\n  geom_line(data = cbg_1, mapping = aes(x = week, y = gi_star, color = \"Xiqianli Village\")) +\n  geom_line(data = cbg_2, mapping = aes(x = week, y = gi_star, color = \"Da'anli Village\")) + \n  geom_line(data = cbg_3, mapping = aes(x = week, y = gi_star, color = \"Sanhe Village\")) +\n  labs(x = \"week\", y = \"Gi* Value\", \n       title = \"Gi* Over Weeks (Hot Spots)\",\n       color = \"Villages\")\n\n\n\n\nCold Spots\n\ncbg_4 &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(loc == \"安南區-崇學里\") |&gt; \n  select(loc, week, gi_star)\n\np &lt;- ggplot(data = cbg_4, \n       aes(x = week, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\ncbg_4 %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n\ncbg_5 &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(loc == \"安南區-和平里\") |&gt; \n  select(loc, week, gi_star)\n\np &lt;- ggplot(data = cbg_5, \n       aes(x = week, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\ncbg_5 %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n\ncbg_6 &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(loc == \"安南區-崇善里里\") |&gt; \n  select(loc, week, gi_star)\n\np &lt;- ggplot(data = cbg_6, \n       aes(x = week, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\ncbg_6 %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  tidyr::unnest_wider(mk)\n\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(loc) %&gt;%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  tidyr::unnest_wider(mk)\n\n\n\n2.8.1 Emerging Hot/Cold Spots\n\nemerging &lt;- ehsa %&gt;% \n  arrange(sl, abs(tau)) %&gt;% \n  slice(1:5)\n\n\n\n2.8.2 Performing EHSA\nemerging_hotspot_analysis() from sfdep will be used to perform EHSA. We need to provide a spacetime object and the variable name. The k parameter, set to 1 by default, handles the number of time lags, and you can specify the number of simulation maps using nsim.\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = dengue_st, \n  .var = \"case_count\", \n  k = 1, \n  nsim = 99)\n\n\n\n2.8.3 Visualise the Distribution of EHSA Classes\nWe can see the distribution of EHSA classes by using ggplot.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar()+\n  theme(axis.text.x = element_text(size = 4))\n\n\n\n\nThe most number of cases belong to the oscillating hot spots class. Oscillating hotspot are villages witnessing regular fluctuations in dengue cases with predictable peaks.\n\nConsecutive Coldspot: villages consistently experiencing low dengue cases over an extended period, often without significant fluctuations.\nOscillating Coldspot: villages with consistent dengue case fluctuations, but at lower levels compared to busier regions.\nSporadic Coldspot: villages in potentially busy regions experiencing inconsistent and sporadic dengue cases.\nSporadic Hotspot: villages witnessing occasional but significant spikes in dengue cases.\nNo Pattern Detected: villages where no clear pattern in dengue cases is discernible over time.\n\n\n\n2.8.4 Visualising EHSA\n\ntainan_ehsa &lt;- tainan %&gt;%\n  left_join(ehsa,\n            by = join_by(loc == location))\n\n\nehsa_sig &lt;- tainan_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\n\ntmap_mode(\"plot\")\ntm_shape(tainan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have performed an emerging hotspot analysis using the extracted data."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#analysis",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#analysis",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.9 Analysis",
    "text": "2.9 Analysis\nOur analysis successfully pinpointed areas in Tainan with higher or lower concentrations of dengue cases, helping health officials and policymakers make better decisions to control the disease.\nA clear trend from August to December showed a gradual decrease in reported dengue cases, possibly due to effective measures like timely medical treatment and public health interventions. The lowering temperatures in December might also contribute, as the mosquitoes transmitting dengue thrive less in cooler weather.\nThe study also revealed shifts in where dengue cases are concentrated within Tainan. These emerging hot or cold spots need more investigation to understand the factors behind these changes. By understanding these dynamics, officials can implement interventions more effectively.\nOur observations highlighted clusters of high dengue cases in neighboring villages. Recognizing these patterns is essential for targeted actions and collaboration among nearby regions to reduce the prevalence of dengue.\nThe top three areas with the most dengue cases are, therefore local authorities should focus their efforts here:\n\nAnnan District - Xiqianli\nAnnan District - Da’anli\nYongkang District - Sanhe\n\nThe top three areas with fewer cases are, therefore local authorities should maintain efforts here:\n\nAnnan District - Chongxueli\nAnnan District - Hepingli\nAnnan District - Chongshan Lane"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#references",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#references",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.11 References",
    "text": "2.11 References\n\nKam, T. S. (2022). R for Geospatial Data Science and Analytics. Retrieved from https://r4gdsa.netlify.app.\nOpenAI. (2022). ChatGPT. Retrieved from https://openai.com/chatgpt"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#eda",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02a.html#eda",
    "title": "Take-home Exercise 2 [DATA PREPARATION]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.4 EDA",
    "text": "2.4 EDA\nNow we will create a series of choropleth maps to visualise the distribution of dengue cases across various counties through August to December.\n\ntmap_mode(\"plot\")\ntm_shape(dengue_join) +\n  tm_fill(\"case_count\",\n          style = \"quantile\",\n          palette = \"Reds\",\n          title = \"Number of Dengue Cases\",\n          legend.show = TRUE,\n          popup.vars = c(\"case_count\")) +\n  tm_layout(main.title = \"Dengue Cases in Tainan\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.5,\n            legend.width = 0.4,\n            frame = TRUE) +\n  tm_borders(alpha = 0.8) +\n  tm_compass(type = \"arrow\", size = 1.5) +\n  tm_scale_bar() +\n  tm_grid(alpha = 0.2)\n\n\n\n\nLet’s visualise the case distribution by month to see if we can gather any insight about dengue cases and month of the year.\nFirst classify the weeks into their respective months.\n\nAugust 2023: 31, 32, 33, 34, 35\nSeptember 2023: 36, 37, 38, 39\nOctober 2023: 40, 41, 42, 43\nNovember 2023: 44, 45, 46, 47\nDecember 2023: 48, 49, 50\n\n\ncreate_dengue_map &lt;- function(data, title, start_week, end_week) {\n  map_data &lt;- data %&gt;%\n    filter(week_number &gt;= start_week & week_number &lt;= end_week)\n  \n  tmap_mode(\"plot\")\n  tm_shape(map_data) +\n    tm_fill(\"case_count\",\n            style = \"quantile\",\n            palette = \"Reds\",\n            title = paste(\"Count\")) +\n        tm_layout(main.title = paste(title),\n              main.title.position = \"center\",\n              main.title.size = 1,  \n              legend.height = 0.5,  \n              legend.width = 0.4,  \n              frame = TRUE) +\n    tm_borders(alpha = 0.8) + \n        tm_grid(alpha = 0.2)\n}\n\naug_map &lt;- create_dengue_map(dengue_join, \"August\", 31, 35)\nsep_map &lt;- create_dengue_map(dengue_join, \"September\", 36, 39)\noct_map &lt;- create_dengue_map(dengue_join, \"October\", 40, 43)\nnov_map &lt;- create_dengue_map(dengue_join, \"November\", 44, 47)\ndec_map &lt;- create_dengue_map(dengue_join, \"December\", 48, 50)\n\ntmap_arrange(aug_map, sep_map, oct_map, nov_map, dec_map)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThroughout the months from August to December, a noticeable trend emerges in Tainan’s various counties: the intensity of red on the maps steadily diminishes. This reduction suggests a decline in the number of reported dengue cases each month. Several factors could contribute to this decline, including effective control measures such as prompt medical treatment and public health interventions. Additionally, the decreasing temperatures towards December might play a role, as mosquitoes, responsible for dengue transmission, thrive in warmer environments.\nMoreover, another observation is the spatial clustering of high dengue case numbers in neighboring counties. Understanding these geographical patterns becomes important for targeted interventions and collaborative efforts among neighboring regions to address and mitigate the prevalence of dengue.\n\n\n\nProceed to Take-home Exercise 2b: Analysis"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#global-morans-i-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#global-morans-i-test",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.4.3 Global Moran’s I Test",
    "text": "2.4.3 Global Moran’s I Test\nMoran’s I test can be performed by using global_moran_test(). This can check whether there is spatial autocorrelation in case_count.\n\nglobal_moran_test(wm_q$case_count,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 10.886, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.396194784      -0.003891051       0.001350842 \n\n\nMoran I statistic standard deviate is 10.886 which indicates a high level of spatial autocorrelation. The low p-value indicates that the the spatial correlation is statistically significant. Therefore, we can tell that villages with a high number of dengue cases are clustered together"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#global-morans-i-permutation-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02b.html#global-morans-i-permutation-test",
    "title": "Take-home Exercise 2[ANALYSIS]: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan",
    "section": "2.4.4 Global Moran’s I Permutation Test",
    "text": "2.4.4 Global Moran’s I Permutation Test\nNow we will perform a simulation on the Moran’s I test.\nWe need to use set.seed() before executing the simulation to ensure consistency and reproducibility in the simulation process.\n\nset.seed(1234)\n\nThen we use global_moran_perm() to perform the Monte Carlo simulation.\n\nglobal_moran_perm(wm_q$case_count,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.39619, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nSince the p-value is smaller than the alpha value of 0.05, we will reject the null hypothesis. The spatial distribution of dengue cases per county are not randomly distributed. The Moran I statistics has a value of 0.397, since it is greater than 0, we can conclude that the spatial distribution of dengue cases does show signs of clustering.\n\n\n\n\n\n\nTip\n\n\n\n✅ Task Complete! We have utilised the extracted data to conduct a global spatial autocorrelation analysis."
  }
]